{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mezlet/PPI-Inhibitors/blob/main/code/seqfeaturesand_gnn_generate_prediction_gnn_with_binders_and_random_both_as_negative_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zy8pv8_EQa"
      },
      "source": [
        "**Set the Runtime->Change Runtime Type to GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLKfwNHlwnSE"
      },
      "source": [
        "# Protein 3d structure assessment with graph neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uOhzY0qAj-0",
        "outputId": "8aa0df30-1fe9-4ce5-bf0b-4db37328fefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_3U6PueCCgv",
        "outputId": "a19245b5-13cd-4e31-d304-d6c7289888ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'PPI-Inhibitors': No such file or directory\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.85-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n",
            "Cloning into 'PPI-Inhibitors'...\n",
            "remote: Enumerating objects: 1341, done.\u001b[K\n",
            "remote: Total 1341 (delta 0), reused 0 (delta 0), pack-reused 1341 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1341/1341), 2.59 GiB | 36.53 MiB/s, done.\n",
            "Resolving deltas: 100% (417/417), done.\n",
            "Updating files: 100% (605/605), done.\n"
          ]
        }
      ],
      "source": [
        "#!rm -r Data\n",
        "!rm -r PPI-Inhibitors\n",
        "!pip install biopython\n",
        "!git clone https://github.com/adibayaseen/PPI-Inhibitors\n",
        "#!pip install py3Dmol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SLRQVvJaNsg5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Using Sklearn One hot encoder to encode the atoms\n",
        "Output is of size N*M where N is the total number of atoms and M is the total number of encoded features\n",
        "'''\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import glob\n",
        "def atom1(structure):\n",
        "    atomslist=np.array(sorted(np.array(['C', 'CA', 'CB', 'CG', 'CH2', 'N','NH2',  'OG','OH', 'O1', 'O2', 'SE','1']))).reshape(-1,1)\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc.fit(atomslist)\n",
        "    atom_list=[]\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_name() in atomslist:\n",
        "            atom_list.append(atom.get_name())\n",
        "        else:\n",
        "            atom_list.append(\"1\")\n",
        "    atoms_onehot=enc.transform(np.array(atom_list).reshape(-1,1)).toarray()\n",
        "    return atoms_onehot\n",
        "##############\n",
        "'''\n",
        "One hot encoded residue infomration using SKlearn Library\n",
        "\n",
        "Output is N*M where N is the total number of atoms and M is the encoded features of the residues.\n",
        "Any unknown  residue is mapped to 1\n",
        "'''\n",
        "\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import glob\n",
        "from Bio.PDB import *\n",
        "\n",
        "\n",
        "def res1(structure):\n",
        "    residuelist=np.array(sorted(np.array(['ALA', 'ARG', 'ASN', 'ASP', 'GLN', 'GLU', 'GLY', 'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL', 'CYS', 'HIS','1']))).reshape(-1,1)\n",
        "    encr = OneHotEncoder(handle_unknown='ignore')\n",
        "    encr.fit(residuelist)\n",
        "\n",
        "\n",
        "\n",
        "    residue_list=[]\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_parent().get_resname() in residuelist:\n",
        "            residue_list.append((atom.get_parent()).get_resname())\n",
        "        else:\n",
        "            residue_list.append(\"1\")\n",
        "\n",
        "    res_onehot=encr.transform(np.array(residue_list).reshape(-1,1)).toarray()\n",
        "\n",
        "    return res_onehot\n",
        "###########\n",
        "\n",
        "'''\n",
        "It calculates the neighbours of each atom i.e. 10 distinct neighbours\n",
        "Output is  in the form of a ditionary representing an  adjacency list where each source atom and neighbouring atom is represented bby its sequence index .\n",
        "'''\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "from Bio.PDB.NeighborSearch import NeighborSearch\n",
        "\n",
        "def neigh1(structure):\n",
        "    #atom_list is a numpy array  that   contains all the atoms of the pdb file in atom object\n",
        "    atom_list=np.array([atom for atom in structure.get_atoms()])\n",
        "\n",
        "    #for atom in structure.get_atoms():\n",
        "    #    atom_list.append(atom)\n",
        "    #neighbour_list contains all the  neighbour atomic pairs  i.e. like if N has neighbours O and C then it is stored as [[N,C],[N,O]] i.e. has dimension N*2 where N is the total number of possible neighbours all the atoms have in an unsorted manner and it stores in the form of  atom object\n",
        "\n",
        "\n",
        "    p4=NeighborSearch(atom_list)\n",
        "    neighbour_list=p4.search_all(6,level=\"A\")\n",
        "    neighbour_list=np.array(neighbour_list)\n",
        "\n",
        "    #dist is the distance between the neighbour and the source atom  i.e. dimension is N*1\n",
        "    dist=np.array(neighbour_list[:,0]-neighbour_list[:,1])\n",
        "    #sorting in ascending order\n",
        "    place=np.argsort(dist)\n",
        "    sorted_neighbour_list=neighbour_list[place]\n",
        "\n",
        "    #old_atom_number is used for  storing atom id of the original protein before sorting\n",
        "    #old_residue_number is used for storing residue number of the original protein before sorting\n",
        "    source_vertex_list_atom_object=np.array(sorted_neighbour_list[:,0])\n",
        "    len_source_vertex=len(source_vertex_list_atom_object)\n",
        "    neighbour_vertex_with_respect_each_source_atom_object=np.array(sorted_neighbour_list[:,1])\n",
        "    old_atom_number=[]\n",
        "    old_residue_number=[]\n",
        "    for i in atom_list:\n",
        "        old_atom_number.append(i.get_serial_number())\n",
        "        old_residue_number.append(i.get_parent().get_id()[1])\n",
        "    old_atom_number=np.array(old_atom_number)\n",
        "    old_residue_number=np.array(old_residue_number)\n",
        "    req_no=len(neighbour_list)\n",
        "    total_atoms=len(atom_list)\n",
        "    #neigh_same_res is the 2D numpy array to store the indices of the  neighbours of  same residue and is of the shape N*10 where N is the total number of atoms\n",
        "    #neigh_diff_res is 2D numpy array to store  the indices of the  neighbours of different residue\n",
        "    #same_flag is used to restrict the neighbours belonging to same residue  to 10\n",
        "    #diff_flag is used to restrict the neighbours belonging to different residue to 10\n",
        "    neigh_same_res=np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    neigh_diff_res=np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    same_flag=[0]*total_atoms\n",
        "    diff_flag=[0]*total_atoms\n",
        "    for i in range(len_source_vertex):\n",
        "        source_atom_id=source_vertex_list_atom_object[i].get_serial_number()\n",
        "        neigh_atom_id=neighbour_vertex_with_respect_each_source_atom_object[i].get_serial_number()\n",
        "        source_atom_res=source_vertex_list_atom_object[i].get_parent().get_id()[1]\n",
        "        neigh_atom_res=neighbour_vertex_with_respect_each_source_atom_object[i].get_parent().get_id()[1]\n",
        "        #finding out index of the source and neighbouring atoms from the original atom array with respect to their residue id and atom id\n",
        "        temp_index1=np.where(source_atom_id==old_atom_number)[0]\n",
        "\n",
        "        temp_index2=np.where(neigh_atom_id==old_atom_number)[0]\n",
        "        for i1 in temp_index1:\n",
        "            if old_residue_number[i1]==source_atom_res:\n",
        "                source_index=i1\n",
        "                break\n",
        "        for i1 in temp_index2:\n",
        "            if old_residue_number[i1]==neigh_atom_res:\n",
        "                neigh_index=i1\n",
        "                break\n",
        "        #if both the residues are same\n",
        "\n",
        "        if source_atom_res==neigh_atom_res :\n",
        "\n",
        "            #limiting the number of neighbours of same residue to 10\n",
        "\n",
        "            if int(same_flag[source_index])< 10:\n",
        "                neigh_same_res[source_index][same_flag[source_index]]=neigh_index\n",
        "                same_flag[source_index]+=1\n",
        "\n",
        "            if int(same_flag[neigh_index])< 10:\n",
        "                neigh_same_res[neigh_index][same_flag[neigh_index]]=source_index\n",
        "                same_flag[neigh_index]+=1\n",
        "\n",
        "        # if both the residues are different\n",
        "        elif source_atom_res!=neigh_atom_res :\n",
        "\n",
        "            #limiting the number of neighbours of different residues to 10\n",
        "\n",
        "            if int(diff_flag[source_index])< 10:\n",
        "                neigh_diff_res[source_index][diff_flag[source_index]]=neigh_index\n",
        "                diff_flag[source_index]+=1\n",
        "\n",
        "\n",
        "            if int(diff_flag[neigh_index])< 10:\n",
        "\n",
        "                neigh_diff_res[neigh_index][diff_flag[neigh_index]]=source_index\n",
        "                diff_flag[neigh_index]+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return neigh_same_res,neigh_diff_res\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "#from QA.data_import import get_dataloader,data1\n",
        "#from QA.temp_network import GNN\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "class GNN_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, v_feats, trainable=True, **kwargs):\n",
        "        #pdb.set_trace()\n",
        "        super(GNN_Layer, self).__init__()\n",
        "        self.v_feats = v_feats\n",
        "        self.filters = filters\n",
        "\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device=device\n",
        "        self.Wsv = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "        #print(\"Wsv shape\",self.Wsv.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        Z,same_neigh,diff_neigh = x\n",
        "        #print(\"input\",x)\n",
        "        #print(Z.shape)\n",
        "        node_signals = Z@self.Wsv\n",
        "        neigh_signals_same=Z@self.Wsr\n",
        "        neigh_signals_diff=Z@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal = (torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal = (torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "        final_res = torch.relu(node_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res,same_neigh,diff_neigh\n",
        "\n",
        "class GNN_First_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, trainable=True, **kwargs):\n",
        "\n",
        "        super(GNN_First_Layer, self).__init__()\n",
        "        self.filters = filters\n",
        "        #pdb.set_trace()\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "        self.Wv = nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wr = nn.Parameter( torch.randn(21,self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        #print(\"Here i am in froward of first layer\")\n",
        "        atoms, residues,same_neigh,diff_neigh = x\n",
        "        #print(\"input atoms\",atoms)\n",
        "        #print(atoms.shape)\n",
        "        #print(\"Wv shape\",self.Wv.shape)\n",
        "        node_signals = atoms@self.Wv\n",
        "        ####\n",
        "        #print(\"input residues\",residues)\n",
        "        #print(residues.shape)\n",
        "        #print(\"Wr shape\",self.Wr.shape)\n",
        "        ####\n",
        "        residue_signals = residues@self.Wr\n",
        "        #print(\"Wsr shape\",self.Wsr.shape)\n",
        "        #print(\"Wdr shape\",self.Wdr.shape)\n",
        "        neigh_signals_same=atoms@self.Wsr\n",
        "        neigh_signals_diff=atoms@self.Wdr\n",
        "        #print(\"neigh_signals_same shape\",neigh_signals_same.shape)\n",
        "        ####\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        \"\"\"\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(1)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(1)\n",
        "        \"\"\"\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        #print(\"same norm\",same_neigh > -1, 1)\n",
        "        #1/0\n",
        "        #Orignal\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        same_norm = torch.sum(same_neigh > -1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1).type(torch.float)\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal=(torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal=(torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "\n",
        "        final_res = torch.relu(node_signals+residue_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "        #pdb.set_trace()\n",
        "        #print(\" in First layer final_res.shape\",final_res.shape,\"same_neigh.shape\",same_neigh.shape,\"diff_neigh.shape\",diff_neigh.shape)\n",
        "        return final_res, same_neigh,diff_neigh\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, in_dims, out_dims, trainable=True, **kwargs):\n",
        "        #pdb.set_trace()\n",
        "        super(Dense, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.out_dims = out_dims\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "\n",
        "        self.W = nn.Parameter( torch.randn(self.in_dims, self.out_dims, device=self.cuda_device,requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        Z = torch.sigmoid(torch.matmul(x, self.W))\n",
        "\n",
        "        return Z\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        #pdb.set_trace()\n",
        "        #Actual\n",
        "        \"\"\"\n",
        "        self.conv1 = GNN_First_Layer(filters=128)\n",
        "        self.conv2 = GNN_Layer(v_feats=128, filters=256)\n",
        "        self.conv3 = GNN_Layer(v_feats=256, filters=512)\n",
        "        self.dense = Dense(in_dims=512, out_dims=1)\n",
        "        \"\"\"\n",
        "        #modified by me\n",
        "        self.conv1 = GNN_First_Layer(filters=512)\n",
        "        self.conv2 = GNN_Layer(v_feats=512, filters=1024)\n",
        "        self.conv3 = GNN_Layer(v_feats=1024, filters=512)\n",
        "        self.dense = Dense(in_dims=512, out_dims=1)\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        x1=self.conv1(x)\n",
        "        #pdb.set_trace()\n",
        "        x2=self.conv2(x1)\n",
        "        #pdb.set_trace()\n",
        "        x3=self.conv3(x2)\n",
        "        #pdb.set_trace()\n",
        "        x=x3[0]\n",
        "        x=torch.sum(x,axis=0).view(1,-1)\n",
        "        #pdb.set_trace()\n",
        "        x = F.normalize(x)\n",
        "        \"\"\"\n",
        "        #pdb.set_trace()\n",
        "        x5=self.dense(x)\n",
        "        #pdb.set_trace()\n",
        "        x6=torch.squeeze(x5,1)\n",
        "        \"\"\"\n",
        "        #print(\"GNN\")\n",
        "\n",
        "        return x#x6,\n",
        "\n",
        "    def processProtein(UniqueProtein, PdBloc):#, PdBloc):\n",
        "        data_list = []\n",
        "        #PdBloc='/content/PPI-Inhibitors/Data/Pdb/'\n",
        "        #data=glob.glob(PdBloc+'/*')\n",
        "        #data=glob.glob(PdBloc)\n",
        "        #data=data[0:2]\n",
        "        #assert (len(UniqueProtein) == len(data)) , \"The two lists must be the same length!\"\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        PData_dict={}\n",
        "        for i in range(len(UniqueProtein)):\n",
        "            #print('Converting PDB to Graph: {}/{}'.format(i+1, len(UniqueProtein)))\n",
        "            UniqueProtein[i]=UniqueProtein[i].split('.pdb')[0]\n",
        "            P1=PdBloc+UniqueProtein[i]+'.pdb'\n",
        "            #if P1 in UniqueProtein:\n",
        "            parser = PDBParser()\n",
        "            with warnings.catch_warnings(record=True) as w:\n",
        "              structure = parser.get_structure(\"\", P1)\n",
        "            one_hot_atom=(atom1(structure))\n",
        "            #print(\"\\none_hot_atom.shape\",one_hot_atom.shape)\n",
        "            #\n",
        "            one_hot_res=(res1(structure))\n",
        "            #print(\"\\none_hot_atom.shape\",one_hot_res.shape)\n",
        "            neigh_same_res,neigh_diff_res=(neigh1(structure))\n",
        "            #print(\"\\neigh_same_res.shape,neigh_diff_re.shape\",neigh_same_res.shape,\"\\n\",neigh_diff_res.shape)\n",
        "            # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "            one_hot_atom=torch.tensor(one_hot_atom,dtype=torch.float32).to(device)\n",
        "            one_hot_res=torch.tensor(one_hot_res,dtype=torch.float32).to(device)\n",
        "            neigh_same_res=torch.tensor(neigh_same_res).to(device).long()\n",
        "            neigh_diff_res=torch.tensor(neigh_diff_res).to(device).long()\n",
        "            GNNData = [one_hot_atom,one_hot_res,neigh_same_res,neigh_diff_res]\n",
        "            #import pdb; pdb.set_trace()\n",
        "            #GNNData.__setitem__('Total atoms', len(one_hot_atom))\n",
        "            #data_list.append(GNNData)\n",
        "            PData_dict[UniqueProtein[i]]= GNNData\n",
        "        return PData_dict\n",
        "def readFile(filename):\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  Name=[];PdbId=[];Ligandnames=[];SMILES=[];labels=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      #if len(d)==6:\n",
        "      name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "      Name.append(name);PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);labels.append(float (y));\n",
        "  return  PdbId,Ligandnames,SMILES,labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SzwASCsDn1O2",
        "outputId": "69f5d2ad-bf77-4e83-e199-c82a25b2796e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\\n!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\\n!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\\n!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\\n!pip install torch-geometric\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\"\"\"\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gglFcYgfcvGE",
        "outputId": "5f7636b0-3496-44b4-f197-217c1a8854be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kora\n",
            "  Downloading kora-0.9.20-py3-none-any.whl.metadata (703 bytes)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from kora) (7.34.0)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.12/dist-packages (from kora) (1.8.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from fastcore->kora) (25.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython->kora)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython->kora) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython->kora) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kora) (0.2.14)\n",
            "Downloading kora-0.9.20-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, kora\n",
            "Successfully installed jedi-0.19.2 kora-0.9.20\n"
          ]
        }
      ],
      "source": [
        "#Compound part\n",
        "!pip install kora\n",
        "import kora.install.rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRjE8tIydDv0"
      },
      "outputs": [],
      "source": [
        "#from torch_geometric.data import InMemoryDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uE6yIRsQ7C5Y",
        "outputId": "18c86d90-0174-43ac-ba7d-e189ad7f02ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Downloading rdkit-2025.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (36.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "etHfJlkb6uhF"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X_mtVh7bdQrc"
      },
      "outputs": [],
      "source": [
        "#from torch_geometric.nn.conv import gcn_conv\n",
        "##\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json,pickle\n",
        "from collections import OrderedDict\n",
        "from rdkit import Chem\n",
        "#from rdkit.Chem import MolFromSmiles\n",
        "import networkx as nx\n",
        "#from utils import *\n",
        "# training function at each epoch\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    print('Training on {} samples...'.format(len(GCN_data)))\n",
        "\n",
        "    model.train()\n",
        "    loss_fn = nn.MSELoss()\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        #data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output,Sfeatures= model(data)#,Sfeatures\n",
        "        loss = loss_fn(output, data.target.view(-1, 1).float().to(device))\n",
        "        #loss = loss_fn(output, data.Target.view(-1, 1).float().to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % LOG_INTERVAL == 0:\n",
        "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
        "                                                                           batch_idx * len(data.x),\n",
        "                                                                           len(train_loader.dataset),\n",
        "                                                                           100. * batch_idx / len(train_loader),\n",
        "                                                                           loss.item()))\n",
        "\n",
        "def predicting(model, device,test_loader,labels):\n",
        "    model.eval()\n",
        "    total_preds = torch.Tensor()\n",
        "    total_labels = torch.Tensor()\n",
        "    print('Make prediction for {} samples...'.format(len(test_loader)))\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader):\n",
        "            #data = data.to(device)\n",
        "            try:\n",
        "              output,Sfeatures = model(data)#,Sfeatures\n",
        "            except Exception as trr:\n",
        "              print (trr)\n",
        "              continue\n",
        "            #output = model(data)\n",
        "            #print (\"output\",output)\n",
        "            #pdb.set_trace()\n",
        "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
        "            total_labels = torch.cat((total_labels, data.target.view(-1, 1).cpu()), 0)\n",
        "    return total_labels.numpy().flatten(),total_preds.numpy().flatten(),Sfeatures\n",
        "##\n",
        "def atom_features(atom):\n",
        "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
        "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
        "                    [atom.GetIsAromatic()])\n",
        "\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def smile_to_graph(smile):\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        "    if mol is not None:\n",
        "      c_size = mol.GetNumAtoms()\n",
        "      features = []\n",
        "      for atom in mol.GetAtoms() :\n",
        "          feature = atom_features(atom)\n",
        "          features.append( feature / sum(feature) )\n",
        "\n",
        "      edges = []\n",
        "      for bond in mol.GetBonds():\n",
        "          edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
        "      g = nx.Graph(edges).to_directed()\n",
        "      edge_index = []\n",
        "      for e1, e2 in g.edges:\n",
        "          edge_index.append([e1, e2])\n",
        "\n",
        "      return c_size, features, edge_index\n",
        "import os\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "from scipy import stats\n",
        "#from torch_geometric.data import InMemoryDataset, DataLoader\n",
        "#from torch_geometric import data as DATA\n",
        "import torch\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score\n",
        "class PrepairDataset():\n",
        "    \"\"\"\n",
        "    def __init__(self, root='/content/PPI-Inhibitors/Data/2p2iInhibitorsSMILES.txt', dataset='PPI-Inhibitor',\n",
        "                 xd=None, xt=None, y=None, transform=None,\n",
        "                 pre_transform=None,smile_graph=None,PdBloc='/content/PPI-Inhibitors/Data/Pdb/*'):\n",
        "\n",
        "        #root is required for save preprocessed data, default is '/tmp'\n",
        "        super(PrepairDataset, self).__init__(root, transform, pre_transform)\n",
        "        # benchmark dataset, default = 'davis'\n",
        "        self.dataset = dataset\n",
        "        if os.path.isfile(self.processed_paths[0]):\n",
        "            print('Pre-processed data found: {}, loading ...'.format(self.processed_paths[0]))\n",
        "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "        else:\n",
        "            print('Pre-processed data {} not found, doing pre-processing...'.format(self.processed_paths[0]))\n",
        "            with open(root) as f:\n",
        "              D = f.readlines()\n",
        "            Name=[];PdbId=[];LigandId=[];SMILES=[];labels=[];\n",
        "            from tqdm import tqdm as tqdm\n",
        "            #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "            for d in tqdm(D):\n",
        "                name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "                Name.append(name);PdbId.append(Pdbid);LigandId.append(Ligandid);SMILES.append(smiles);labels.append(y);\n",
        "            PdBloc='/content/PPI-Inhibitors/Data/Pdb'\n",
        "            self.process(SMILES, PdbId, labels,smile_graph,PdBloc)\n",
        "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "    \"\"\"\n",
        "    def ProcessfromFile(self,filename,smile_graph):\n",
        "        PdBloc='/content/PPI-Inhibitors/Data/Pdb'\n",
        "        with open(filename) as f:\n",
        "          D = f.readlines()\n",
        "        Name=[];PdbId=[];LigandId=[];SMILES=[];labels=[];\n",
        "        All_data_list=[]\n",
        "        from tqdm import tqdm as tqdm\n",
        "        #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "        for d in tqdm(D):\n",
        "            if len(d)==6:\n",
        "              name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "              Name.append(name);PdbId.append(Pdbid);LigandId.append(Ligandid);SMILES.append(smiles);labels.append(y);\n",
        "            elif len(d)==2:\n",
        "              Pdbid,smiles= d.split()\n",
        "              PdbId.append(Pdbid);SMILES.append(smiles);labels.append(float(-1.0));\n",
        "         ########\n",
        "        UniqueProtein=list (set (PdbId))\n",
        "        UniqueSMILES=list (set (SMILES))\n",
        "        ProteinData_dict=PrepairDataset.processProtein(UniqueProtein,PdBloc)\n",
        "        SmilesData_dict=PrepairDataset.processSMILES(UniqueSMILES, labels_list,smile_graph)\n",
        "        for e in range(len(SMILES)):\n",
        "          Pdata,Sdata,label=PdbId[e],SMILES[e],labels[e]\n",
        "          #Preprocessed data\n",
        "          if Sdata in SmilesData_dict and Pdata in ProteinData_dict:\n",
        "            Pdata,Sdata=ProteinData_dict[Pdata],SmilesData_dict[Sdata]\n",
        "            All_data=DATA.data(Pdata,Sdata,label)\n",
        "            All_data_list.append(All_data)\n",
        "        return All_data_list\n",
        "    # Customize the process method to fit the task of drug-target affinity prediction\n",
        "    # Inputs:\n",
        "    # XD - list of SMILES\n",
        "    #XT: target Protein PDB id\n",
        "    # Y: list of labels (i.e. affinity)\n",
        "    # Return: PyTorch-Geometric format processed data\n",
        "    def process(xd, xt, y,smile_graph):\n",
        "        assert (len(xd) == len(xt) and len(xt) == len(y)), \"The three lists must be the same length!\"\n",
        "        data_list = []\n",
        "        data_len = len(xd)\n",
        "        print(\"data len\",data_len)\n",
        "        smiles_list=[];targetP_list=[];labels_list=[];\n",
        "        for i in range(data_len):\n",
        "            #print('Converting SMILES to graph: {}/{}'.format(i+1, data_len))\n",
        "            smiles = xd[i]\n",
        "            targetP = xt[i]\n",
        "            labels = y[i]\n",
        "            smiles_list.append(smiles);targetP_list.append(targetP);labels_list.append(labels);\n",
        "        ########\n",
        "        UniqueProtein=list (set (targetP_list))\n",
        "        UniqueSMILES=list (set (smiles_list))\n",
        "        ProteinData_dict=PrepairDataset.processProtein(UniqueProtein[0:2])#,PdBloc)\n",
        "        print( \"total pdbs\",len(ProteinData_dict))\n",
        "        SmilesData_dict=PrepairDataset.processSMILES(UniqueSMILES, labels_list,smile_graph)\n",
        "        All_data_list=[]\n",
        "        \"\"\"\n",
        "        with open(filename) as f:\n",
        "          D = f.readlines()\n",
        "        Name=[];PdbId=[];LigandId=[];SMILES=[];labels=[];\n",
        "        All_data_list=[]\n",
        "        from tqdm import tqdm as tqdm\n",
        "        #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "        for d in tqdm(D):\n",
        "            if len(d)==6:\n",
        "              name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "              Name.append(name);PdbId.append(Pdbid);LigandId.append(Ligandid);SMILES.append(smiles);labels.append(y);\n",
        "              #Preprocessed data\n",
        "              Pdata,Sdata,label=ProteinData_dict[Pdbid],SmilesData_dict[smiles],y\n",
        "              All_data=DATA.data(Pdata,Sdata,label)\n",
        "              All_data_list.append(All_data)\n",
        "            elif len(d)==2:\n",
        "              Pdbid,smiles= d.split()\n",
        "              PdbId.append(Pdbid);SMILES.append(smiles);labels.append(float(-1.0));\n",
        "              #Preprocessed data\n",
        "              Pdata,Sdata,label=ProteinData_dict[Pdbid],SmilesData_dict[smiles],float(-1.0)\n",
        "              All_data=DATA.data(Pdata,Sdata,label)\n",
        "              All_data_list.append(All_data)\n",
        "        return All_data_list\n",
        "        \"\"\"\n",
        "        for e in range(len(smiles_list)):\n",
        "          Pdata,Sdata,label=targetP_list[e],smiles_list[e],labels_list[e]\n",
        "          #Preprocessed data\n",
        "          if Sdata in SmilesData_dict and Pdata in ProteinData_dict:\n",
        "            print(\"Sdata,Pdata,label\",Sdata,Pdata,label)\n",
        "            GNN_data,GCN_data=ProteinData_dict[Pdata],SmilesData_dict[Sdata]\n",
        "            All_data=(GNN_data,GCN_data,label)\n",
        "            All_data_list.append(All_data)\n",
        "        return All_data_list\n",
        "    def processProtein(UniqueProtein, PdBloc):#, PdBloc):\n",
        "        data_list = []\n",
        "        #PdBloc='/content/PPI-Inhibitors/Data/Pdb/'\n",
        "        #data=glob.glob(PdBloc+'/*')\n",
        "        #data=glob.glob(PdBloc)\n",
        "        #data=data[0:2]\n",
        "        #assert (len(UniqueProtein) == len(data)) , \"The two lists must be the same length!\"\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        PData_dict={}\n",
        "        for i in range(len(UniqueProtein)):\n",
        "            #print('Converting PDB to Graph: {}/{}'.format(i+1, len(UniqueProtein)))\n",
        "            UniqueProtein[i]=UniqueProtein[i].split('.pdb')[0]\n",
        "            P1=PdBloc+UniqueProtein[i]+'.pdb'\n",
        "            #if P1 in UniqueProtein:\n",
        "            parser = PDBParser()\n",
        "            with warnings.catch_warnings(record=True) as w:\n",
        "              structure = parser.get_structure(\"\", P1)\n",
        "            one_hot_atom=(atom1(structure))\n",
        "            #print(\"\\none_hot_atom.shape\",one_hot_atom.shape)\n",
        "            #\n",
        "            one_hot_res=(res1(structure))\n",
        "            #print(\"\\none_hot_atom.shape\",one_hot_res.shape)\n",
        "            neigh_same_res,neigh_diff_res=(neigh1(structure))\n",
        "            #print(\"\\neigh_same_res.shape,neigh_diff_re.shape\",neigh_same_res.shape,\"\\n\",neigh_diff_res.shape)\n",
        "            # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "            one_hot_atom=torch.tensor(one_hot_atom,dtype=torch.float32).to(device)\n",
        "            one_hot_res=torch.tensor(one_hot_res,dtype=torch.float32).to(device)\n",
        "            neigh_same_res=torch.tensor(neigh_same_res).to(device).long()\n",
        "            neigh_diff_res=torch.tensor(neigh_diff_res).to(device).long()\n",
        "            GNNData = [one_hot_atom,one_hot_res,neigh_same_res,neigh_diff_res]\n",
        "            #import pdb; pdb.set_trace()\n",
        "            #GNNData.__setitem__('Total atoms', len(one_hot_atom))\n",
        "            #data_list.append(GNNData)\n",
        "            PData_dict[UniqueProtein[i]]= GNNData\n",
        "        return PData_dict\n",
        "    def processSMILES(xd, targets,smile_graph):\n",
        "        #assert (len(xd) == len(smile_graph)), \"The two lists must be the same length!\"\n",
        "        data_list = []\n",
        "        data_len = len(xd)\n",
        "        Sdata_dict={}\n",
        "        for i in range(data_len):\n",
        "            print('Converting SMILES to graph: {}/{}'.format(i+1, data_len))\n",
        "            smiles = xd[i]\n",
        "            target=targets[i]\n",
        "            print(\"target\",target)\n",
        "            mol = Chem.MolFromSmiles(smile)\n",
        "            if smile in smile_graph and mol is not None and smile_graph[smiles] is not None:\n",
        "              #import pdb; pdb.set_trace()\n",
        "              # convert SMILES to molecular representation using rdkit\n",
        "              c_size, features, edge_index = smile_graph[smiles]\n",
        "              if c_size>1:\n",
        "                # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "                GCNData = DATA.Data(x=torch.Tensor(features),\n",
        "                                    edge_index=torch.LongTensor(edge_index).transpose(1, 0),target=torch.LongTensor([target]))\n",
        "                #import pdb; pdb.set_trace()\n",
        "              # GCNData.target = torch.LongTensor([target])\n",
        "                #GCNData.__setitem__('c_size', torch.LongTensor([c_size]))\n",
        "                # append graph, label and target sequence to data list\n",
        "                #data_list.append(GCNData)\n",
        "                Sdata_dict[smiles]=GCNData\n",
        "        #Sdata_dict=dict(zip(xd,data_list))\n",
        "        return Sdata_dict\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "        print('Graph construction done. Saving to file.')\n",
        "        data, slices = self.collate(data_list)\n",
        "        # save preprocessed data:\n",
        "        torch.save((data, slices), self.processed_paths[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "w_uTP5QndCu5"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import DataStructs\n",
        "def getFP(s,r = 3,nBits =2048):\n",
        "    compound = Chem.MolFromSmiles(s.strip())\n",
        "    if compound is not None:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(compound, r, nBits = nBits)\n",
        "        #fp = pat.GetAvalonCountFP(compound,nBits=nBits)\n",
        "        m = np.zeros((0, ), dtype=np.int8)\n",
        "        DataStructs.ConvertToNumpyArray(fp, m)\n",
        "        return m\n",
        "def twomerFromSeq(s):\n",
        "    k=2\n",
        "    groups={'A':'1','V':'1','G':'1','I':'2','L':'2','F':'2','P':'2','Y':'3',\n",
        "            'M':'3','T':'3','S':'3','H':'4','N':'4','Q':'4','W':'4',\n",
        "            'R':'5','K':'5','D':'6','E':'6','C':'7'}\n",
        "    crossproduct=[''.join (i) for i in product(\"1234567\",repeat=k)]\n",
        "    for i in range (0,len(crossproduct)): crossproduct[i]=int(crossproduct[i])\n",
        "    ind=[]\n",
        "    for i in range (0,len(crossproduct)): ind.append(i)\n",
        "    combinations=dict(zip(crossproduct,ind))\n",
        "\n",
        "    V=np.zeros(int((math.pow(7,k))))      #defines a vector of 343 length with zero entries\n",
        "    try:\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                c+=groups[kmer[l]]\n",
        "                V[combinations[int(c)]]+=1\n",
        "    except:\n",
        "        count={'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0}\n",
        "        for q in range(0,len(s)):\n",
        "            if s[q]=='A' or s[q]=='V' or s[q]=='G':\n",
        "                count['1']+=1\n",
        "            if s[q]=='I' or s[q]=='L'or s[q]=='F' or s[q]=='P':\n",
        "                count['2']+=1\n",
        "            if s[q]=='Y' or s[q]=='M'or s[q]=='T' or s[q]=='S':\n",
        "                count['3']+=1\n",
        "            if s[q]=='H' or s[q]=='N'or s[q]=='Q' or s[q]=='W':\n",
        "                count['4']+=1\n",
        "            if s[q]=='R' or s[q]=='K':\n",
        "                count['5']+=1\n",
        "            if s[q]=='D' or s[q]=='E':\n",
        "                count['6']+=1\n",
        "            if s[q]=='C':\n",
        "                count['7']+=1\n",
        "        val=list(count.values()  )           #[ 0,0,0,0,0,0,0]\n",
        "        key=list(count.keys()     )           #['1', '2', '3', '4', '5', '6', '7']\n",
        "        m=0\n",
        "        ind=0\n",
        "        for t in range(0,len(val)):     #find maximum value from val\n",
        "            if m<val[t]:\n",
        "                m=val[t]\n",
        "                ind=t\n",
        "        m=key [ind]                     # m=group number of maximum occuring group alphabets in protein\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                if kmer[l] not in groups:\n",
        "                    c+=m\n",
        "                else:\n",
        "                    c+=groups[kmer[l]]\n",
        "            V[combinations[int(c)]]+=1\n",
        "\n",
        "    V=V/(len(s)-1)\n",
        "    return np.array(V)\n",
        "from scipy import spatial\n",
        "def chainLabel(Cname_T,xl_T,Cname,xl):\n",
        "    \"\"\"\n",
        "    Cname_T: Target chain Name\n",
        "    xl_T: Target chain co-ordinates\n",
        "    Cname: Off Target chain Name\n",
        "    xl: Off Target chain co-ordinates\n",
        "    \"\"\"\n",
        "    tc = getCoords(xl_T)\n",
        "    nc = getCoords(xl)\n",
        "    D = getDist(tc, nc, thr = 8.0)\n",
        "    feats=extract_feats(generate_pair_features(D,xl_T,xl))\n",
        "    return feats\n",
        "def generate_pair_features(dist_info,xl,xr):\n",
        "    prot_dic=make_dic()\n",
        "#    pdb.set_trace()\n",
        "    for rec in dist_info:\n",
        "\n",
        "        try:\n",
        "            l_letter= three_to_one(xl[rec[0]].get_resname())\n",
        "            r_letter= three_to_one(xr[rec[1]].get_resname())\n",
        "#            print(l_letter,l_letter)\n",
        "            if (l_letter,r_letter) in prot_dic.keys():\n",
        "                prot_dic[(l_letter,r_letter)]+=1\n",
        "            elif (r_letter,l_letter) in prot_dic.keys():\n",
        "                prot_dic[(r_letter,l_letter)]+=1\n",
        "        except:\n",
        "            prot_dic[('_','_')]+=1\n",
        "    return prot_dic\n",
        "def getCoords(R):\n",
        "    \"\"\"\n",
        "    Get atom coordinates given a list of biopython residues\n",
        "    \"\"\"\n",
        "    Coords = []\n",
        "    for (idx, r) in enumerate(R):\n",
        "        v = [ak.get_coord() for ak in r.get_list()]\n",
        "        Coords.append(v)\n",
        "    return Coords\n",
        "def InterfaceFeatures(Complexs,pdbloc):\n",
        "    Found =  listdir(pdbloc)\n",
        "    InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "    comp_id=list(set(Complexs))\n",
        "    for ids in range(len(comp_id)):\n",
        "        if comp_id[ids]+'.pdb' in Found:\n",
        "            stx=pdbloc+'/'+comp_id[ids]+'.pdb'#'/2XA0.pdb'\n",
        "            chains=Struct2chain(stx)\n",
        "            for j in range(len(chains)):\n",
        "                Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "                for k in range(j,len(chains)):\n",
        "                    Cname,seq,L,xl=chains[k]\n",
        "                    #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                    name=comp_id[ids]#+'_'+Cname_T+'_2_'+Cname\n",
        "                    Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                    InterfaceF=np.array(Interface)\n",
        "                    InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                    if name not in InterfaceFeatures.keys():\n",
        "                        InterfaceFeatures[name]=Interface\n",
        "    #pickle.dump(InterfaceFeatures, open(path+Filename+\"_InterfaceFeatures.npy\", \"wb\"))\n",
        "    return InterfaceFeatures\n",
        "def getDist(C0, C1, thr=np.inf):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    N0 = []\n",
        "    N1 = []\n",
        "    for i in range(len(C0)):\n",
        "        for j in range(len(C1)):\n",
        "            d = spatial.distance.cdist(C0[i], C1[j]).min()\n",
        "            # dji=spatial.distance.cdist(C1[j], C0[i]).min()\n",
        "            #d=min(dij,dji)\n",
        "            #print d\n",
        "            if (d < thr):  # and not np.isnan(self.Phi[i]) and not np.isnan(self.Phi[j])\n",
        "                N0.append((i, j, d))\n",
        "                N1.append((j, i, d))\n",
        "    return (N0, N1)\n",
        "from Bio import SeqIO\n",
        "from Bio.SeqIO import FastaIO\n",
        "from itertools import product\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from sklearn.preprocessing import normalize\n",
        "import math\n",
        "import numpy as np\n",
        "from Bio.Data import IUPACData\n",
        "from Bio.PDB.Polypeptide import *\n",
        "def prot_feats_seq(seq):\n",
        "    #Interfacedict=pickle.load(open(path+\"InhibitorNewModel2022/InterfaceFeatures2chainsSVM.npy\",\"rb\"))\n",
        "    #InterfaceF=Interfacedict[complexname]\n",
        "    aa=['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "    f=[]\n",
        "    X = ProteinAnalysis(str(seq))\n",
        "    X.molecular_weight() #throws an error if 'X' in sequence. we skip such sequences\n",
        "    p=X.get_amino_acids_percent()\n",
        "    dp=[]\n",
        "    for a in aa:\n",
        "        dp.append(p[a])\n",
        "    dp=np.array(dp)\n",
        "    dp=normalize(np.atleast_2d(dp), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "    f.extend(dp[0])\n",
        "\n",
        "    tm=np.array(twomerFromSeq(str(seq)))\n",
        "    tm=normalize(np.atleast_2d(tm), norm='l2', copy=True, axis=1,return_norm=False)\n",
        "    f.extend(tm[0])\n",
        "    return np.array(f)\n",
        "def Struct2chain(stx):\n",
        "    \"\"\"\n",
        "    Seq: sequence of the chain\n",
        "    seq_L:sequence Length\n",
        "    \"\"\"\n",
        "    p = PDBParser()\n",
        "    L=[]\n",
        "    stx=p.get_structure('X',stx)\n",
        "    for model in stx:\n",
        "        for C in model:\n",
        "            RL=[]\n",
        "            for R in C:\n",
        "                RL.append(R)\n",
        "            pp=PPBuilder().build_peptides(C)\n",
        "            if len(pp)==0:\n",
        "                pp=CaPPBuilder().build_peptides(C)\n",
        "            seq=''.join([str(p.get_sequence()) for p in pp])\n",
        "            #seq=''.join([p.get_sequence().tostring() for p in pp])\n",
        "            seq_L=len(seq)\n",
        "            L.append((C.full_id[2],seq,seq_L,RL))\n",
        "    return L\n",
        "def extract_feats(dic):\n",
        "    feats=[]\n",
        "    key_list=np.load('/content/PPI-Inhibitors/Features/'+'prote_letter_pair_keys.npy')#to keep features order same\n",
        "    for key in key_list:\n",
        "#        pdb.set_trace()\n",
        "        feats.append(dic[(key[0].decode('utf-8'),key[1].decode('utf-8'))])\n",
        "\n",
        "    return feats\n",
        "def make_dic():\n",
        "    prot_dic={}\n",
        "    letters=IUPACData.protein_letters\n",
        "    for i in range(len(letters)):\n",
        "        for j in range(i,len(letters)):\n",
        "            prot_dic[(letters[i],letters[j])]=0.0\n",
        "    prot_dic[('_','_')]=0.0# for Amino acids other than 20 natural\n",
        "    return prot_dic\n",
        "def LoadProtein_SVM_Features(UniqueProtein,Pdbloc):\n",
        "    pdbname=listdir(Pdbloc)\n",
        "    InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "    SequenceFeatures=[];SequenceFeatures=dict(SequenceFeatures)\n",
        "    AllFeatures=[];AllFeatures=dict(AllFeatures)\n",
        "    for  b in range(len(UniqueProtein)):\n",
        "        if UniqueProtein[b]+'.pdb'in pdbname:\n",
        "            stx=Pdbloc+UniqueProtein[b]+'.pdb'#directory+'/2XA0.pdb'#\n",
        "            chains=Struct2chain(stx)\n",
        "            #########Interface Features\n",
        "            for j in range(len(chains)):\n",
        "                Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "                for k in range(j,len(chains)):\n",
        "                    Cname,seq,L,xl=chains[k]\n",
        "                    #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                    name=UniqueProtein[b]#+'_'+Cname_T+'_2_'+Cname\n",
        "                    Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                    seq_TF=prot_feats_seq(seq_T)\n",
        "                    seq_NTF=prot_feats_seq(seq)\n",
        "                    SeQFeatures=(seq_TF+seq_NTF)/2\n",
        "                    InterfaceF=np.array(Interface)\n",
        "                    InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                    if name not in InterfaceFeatures.keys():\n",
        "                        InterfaceFeatures[name]=Interface\n",
        "                        SequenceFeatures[name]=SeQFeatures\n",
        "                        AllFeatures[name]=np.append(SeQFeatures,Interface)\n",
        "    return InterfaceFeatures,SequenceFeatures,AllFeatures\n",
        "def External_GenerateRandomNegative(posexamples):\n",
        "    NegtiveRatio=1\n",
        "    ###SuperDrugbank###Names\n",
        "    SuperdrugNames=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#'approved_drugs_chemical_structure_identifiers.xlsx'\n",
        "    SuperdrugNames=SuperdrugNames[1:]\n",
        "    SuperdrugNames = np.array([s[0] for s in SuperdrugNames])\n",
        "    ###############SuperDrugbank\n",
        "    df_Superdrug=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"C\").values\n",
        "    df_Superdrug=df_Superdrug[1:]\n",
        "    ###\n",
        "    df_Superdrug_Compounds=np.array([c[0] for c in df_Superdrug])#3638\n",
        "    SuperDrug_dict=dict (zip (SuperdrugNames,df_Superdrug_Compounds))\n",
        "    ################\n",
        "    #path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "    DBD5_ProteinData_dict=pickle.load(open('/content/PPI-Inhibitors/Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "    Ubench5CompNames=list (set (list (DBD5_ProteinData_dict.keys())))\n",
        "    ####\n",
        "    AllNeg=[];AllPos=[];complex_ligand_dict={};\n",
        "    for key,val in  posexamples:\n",
        "      #print(key,val,posexamples[key,val][1])\n",
        "      if key not in complex_ligand_dict:\n",
        "        complex_ligand_dict[key]=posexamples[key,val][1]\n",
        "      else:\n",
        "        #print(\"else\",key,val)\n",
        "        complex_ligand_dict[key]=np.append( complex_ligand_dict.get(key, ()) ,posexamples[key,val][1])\n",
        "    Complexnames=list (complex_ligand_dict.keys())\n",
        "    totalcomp=list (set (complex_ligand_dict.keys()))\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "        #print(origanlL)\n",
        "        #print(\"complexname=\",everycomp,\"origanlInhibitors\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        #print(pos)\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        AllPos.extend(pos)\n",
        "        while (len(negs)<NN):# and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            LigandR = random.choice(SuperdrugNames)\n",
        "            LigandR_smile=SuperDrug_dict[LigandR]\n",
        "            Npair=((everycomp,LigandR_smile))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(LigandR_smile) is not None:\n",
        "                negs.append(Npair)\n",
        "#                print(\"Npair SuperdrugNames\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "    #print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    #print(\"second method Cr\")\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):\n",
        "            for everyL in origanlL:\n",
        "                ComplexR = random.choice(Ubench5CompNames)\n",
        "                Npair=((ComplexR,everyL))\n",
        "                if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "    #                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                    negs.append(Npair)\n",
        "        AllNeg.extend(negs)\n",
        "        ###################\n",
        "    #print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    return np.array(AllPos),np.array(AllNeg),SuperDrug_dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "def PredictScorefromFile(filename,Pdbloc,Pscaler,Cscaler,trainedModel_IPPI,train_GNN,LOCOcomplexname):\n",
        "  githubpath='/content/PPI-Inhibitors/'\n",
        "#filename,Pdbloc,Pscaler,Cscaler,trainedModel_IPPI,train_GNN=githubpath+'Data/External data/2dyh_all.txt',githubpath+'Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  InhibitedComp=[];PdbId=[];Ligandnames=[];SMILES=[];targets=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      Pdbid,inhibtedc,Ligandid,smiles = d.split()\n",
        "      if getFP(smiles) is not None:\n",
        "        PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);InhibitedComp.append(inhibtedc);#labels.append(float (y));\n",
        "  ################\n",
        "  \"\"\"\n",
        "  Result_dict={}\n",
        "  pos=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "  Pos,Negs,SuperDrug_dict=External_GenerateRandomNegative(pos)\n",
        "  poslabel=1.0*np.ones(len(Pos));neglabel=-1.0*np.ones(len(Negs));targets=np.append(poslabel,neglabel )\n",
        "  All_examples=[];All_examples.extend(Pos);All_examples.extend(Negs)\n",
        "  #Write File for External\n",
        "  External_All_Examples=open('/content/drive/MyDrive/GNN-PPI-Inhibitor/_'+LOCOcomplexname+'_'+filename.split('.txt')[0].split('/')[-1]+'_External_All_Examples.txt',\"w\")\n",
        "  \"\"\"\n",
        "  complexnames=[];SMILES=[];targets=[];\n",
        "  #/content/PPI-Inhibitors/Data/External data/2dyh_all_External_All_Examples.txt\n",
        "  with open('/content/PPI-Inhibitors/Data/External data/'+filename.split('.txt')[0].split('/')[-1]+'_External_All_Examples.txt') as f:\n",
        "    D = f.readlines()\n",
        "  for d in tqdm(D):\n",
        "      complexname,smiles,target= d.split()\n",
        "      complexnames.append(complexname);SMILES.append(smiles);targets.append(target);#All_examples.append()\n",
        "  pdbname=listdir(Pdbloc);mypdb=[]\n",
        "  for p in pdbname:\n",
        "    if p.split('.pdb')[0] in Pdbid:\n",
        "      mypdb.append(p)\n",
        "  UniqueProtein=list (set (mypdb))\n",
        "  External_Protein_GNN_Data_dict=PrepairDataset.processProtein(UniqueProtein,Pdbloc)\n",
        "  ##########for Seq+interface features\n",
        "  #pdbname=listdir(Pdbloc)\n",
        "  s,i,External_ProteinSeqandInterfaceData_dict=LoadProtein_SVM_Features(UniqueProtein,Pdbloc)\n",
        "  ##############3 for sequence fedatures of DBD5 pdb's\n",
        "  Ubench5InterfaceandSeq_dict=pickle.load(open(githubpath+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "  All_External_ProteinSeqandInterfaceData_dict=dict( list (External_ProteinSeqandInterfaceData_dict.items())+list (Ubench5InterfaceandSeq_dict.items()))\n",
        "  #Testing\n",
        "  DBD5_Protein_GNN_Data_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "  #Pos_ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "  All_Protein_GNN_Data_dict=dict( list (External_Protein_GNN_Data_dict.items())+list (DBD5_Protein_GNN_Data_dict.items()))\n",
        "  for d in All_Protein_GNN_Data_dict:\n",
        "    data=All_Protein_GNN_Data_dict[d]\n",
        "    All_Protein_GNN_Data_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "  #####################\n",
        "  Cttname=[];Ctt=[];Pttname=[];Ptt=[];\n",
        "  for (complexname,ligandsmile) in zip(complexnames,SMILES):#All_examples:#\n",
        "    Cttname.append(ligandsmile);Ctt.append(getFP(ligandsmile));\n",
        "    Pttname.append(complexname);Ptt.append(All_External_ProteinSeqandInterfaceData_dict[complexname][0:69]);\n",
        "  #standarization\n",
        "\n",
        "  Ctt = Cscaler.transform(Ctt)\n",
        "  Cttdict=dict (zip (Cttname,torch.FloatTensor( Ctt).cuda()))\n",
        "  Ptt = Pscaler.transform(Ptt)\n",
        "  Pttdict=dict (zip (Pttname,torch.FloatTensor( Ptt).cuda()))\n",
        "  #########\n",
        "  Y_t,Z,Targets=[],[],[]\n",
        "  for target,(complexname,ligandsmile) in zip(targets,zip(complexnames,SMILES)):#zip(targets,All_examples):#\n",
        "    #target=targets[nt]\n",
        "    #IPPI_Net(All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname],Pttdict[GNNcomplex],GNN_model)\n",
        "    #External_All_Examples.write(complexname+' '+ligandsmile+' '+str(target)+'\\n')\n",
        "    test_score=trainedModel_IPPI(All_Protein_GNN_Data_dict[complexname],Cttdict[ligandsmile],Pttdict[complexname],train_GNN)\n",
        "    #print (test_score)\n",
        "    test_score=test_score.cpu().data.numpy()[0]\n",
        "    Z.append(test_score);Targets.append(float (target))\n",
        "    #Result_dict[(complexname,Ligandname)]=test_score\n",
        "  return Z,Targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0NjGHV8ewJC"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZfmspC7D6D",
        "outputId": "da7b64f4-aa2d-4327-ee58-73a96f52c5e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15695/15695 [00:00<00:00, 248112.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test complex  2XA0\n",
            "test complex  3WN7\n",
            "test complex  3UVW\n",
            "test complex  1YCR\n",
            "test complex  4ESG\n",
            "test complex  3D9T\n",
            "test complex  2FLU\n",
            "test complex  4QC3\n",
            "test complex  2RNY\n",
            "test complex  4AJY\n",
            "test complex  2E3K\n",
            "test complex  2B4J\n",
            "test complex  1YCQ\n",
            "test complex  3DAB\n",
            "test complex  4GQ6\n",
            "test complex  1NW9\n",
            "test complex  4YY6\n",
            "test complex  3TDU\n",
            "test complex  1BKD\n",
            "test complex  1BXL\n",
            "test complex  1Z92\n",
            "Epoch 0 Batch loss 1.152987\n",
            "LOCOcomplexname 1Z92 Epoch 0 auc PR 0.631 0.265 loss 0.113\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/24 [00:00<?, ?it/s][21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:58:59] DEPRECATION WARNING: please use MorganGenerator\n",
            "100%|██████████| 24/24 [00:00<00:00, 861.11it/s]\n",
            "100%|██████████| 72/72 [00:00<00:00, 616305.89it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 2889.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain B is discontinuous at line 3220.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:05] DEPRECATION WARNING: please use MorganGenerator\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "External_Auc,PR 0.458 0.349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/28 [00:00<?, ?it/s][21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11 12 13\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11 12 13\n",
            "[21:59:07] Can't kekulize mol.  Unkekulized atoms: 5 6 7 8 9 10 11 12 13\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] Explicit valence for atom # 0 Cl, 1, is greater than permitted\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:07] DEPRECATION WARNING: please use MorganGenerator\n",
            "100%|██████████| 28/28 [00:00<00:00, 2465.22it/s]\n",
            "100%|██████████| 72/72 [00:00<00:00, 585251.72it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 6961.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain E is discontinuous at line 7019.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain A is discontinuous at line 7033.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/PDB/StructureBuilder.py:100: PDBConstructionWarning: WARNING: Chain E is discontinuous at line 7104.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/Bio/SeqUtils/ProtParam.py:106: BiopythonDeprecationWarning: The get_amino_acids_percent method has been deprecated and will likely be removed from Biopython in the near future. Please use the amino_acids_percent attribute instead.\n",
            "  warnings.warn(\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n",
            "[21:59:21] DEPRECATION WARNING: please use MorganGenerator\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covid19_External_Auc,PR 0.511 0.371\n",
            "Epoch 1 Batch loss 0.7964375\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "class IPPI_MLP_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IPPI_MLP_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2560, 512)#1024)\n",
        "        #self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 100)\n",
        "        self.fc6 = nn.Linear(100, 1)\n",
        "    def forward(self, proteinF,LigandFeatures,ProteinInterfaceF,GNN_model):\n",
        "          #call GNN for protein\n",
        "          #result,PFeatures=GNN_model(proteinF)\n",
        "          PFeatures=GNN_model(proteinF)\n",
        "          #print(PFeatures[0])\n",
        "          Cfeatures=LigandFeatures#torch.FloatTensor()#.cuda()#Compound_Net(LigandFeatures)\n",
        "          P_all_Features=PFeatures[0]#torch.hstack((PFeatures[0])#,ProteinInterfaceF))\n",
        "          PC_Features=torch.hstack((P_all_Features,Cfeatures))\n",
        "          x = torch.tanh(self.fc1(PC_Features))#.to('cuda:0')\n",
        "         # x = torch.tanh(self.fc2(x))#.to('cuda:1')\n",
        "          x = torch.tanh(self.fc3(x))#.to('cuda:2')\n",
        "          #x = self.fc5(x)\n",
        "          x = self.fc6(x)\n",
        "          return x\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score\n",
        "#For End to End LEarning\n",
        "import pickle\n",
        "path,githubpath='/content/drive/MyDrive/GNN-PPI-Inhibitor/','/content/PPI-Inhibitors/'\n",
        "Ubench5InterfaceandSeq_dict=pickle.load(open(githubpath+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "Pos_seqandInterfaceF_dict=pickle.load(open(githubpath+'Features/Pos_seqandInterfaceF_dict.npy',\"rb\"))\n",
        "Complex_AllFeatures_dict=dict( list (Pos_seqandInterfaceF_dict.items())+list (Ubench5InterfaceandSeq_dict.items()))\n",
        "##############\n",
        "ComplexInterfaceFeatures={}\n",
        "for key in Complex_AllFeatures_dict:\n",
        "  if len(key.split('_'))>1:\n",
        "    compname=key.split('_')[0]\n",
        "    ComplexInterfaceFeatures[compname]=Complex_AllFeatures_dict[key]#.cuda()\n",
        "  else:\n",
        "    ComplexInterfaceFeatures[key]=Complex_AllFeatures_dict[key]#.cuda()\n",
        "#################################\n",
        "CompoundFingerprintFeaturesDict=pickle.load(open(githubpath+'Features/Compound_Fingerprint_Features_Dict.npy',\"rb\"))\n",
        "#Load Protein data for GNN\n",
        "ProteinDataGNN_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "DBD5_ProteinDataGNN_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "All_ProteinData_dict=dict( list (ProteinDataGNN_dict.items())+list (DBD5_ProteinDataGNN_dict.items()))\n",
        "for d in All_ProteinData_dict:\n",
        "  data=All_ProteinData_dict[d]\n",
        "  All_ProteinData_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "#########\n",
        "from tqdm import tqdm as tqdm\n",
        "import pickle\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "###########\n",
        "#with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN_Binary.txt') as f:\n",
        "with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN.txt') as f:\n",
        "    D = f.readlines()\n",
        "Labels=[];Ligandnames=[];Complexs=[];TestPoscomplexes=[];#SMILESlist=[];\n",
        "for d in tqdm(D):\n",
        "  if len(d.split())==4:\n",
        "      TestPoscomp,Complexname,Ligandname,label = d.split()\n",
        "  else:\n",
        "      TestPoscomp,Complexname,Ligandname,label = d.split()[0],d.split()[1],(' ').join(d.split()[2:-1]),d.split()[-1]\n",
        "  TestPoscomplexes.append(TestPoscomp),Ligandnames.append(Ligandname);Complexs.append(Complexname);Labels.append(float (label))\n",
        "##\n",
        "#########Make dictionary, Rootcomplexname=(complexname,compoundname),label\n",
        "Allexamples=dict (zip(zip(TestPoscomplexes,zip(Complexs,Ligandnames)),Labels))\n",
        "Alldata=list (Allexamples.keys())\n",
        "KK=[k[0].split('_')[0] for k in Alldata]\n",
        "# groups = pd.DataFrame(KK) # <-- REMOVE THIS LINE. You don't need it.\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "from torchmetrics.classification import BinaryHingeLoss\n",
        "AUC_ROC_final=[];Avg_P_final=[];Z=[];Yo=[];Y_t=[];Y_score=[];\n",
        "from os import listdir\n",
        "AlltestExamples=[];Externallabels=[];ExternalscoresLOCO=[];covid19_Externallabels=[];covid19_ExternalscoresLOCO=[];Y_score=[];Y_t=[];classratio_dict={};\n",
        "AUC_ROC_final=[];Avg_P_final=[];\n",
        "Complexs,Ligandnames, Labels=np.array(Complexs),np.array(Ligandnames),np.array(Labels)\n",
        "# Pass the 1D list 'KK' directly to the 'groups' parameter\n",
        "Alldata=np.array(Alldata, dtype=object)\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=KK):\n",
        "    train,test=Alldata[trainindex],Alldata[testindex]\n",
        "    Ctr=[];Ptr=[];y_train=[];Ctrname=[];Ptrname=[];Xtr=[];G=[];Cttname=[];Ctt=[];y_test=[];Ptt=[];Pttname=[];\n",
        "    #Split train and test\n",
        "    for t in train:\n",
        "        Ctrname.append(t[1][1]);Ctr.append(CompoundFingerprintFeaturesDict[t[1][1]]);\n",
        "        #change this only for GNN Complex_AllFeatures_dict with All_ProteinData_dic and t\n",
        "        #####\n",
        "        GNNcomp=t[1][0].split('_')[0]#t[1][0].split('_')[0]\n",
        "        SeqonlyF=ComplexInterfaceFeatures[GNNcomp][0:69]\n",
        "        Ptrname.append(GNNcomp);Ptr.append(SeqonlyF);\n",
        "        #print (ComplexInterfaceFeatures[GNNcomp])\n",
        "        #s,i,External_ProteinSeqandInterfaceData_dict=LoadProtein_SVM_Features([GNNcomp],githubpath+'Data/Pdb/')#UniqueProtein,Pdbloc)\n",
        "        #1/0\n",
        "        y_train.append(Allexamples[t[0],t[1]])\n",
        "    #Split train and test\n",
        "    for t in test:\n",
        "        GNNcomp=t[1][0].split('_')[0]\n",
        "        Cttname.append(t[1][1]);Ctt.append(CompoundFingerprintFeaturesDict[t[1][1]]);\n",
        "        SeqonlyF=ComplexInterfaceFeatures[GNNcomp][0:69]\n",
        "        Pttname.append(GNNcomp);Ptt.append(SeqonlyF);#ComplexInterfaceFeatures[GNNcomp]);\n",
        "        y_test.append(Allexamples[t[0],t[1]])\n",
        "    #standarization\n",
        "    Pscaler = StandardScaler().fit(Ptr)\n",
        "    Cscaler = StandardScaler().fit(Ctr)\n",
        "    Ctr = Cscaler.transform(Ctr)\n",
        "    Ptr=Pscaler.transform(Ptr)\n",
        "    Ptt=Pscaler.transform(Ptt)\n",
        "    Ptrdict=dict (zip(Ptrname,torch.FloatTensor(Ptr).cuda()))\n",
        "    Ctrdict=dict (zip (Ctrname,torch.FloatTensor( Ctr).cuda()))\n",
        "    Ctt = Cscaler.transform(Ctt)\n",
        "    Cttdict=dict (zip (Cttname,torch.FloatTensor( Ctt).cuda()))\n",
        "    Pttdict=dict (zip(Pttname,torch.FloatTensor(Ptt).cuda()))\n",
        "    ######################################3\n",
        "    #CompoundNet=Compound_Net().cuda()7\n",
        "    ################7\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #############\n",
        "    GNN_model=GNN().cuda()\n",
        "    ############\n",
        "    \"\"\"\n",
        "    Mcomplexname,Mepoch='3TDU','10'\n",
        "    IPPI_Net.load_state_dict(torch.load(path+'IPPI_Net_'+ Mcomplexname+'_Epoch'+str (Mepoch)))\n",
        "    GNN_model.load_state_dict(torch.load(path+'GNN_model_'+ Mcomplexname+'_Epoch'+str (Mepoch)))\n",
        "    #############\n",
        "    \"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    #criterion = BinaryHingeLoss().cuda()\n",
        "    optimizer = optim.Adam(list (IPPI_Net.parameters()) + list( GNN_model.parameters()),lr=0.0001,weight_decay=0.0)#0001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    ####\n",
        "    print (\"test complex \",test[0][0].split('_')[0] )\n",
        "    #Done=['3UVW','2B4J','2RNY','4AJY','1YCR','4QC3','1NW9','2E3K','4GQ6','4YY6']\n",
        "    Done=['4GQ6','1BXL','4ESG','2FLU','1YCQ','2XA0','3D9T','2B4J','2RNY','4AJY', '1YCR','4QC3','1BKD','1NW9','2E3K','4YY6','4GQ6','3UVW','3DAB','3WN7','1F47','3TDU']#,'1Z92'\n",
        "    Batchlosslist=[]\n",
        "    ################load classratio_dict based on rrot complex\n",
        "    classratio_dict=pickle.load(open(githubpath+'Features/Classratio_GNNdict.npy','rb'))\n",
        "    Mepoch=0\n",
        "    if test[0][0].split('_')[0] in Done:\n",
        "      continue\n",
        "    \"\"\"\n",
        "    if test[0][0].split('_')[0]!=Mcomplexname:\n",
        "      continue\n",
        "    \"\"\"\n",
        "    for epoch in range(30):\n",
        "      total_preds = torch.Tensor()\n",
        "      total_labels = torch.Tensor()\n",
        "      Batchloss=0\n",
        "      epoch=epoch+int (Mepoch)\n",
        "      #output=IPPI_Net(train)\n",
        "      for n in range(len(train)):\n",
        "        #print (train[n])\n",
        "        complexname,Ligandname =train[n][1]\n",
        "        GNNcomplex=complexname.split('_')[0]\n",
        "        #output=IPPI_Net(GNN_model,All_ProteinData_dict[GNNcomplex],complexname,CompoundNet,CompoundFingerprintFeaturesDict[Ligandname])\n",
        "        #standrized\n",
        "        output=IPPI_Net(All_ProteinData_dict[GNNcomplex],Ctrdict[Ligandname],Ptrdict[GNNcomplex],GNN_model)\n",
        "        #loss = criterion(output, y_train[n].view(-1))AQQAQ\n",
        "        loss=criterion(output, y_train[n].reshape(1))\n",
        "        #print(\"loss\",loss.cpu().data.numpy())\n",
        "        if y_train[n]==1.0:\n",
        "              #print(train[n][0],classratio_dict[train[n][0]])\n",
        "              loss=classratio_dict[GNNcomplex]*loss\n",
        "        Batchloss=Batchloss+loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #params = list (IPPI_Net.parameters()) + list( GNN_model.parameters()) + list (CompoundNet.parameters())\n",
        "        #print(train[n][1],\"loss\",loss.cpu().data.numpy())\n",
        "      #############\n",
        "      Batchlosslist.append(Batchloss.cpu().data.numpy()/len(train))\n",
        "      print(\"Epoch\",epoch,\"Batch loss\",Batchloss.cpu().data.numpy()/len(train))\n",
        "      Y_t,Y_score=[],[]\n",
        "      if epoch%5==0 or Batchloss.cpu().data.numpy()/len(train)<0.1:\n",
        "        #Testing\n",
        "        Y_score,Y_t=[],[]\n",
        "        for nt in range(len(test)):\n",
        "          complexname,Ligandname =test[nt][1]\n",
        "          GNNcomplex=complexname.split('_')[0]\n",
        "          test_score=IPPI_Net(All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname],Pttdict[GNNcomplex],GNN_model)\n",
        "          Y_score.extend(test_score.cpu().data.numpy())\n",
        "          Y_t.append(y_test[nt])\n",
        "        auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "        average_PR=average_precision_score(Y_t,Y_score)\n",
        "        LOCOcomplexname=test[0][0].split('_')[0]\n",
        "        print(\"LOCOcomplexname\",LOCOcomplexname,\"Epoch\",epoch,\"auc\",\"PR\",auc_roc.round(3),average_PR.round(3),\"loss\",loss.cpu().data.numpy().round(3))\n",
        "        file_model = path+'/IPPI_Net_'+ LOCOcomplexname+'_Epoch'+str (epoch)\n",
        "        torch.save(IPPI_Net.state_dict(), file_model)\n",
        "        file_model = path+'/GNN_model_'+ LOCOcomplexname+'_Epoch'+str (epoch)\n",
        "        torch.save(GNN_model.state_dict(), file_model)\n",
        "        #print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "        #(filename,Pdbloc,Pscaler,Cscaler,trainedModel_IPPI,train_GNN,LOCOcomplexname)\n",
        "        External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model,LOCOcomplexname)\n",
        "        ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "        External_Auc= roc_auc_score(External_labels, External_score)\n",
        "        External_AP=average_precision_score(External_labels, External_score)\n",
        "        print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "        #########\\\n",
        "\n",
        "        #covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "        Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model,LOCOcomplexname)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "        covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "        Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "        ###\n",
        "        Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "        print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))\n",
        "        if Batchloss.cpu().data.numpy()/len(train)<0.1:\\\n",
        "          break;\n",
        "      ##########\n",
        "    #Testing\n",
        "    Y_score,Y_t=[],[]\n",
        "    for nt in range(len(test)):\n",
        "      complexname,Ligandname =test[nt][1]\n",
        "      GNNcomplex=complexname.split('_')[0]\n",
        "      test_score=IPPI_Net(All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname],Pttdict[GNNcomplex],GNN_model)\n",
        "      Y_score.extend(test_score.cpu().data.numpy())\n",
        "      Y_t.append(y_test[nt])\n",
        "    TestComplex=test[0][0]\n",
        "    Z=np.append(Z,Y_score);Yo=np.append(Yo,Y_t);\n",
        "    Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "    average_P_score=average_precision_score(Y_t,Y_score)\n",
        "    np.save(path+TestComplex+'_Loss.npy''_Epoch'+str (epoch),Batchlosslist)\n",
        "    #np.save (path+TestComplex+'_AUC_list.npy''_Epoch'+str (epoch),AUC_list)\n",
        "    #np.save (path+ TestComplex+'_PR_list.npy''_Epoch'+str (epoch),PR_list)\n",
        "    ####\n",
        "    file_model = path+'/IPPI_Net_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "    torch.save(IPPI_Net.state_dict(), file_model)\n",
        "    file_model = path+'/GNN_model_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "    torch.save(GNN_model.state_dict(), file_model)\n",
        "    Y_t=np.array(Y_t)\n",
        "    #print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "    print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "    #####################External\n",
        "    External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model,LOCOcomplexname)\n",
        "    #Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "    #print (\"RFPP\",RFPP_all)\n",
        "    ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "    External_Auc= roc_auc_score(External_labels, External_score)\n",
        "    External_AP=average_precision_score(External_labels, External_score)\n",
        "    print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "    #########\\\n",
        "    #covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "    Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model,LOCOcomplexname)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "    covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "    Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "    ###\n",
        "    Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "    print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")\n",
        "##########\n",
        "External_fpr, External_tpr, External_thresholds = roc_curve(Externallabels,ExternalscoresLOCO)\n",
        "External_Auc = roc_auc_score(Externallabels,ExternalscoresLOCO)\n",
        "External_Auc=(External_Auc).round(2)\n",
        "fig = plt.figure()\n",
        "plt.plot(External_fpr, External_tpr,color='k',marker='d',label='External_Auc:{: .2f}'.format(External_Auc))\n",
        "plt.title('AUCROC External');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUCROC External SVM PPI Inhibitors Random and 2 Times Binders combine Negative.pdf\", bbox_inches='tight')\n",
        "#######\n",
        "covid19_External_fpr, covid19_External_tpr, covid19_External_thresholds = roc_curve(covid19_Externallabels,covid19_ExternalscoresLOCO)\n",
        "covid19_External_Auc = roc_auc_score(covid19_Externallabels,covid19_ExternalscoresLOCO)\n",
        "covid19_External_Auc=(covid19_External_Auc).round(2)\n",
        "fig = plt.figure()\n",
        "plt.plot(covid19_External_fpr, covid19_External_tpr,color='k',marker='d',label='covid19_External_Auc:{: .2f}'.format(covid19_External_Auc))\n",
        "plt.title('covid19 AUCROC External');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUCROC covid19 External SVM PPI Inhibitors Random and 2 Times Binders combine Negative.pdf\", bbox_inches='tight')\n",
        "########\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr Over all complexes \\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])))#,\"\\n\")\n",
        "#######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUCROC PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "print(\"averge score of test\",np.average(Tscores_list))\n",
        "\"\"\"\n",
        "np.save(path+'All_loss.npy',All_loss)\n",
        "np.save (path+ 'AUC_list.npy',AUC_list)\n",
        "np.save (path+ 'PR_list.npy',PR_list)\n",
        "fig=plt.figure()\n",
        "plt.plot(All_loss,color='k',marker=',',label='Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "fig .savefig(path+\"PPI Inhibitors loss over all complexes 30 epochs.pdf\", bbox_inches='tight')\n",
        "############\n",
        "fig=plt.figure()\n",
        "plt.plot(All_loss,color='k',marker=',',label='Loss')\n",
        "plt.plot( AUC_list,color='b',marker=',',label='AUC')\n",
        "plt.plot( PR_list,color='m',marker=',',label='PR')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "fig .savefig(path+\"PPI Inhibitors loss-PR over all complexes.pdf\", bbox_inches='tight')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFCNvL4yv-Ao"
      },
      "outputs": [],
      "source": [
        "External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model,LOCOcomplexname)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "#########\\\n",
        "#covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model,LOCOcomplexname)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CYhVz7KSNb"
      },
      "outputs": [],
      "source": [
        "seqF,Interface,External_ProteinSeqandInterfaceData_dict=LoadProtein_SVM_Features(GNNcomp,githubpath+'Data/Pdb/')#UniqueProtein,Pdbloc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5MqZyHbKata"
      },
      "outputs": [],
      "source": [
        "LoadProtein_SVM_Features(GNNcomp,githubpath+'Data/Pdb/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EllhLn9m303h"
      },
      "outputs": [],
      "source": [
        "#def LoadProtein_SVM_Features(UniqueProtein,Pdbloc):\n",
        "UniqueProtein,Pdbloc=[GNNcomp],githubpath+'Data/Pdb/'\n",
        "pdbname=listdir(Pdbloc)\n",
        "InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "SequenceFeatures=[];SequenceFeatures=dict(SequenceFeatures)\n",
        "AllFeatures=[];AllFeatures=dict(AllFeatures)\n",
        "for  b in range(len(UniqueProtein)):\n",
        "    if UniqueProtein[b]+'.pdb'in pdbname:\n",
        "        stx=Pdbloc+UniqueProtein[b]+'.pdb'#directory+'/2XA0.pdb'#\n",
        "        chains=Struct2chain(stx)\n",
        "        #########Interface Features\n",
        "        for j in range(len(chains)):\n",
        "            Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "            for k in range(j,len(chains)):\n",
        "                Cname,seq,L,xl=chains[k]\n",
        "                #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                name=UniqueProtein[b]#+'_'+Cname_T+'_2_'+Cname\n",
        "                Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                seq_TF=prot_feats_seq(seq_T)\n",
        "                seq_NTF=prot_feats_seq(seq)\n",
        "                SeQFeatures=(seq_TF+seq_NTF)/2\n",
        "                InterfaceF=np.array(Interface)\n",
        "                InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                if name not in InterfaceFeatures.keys():\n",
        "                    InterfaceFeatures[name]=Interface\n",
        "                    SequenceFeatures[name]=SeQFeatures\n",
        "                    AllFeatures[name]=np.append(SeQFeatures,Interface)\n",
        "    #return InterfaceFeatures,SequenceFeatures,AllFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_PTn1pX1rzg"
      },
      "outputs": [],
      "source": [
        "len(SequenceFeatures['3UVW'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT5FHagA2KYv"
      },
      "outputs": [],
      "source": [
        "t.split('_')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alZXhEtIsCSZ"
      },
      "outputs": [],
      "source": [
        "ComplexInterfaceFeatures={}\n",
        "for key in Complex_AllFeatures_dict:\n",
        "  if len(key.split('_'))>1:\n",
        "    compname=key.split('_')[0]\n",
        "    ComplexInterfaceFeatures[compname]=Complex_AllFeatures_dict[key]\n",
        "  else:\n",
        "    ComplexInterfaceFeatures[key]=Complex_AllFeatures_dict[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0hKsuZquY-F"
      },
      "outputs": [],
      "source": [
        "Ptrdict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3JfpKx7rDTR"
      },
      "outputs": [],
      "source": [
        "#####Testing\n",
        "Y_score,Y_t=[],[]\n",
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt][1]\n",
        "  GNNcomplex=complexname.split('_')[0]\n",
        "  test_score=IPPI_Net(All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname],GNN_model)\n",
        "  Y_score.extend(test_score.cpu().data.numpy())\n",
        "  Y_t.append(y_test[nt])\n",
        "TestComplex=test[0][0]\n",
        "Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "average_P_score=average_precision_score(Y_t,Y_score)\n",
        "TestComplex=test[0][0].split('_')[0]\n",
        "file_model = path+'/IPPI_Net_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "torch.save(IPPI_Net.state_dict(), file_model)\n",
        "file_model = path+'/GNN_model_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "torch.save(GNN_model.state_dict(), file_model)\n",
        "Y_t=np.array(Y_t)\n",
        "#print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Nvo-7PebeZ"
      },
      "outputs": [],
      "source": [
        "file_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCu_Kaud8j1N"
      },
      "outputs": [],
      "source": [
        "#####################External\n",
        "Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)\n",
        "#Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "#print (\"RFPP\",RFPP_all)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "#########\\\n",
        "#covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqGRygHxEOy8"
      },
      "outputs": [],
      "source": [
        "#Testing\n",
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt][1]\n",
        "  GNNcomplex=complexname.split('_')[0]\n",
        "  test_score=IPPI_Net(GNN_model,All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname])\n",
        "  Y_score.extend(test_score.cpu().data.numpy())\n",
        "  Y_t.append(y_test[nt])\n",
        "TestComplex=test[0][0]\n",
        "Z=np.append(Z,Y_score);Yo=np.append(Yo,Y_t);\n",
        "Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "average_P_score=average_precision_score(Y_t,Y_score)\n",
        "np.save(path+TestComplex+'_Loss.npy''_Epoch'+str (epoch),Batchlosslist)\n",
        "#np.save (path+TestComplex+'_AUC_list.npy''_Epoch'+str (epoch),AUC_list)\n",
        "#np.save (path+ TestComplex+'_PR_list.npy''_Epoch'+str (epoch),PR_list)\n",
        "####\n",
        "file_model = path+'/IPPI_Net_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "torch.save(IPPI_Net.state_dict(), file_model)\n",
        "file_model = path+'/GNN_model_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "torch.save(GNN_model.state_dict(), file_model)\n",
        "Y_t=np.array(Y_t)\n",
        "#print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####################External\n",
        "Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)\n",
        "#Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "#print (\"RFPP\",RFPP_all)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "#########\\\n",
        "#covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")\n",
        "##########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT79QXu2MhLX"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZQ9Ua7UQB4i"
      },
      "outputs": [],
      "source": [
        "classratio_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CQSUp-SKNwp"
      },
      "outputs": [],
      "source": [
        "classratio_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1HTPrWNvT-n"
      },
      "outputs": [],
      "source": [
        "plt.plot(Batchlosslist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SbBr9HHfpek"
      },
      "outputs": [],
      "source": [
        "AUC_ROC_final=[];Avg_P_final=[];Z=[];Yo=[];Y_t=[];Y_score=[];\n",
        "from os import listdir\n",
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt][1]\n",
        "  GNNcomplex=complexname.split('_')[0]\n",
        "  test_score=IPPI_Net(GNN_model,All_ProteinData_dict[GNNcomplex],complexname,CompoundNet,Cttdict[Ligandname])\n",
        "  Y_score.extend(test_score.cpu().data.numpy())\n",
        "  Y_t.append(y_test[nt])\n",
        "TestComplex=test[0][0]\n",
        "Z=np.append(Z,Y_score);Yo=np.append(Yo,Y_t);\n",
        "Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "average_P_score=average_precision_score(Y_t,Y_score)\n",
        "Y_t=np.array(Y_t)\n",
        "#print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####################External\n",
        "Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,CompoundNet,GNN_model)\n",
        "#Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "#print (\"RFPP\",RFPP_all)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "#########\\\n",
        "#covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,CompoundNet,GNN_model)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7T1TKsxTpx3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiIWaAftSZfc"
      },
      "outputs": [],
      "source": [
        "loss = criterion(output, y_train[n].reshape(1)[:,]>0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7acFDtz-J8V"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkqv80gKSITV"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cSPgJA_RJAh"
      },
      "outputs": [],
      "source": [
        "y_train[n].reshape(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlXAJSuSTNLB"
      },
      "outputs": [],
      "source": [
        "train[n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mC-I6xNQXua"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phtDWlTwP8E3"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-CSPe-EP-wd"
      },
      "outputs": [],
      "source": [
        "y_train[n].view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEGfRHfa6ErJ"
      },
      "outputs": [],
      "source": [
        "y_train[n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1w3BvZK6PMs"
      },
      "outputs": [],
      "source": [
        "y_train[n-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqCGBnCpMfTz"
      },
      "outputs": [],
      "source": [
        "Batchloss=Batchloss+loss.cpu().data.numpy()\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "#params = list (IPPI_Net.parameters()) + list( GNN_model.parameters()) + list (CompoundNet.parameters())\n",
        "print(\"Epoch\",epoch,\"loss\",loss.cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzcKx9mJC0KR"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvptbH8vAMPp"
      },
      "outputs": [],
      "source": [
        "loss.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlThYaMgC806"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCDE8O4BAP3O"
      },
      "outputs": [],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7h6lFd4B6gj"
      },
      "outputs": [],
      "source": [
        "target=y_train[n].reshape(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3StYc19dvjO"
      },
      "outputs": [],
      "source": [
        "loss = criterion(total_preds, total_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij9ganBBLmCE"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkKLFRSMd18j"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.classification import BinaryHingeLoss\n",
        "bhl = BinaryHingeLoss()\n",
        "loss=bhl(total_preds,total_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVX4BOTIf6-t"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-T5WjSOdWaH"
      },
      "outputs": [],
      "source": [
        "total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmOoli43dZ9t"
      },
      "outputs": [],
      "source": [
        "total_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtSxXjU2UPU7"
      },
      "outputs": [],
      "source": [
        "total_labels[:,0]>0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv0je1kzEiud"
      },
      "outputs": [],
      "source": [
        "Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)\n",
        "#Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "#print (\"RFPP\",RFPP_all)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "#########\\\n",
        "#covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlvBSu7wFPG4"
      },
      "outputs": [],
      "source": [
        "Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvS2DIUjDezY"
      },
      "outputs": [],
      "source": [
        "Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsWhD0OBk0d-"
      },
      "outputs": [],
      "source": [
        "ProteinDataGNN_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYHhMYlua_Re"
      },
      "outputs": [],
      "source": [
        "CompoundFingerprintFeaturesDict[Ligandname]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BBzoOFEcHzO"
      },
      "outputs": [],
      "source": [
        "Ligandname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d91rJeHlYBsg"
      },
      "outputs": [],
      "source": [
        "All_ProteinData_dict[GNNcomplex]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO4TS0sPUGQ2"
      },
      "outputs": [],
      "source": [
        "CompoundFingerprintFeaturesDict[Ligandname]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23VmclR6cbyI"
      },
      "outputs": [],
      "source": [
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt][1]\n",
        "  GNNcomplex=complexname.split('_')[0]\n",
        "  test_score=IPPI_Net(GNN_model,All_ProteinData_dict[GNNcomplex],complexname,CompoundFingerprintFeaturesDict[Ligandname])\n",
        "  Y_score.append(test_score.cpu().data.numpy())\n",
        "  Y_t.append(y_test[nt])\n",
        "auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "average_PR=average_precision_score(Y_t,Y_score)\n",
        "print(\"Epoch\",epoch,\"auc\",\"PR\",auc_roc.round(3),average_PR.round(3),\"loss\",loss.cpu().data.numpy().round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RH--Lt4bCln"
      },
      "outputs": [],
      "source": [
        "Covid19_RFPP_all=PredictRFPPfromFile(path+'HansonACE2hits.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Covid19_External_score,Covid19_External_labels=PredictScorefromFile(path+'HansonACE2hits.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR,RFPP\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3),Covid19_RFPP_all,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyMmWZ7rcZa7"
      },
      "outputs": [],
      "source": [
        "PredictScorefromFile(path+'HansonACE2hits.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcJvvm7bRUkK"
      },
      "outputs": [],
      "source": [
        "Externalpath='/content/PPI-Inhibitors/Data/External data/'\n",
        "#Result=PredictScorefromFile(Externalpath+'External1.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "Pdbloc=Externalpath+'pdb/'\n",
        "#trainedModel_IPPI,train_GNN=IPPI_Net,GNN_model\n",
        "with open(Externalpath+'2dyh_all.txt') as f:\n",
        "  D = f.readlines()\n",
        "InhibitedComp=[];PdbId=[];Ligandnames=[];SMILES=[];labels=[];\n",
        "All_data_list=[]\n",
        "from tqdm import tqdm as tqdm\n",
        "#2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "for d in tqdm(D):\n",
        "    #if len(d)==6:\n",
        "    Pdbid,inhibtedc,Ligandid,smiles = d.split()\n",
        "    PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);InhibitedComp.append(inhibtedc);#labels.append(float (y));\n",
        "################\n",
        "pdbname=listdir(Pdbloc)\n",
        "mypdb=[]\n",
        "for p in pdbname:\n",
        "  if p.split('.pdb')[0] in Pdbid:\n",
        "    mypdb.append(p)\n",
        "UniqueProtein=list (set (mypdb))\n",
        "pos=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "Pos,Negs,labels=External_GenerateRandomNegative(pos)\n",
        "1/0\n",
        "#Negs=External_GenerateRandomNegative(pos)\n",
        "#omplexnames=list (complex_ligand_dict.keys())\n",
        "AllNeg=[];AllPos=[];\n",
        "totalcomp=list(set (np.array([([t][0]) for t in pos])))\n",
        "External_ProteinData_dict=PrepairDataset.processProtein(UniqueProtein,Pdbloc)\n",
        "Result_dict={}\n",
        "#Testing\n",
        "Y_t,Z=[],[]\n",
        "for nt in range(len(InhibitedComp)):\n",
        "  complexname,Ligandname, ligandsmile,inhibitC =PdbId[nt], Ligandnames[nt], SMILES[nt],InhibitedComp[nt]\n",
        "  test_score=trainedModel_IPPI(train_GNN,External_ProteinData_dict[complexname],complexname,ligandsmile)\n",
        "  if test_score!=0.0:\n",
        "      test_score=test_score.cpu().data.numpy()[0]\n",
        "      Z.append(test_score)\n",
        "      Result_dict[(complexname,Ligandname)]=(inhibitC,test_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoKBzkYCilus"
      },
      "outputs": [],
      "source": [
        "    posexamples=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "    NegtiveRatio=1\n",
        "    ###SuperDrugbank\n",
        "    ###Names\n",
        "    SuperdrugNames=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#'approved_drugs_chemical_structure_identifiers.xlsx'\n",
        "    SuperdrugNames=SuperdrugNames[1:]\n",
        "    SuperdrugNames = np.array([s[0] for s in SuperdrugNames])\n",
        "    ###############\n",
        "    ###SuperDrugbank\n",
        "    df_Superdrug=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"C\").values\n",
        "    df_Superdrug=df_Superdrug[1:]\n",
        "    ###\n",
        "    df_Superdrug_Compounds=np.array([c[0] for c in df_Superdrug])#3638\n",
        "    SuperDrug_dict=dict (zip (SuperdrugNames,df_Superdrug_Compounds))\n",
        "    ################\n",
        "    path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "    DBD5_ProteinData_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "    Ubench5CompNames=list (set (list (DBD5_ProteinData_dict.keys())))\n",
        "    ####\n",
        "    AllNeg=[];AllPos=[];\n",
        "    complex_ligand_dict={}\n",
        "    for key,val in  posexamples:\n",
        "      print(key,val,posexamples[key,val][1])\n",
        "      if key not in complex_ligand_dict:\n",
        "        complex_ligand_dict[key]=posexamples[key,val][1]\n",
        "      else:\n",
        "        #print(\"else\",key,val)\n",
        "        complex_ligand_dict[key]=np.append( complex_ligand_dict.get(key, ()) ,posexamples[key,val][1])\n",
        "    Complexnames=list (complex_ligand_dict.keys())\n",
        "    #totalligands_train=list (set (totalligands_train))\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "        print(\"complexname=\",everycomp,\"origanlInhibitors\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        AllPos.extend(pos)\n",
        "        while (len(negs)<NN):# and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            LigandR = random.choice(SuperdrugNames)\n",
        "            LigandR_smile=SuperDrug_dict[LigandR]\n",
        "            Npair=((everycomp,LigandR_smile))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(LigandR_smile) is not None:\n",
        "                negs.append(Npair)\n",
        "#                print(\"Npair SuperdrugNames\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "    print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    print(\"second method Cr\")\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):\n",
        "            for everyL in origanlL:\n",
        "                ComplexR = random.choice(Ubench5CompNames)\n",
        "                Npair=((ComplexR,everyL))\n",
        "                if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "    #                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                    negs.append(Npair)\n",
        "        AllNeg.extend(negs)\n",
        "        ###################\n",
        "    print(\"N=\",len(AllNeg),\"P\",len(AllPos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXEY-sbVluVB"
      },
      "outputs": [],
      "source": [
        "Covid19_RFPP_all=PredictRFPPfromFile(path,path+'/HansonACE2hits.txt',path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Covid19_External_score,Covid19_External_labels=PredictScorefromFileSVM(path,path+'/HansonACE2hits.txt',path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1Vwwg-9jpfr"
      },
      "outputs": [],
      "source": [
        "Externalpath='/content/PPI-Inhibitors/Data/External data/'\n",
        "Resultdict,External_score,External_labels=PredictScorefromFile(Externalpath+'HansonACE2hits.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'HansonACE2hits.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "print (\"RFPP\",RFPP_all)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR,RFPP\",round (External_Auc,3), round (External_AP,3),RFPP_all,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLfpMbtJe41B"
      },
      "outputs": [],
      "source": [
        "Externalpath='/content/PPI-Inhibitors/Data/External data/'\n",
        "Resultdict,Tscores,Tlabels=PredictScorefromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "print (\"RFPP\",RFPP_all)\n",
        "T_Auc=roc_auc_score(np.array(Tlabels), np.array(Tscores))#Tscores,Tlabels\n",
        "T_PR=average_precision_score(np.array(Tlabels), np.array(Tscores))\n",
        "print (T_Auc,T_PR)\n",
        "Tscores_list.append(Tscores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrLma57AbAVI"
      },
      "outputs": [],
      "source": [
        "PredictScorefromFile(Externalpath+'External1.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80_vkg5NbjDa"
      },
      "outputs": [],
      "source": [
        "GNN_model(All_ProteinData_dict[complexname])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8wjHfblXoJ4"
      },
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkc3FiVMJv-Q"
      },
      "outputs": [],
      "source": [
        "len(Tscores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-kK_-O0sl56"
      },
      "outputs": [],
      "source": [
        "plt.plot(Tscores,color='r')\n",
        "plt.plot(Tlabels,color='b')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYnw56aPAckO"
      },
      "outputs": [],
      "source": [
        "complexname,Ligandname =test[nt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRqKHtk4XeIt"
      },
      "outputs": [],
      "source": [
        "complexname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icgyydgs_4Oe"
      },
      "outputs": [],
      "source": [
        "All_examples[complexname,ligandname][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsKQceM2F_U5"
      },
      "outputs": [],
      "source": [
        "All_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yirvEdXkEOfe"
      },
      "outputs": [],
      "source": [
        "Superdrug_SMILES#Features all unique Compounds of Superdrugbank and truepositive examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKGyWzt7EHp3"
      },
      "outputs": [],
      "source": [
        "Actual_Compound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-g-RTn-D5aO"
      },
      "outputs": [],
      "source": [
        "Pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Rd5miiCedR"
      },
      "outputs": [],
      "source": [
        "Actual_Compound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4kgn1MjCfIU"
      },
      "outputs": [],
      "source": [
        "\n",
        "##########loop\n",
        "RFPP_all=[]\n",
        "perntile_values = [0,1,5,10,20,40,50,60,70,80,90,95,99,100]#99\n",
        "for Pi_index in range(len(P2C_dict)):\n",
        "  ####For one example Pi\n",
        "  Pi=P2C_dict[Pi_index ]#pick one protein P index\n",
        "  Pi_Feature=External_ProteinData_dict[Pi]#P features of that index\n",
        "  Actual_Compound=P2C_dict[Pi]#Actul_Compound paired with Pi\n",
        "  #Actual_Compound_Features=np.array([U[c] for c in Actual_Compound])\n",
        "  Ctt=np.vstack((Actual_Compound,Superdrug_SMILES))#Features all unique Compounds of Superdrugbank and truepositive examples\n",
        "  Ptt=np.array([Pi for i in range(len(Ctt))])#Copy same feature of protein equal to number of unique compounds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuTEKRM1CTWq"
      },
      "outputs": [],
      "source": [
        "P2C_dict[Pi_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBUsudDUviNZ"
      },
      "outputs": [],
      "source": [
        "Resultdict,Tscores,Tlabels,RFPP_list=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRFllVhddGFE"
      },
      "outputs": [],
      "source": [
        "T_Auc=roc_auc_score(np.array(Tlabels), np.array(Tscores))#Tscores,Tlabels\n",
        "T_PR=average_precision_score(np.array(Tlabels), np.array(Tscores))\n",
        "print (T_Auc,T_PR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx9mRKr8dfXr"
      },
      "outputs": [],
      "source": [
        "Tscores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoKQCOHdbAvm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score\n",
        "#For End to End LEarning\n",
        "\"\"\"\n",
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "#########\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "\"\"\"\n",
        "pos_Pid,Ligandnames,pos_smiles,pos_label=readFile('/content/PPI-Inhibitors/Data/2p2iInhibitorsSMILES.txt')\n",
        "###\n",
        "\"\"\"\n",
        "UniqueProtein=list (set (pos_Pid))\n",
        "ProteinData_dict=PrepairDataset.processProtein(UniqueProtein)##PdBloc)\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "# save dictionary to pickle file\n",
        "with open(path+'ProteinData_dict.pickle', 'wb') as filename:\n",
        "    pickle.dump(ProteinData_dict, filename, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\"\"\"\n",
        "#pickle.dump(path+'ProteinData_dict.npy',ProteinData_dict)\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "import pickle\n",
        "ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "import pandas as pd\n",
        "fields=['(Complexname)','Binders SMILES']\n",
        "df=pd.read_csv('/content/PPI-Inhibitors/Data/BindersWithComplexname.csv', skipinitialspace=True, usecols=fields)\n",
        "# See the keys\n",
        "#print(df.keys())\n",
        "neg_Pidname,neg_smiles=df[df.keys()[0]].values,df[df.keys()[1]].values\n",
        "\n",
        "neg_Pid=[n.split('_')[0] for n in neg_Pidname]\\\n",
        "\n",
        "New_binders_dict=AppendlistinDict(neg_Pid,neg_smiles)\n",
        "####\n",
        "neg_label=-1.0*(np.ones(len(neg_smiles)))\n",
        "#####\n",
        "Z = []; Yo = []; A = [];Yp=[]\n",
        "AUC_ROC_final=[];Precision_final=[];Recall_final=[];Avg_P_final=[];\n",
        "####\n",
        "comp=dict(zip(zip (pos_Pid,Ligandnames),zip(pos_Pid,pos_smiles)))\n",
        "#####\n",
        "complex_ligand_dict=AppendlistinDict(pos_Pid,Ligandnames)\n",
        "Ldict=dict (zip(Ligandnames,pos_smiles))\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[]; PR_list=[];All_loss=[]\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateNegativeBinderonly(trainindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    trainSMILES=[Ldict[t[1]] for t in trainPos]\n",
        "    traincomp=[t[0] for t in trainPos]\n",
        "    trainPos=list (zip(traincomp,trainSMILES))\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    poslabel=1.0*np.ones(len(trainPos));neglabel=-1.0*np.ones(len(trainNeg))\n",
        "    y_train=np.append(poslabel,neglabel )\n",
        "    ###########\n",
        "    testPos,testNeg=GenerateNegativeBinderonly(testindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    testSMILES=[Ldict[t[1]] for t in testPos]\n",
        "    testcomp=[t[0] for t in testPos]\n",
        "    testPos=list (zip(testcomp,testSMILES))\n",
        "    poslabel=1.0*np.ones(len(testPos));neglabel=-1.0*np.ones(len(testNeg))\n",
        "    y_test=np.append(poslabel,neglabel )\n",
        "    test=np.vstack((testPos,testNeg))\n",
        "    #All examples passed for negative examples generation\n",
        "    ######################################3\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #############\n",
        "    GNN_model=GNN().cuda()\n",
        "    #####\n",
        "    epoch='29'\n",
        "    #Done=['1BKD','3D9T','1YCQ','2FLU','4QC3','2B4J','3UVW']\n",
        "    Done=['3UVW','2B4J','2RNY','4AJY','1YCR','4QC3','1NW9','2E3K','4GQ6','4YY6','1BXL','4ESG','3DAB','2FLU','1YCQ','2XA0','1Z92', '1F47','3TDU','3D9T','1BKD','3WN7']#\n",
        "    complexnameT=test[0][0]\n",
        "    #print (\"test complex \",complexnameT)\n",
        "    for d in Done:\n",
        "      if complexnameT==d:\n",
        "        IPPI_Net.load_state_dict(torch.load(path+'IPPI_Net_'+ complexnameT+'_Epoch'+str (epoch)))\n",
        "        GNN_model.load_state_dict(torch.load(path+'GNN_model_'+ complexnameT+'_Epoch'+str (epoch)))\n",
        "        #Testing\n",
        "        Y_t,Y_score=[],[]\n",
        "        for nt in range(len(test)):\n",
        "          complexname,Ligandname =test[nt]\n",
        "          #print(\"n\",n)\n",
        "          test_score=IPPI_Net(GNN_model,ProteinData_dict[complexname],complexname,Ligandname)\n",
        "          if test_score!=0.0:\n",
        "              Y_score.extend(test_score.cpu().data.numpy())\n",
        "              Y_t.append(y_test[nt])\n",
        "        Z=np.append(Z,Y_score);Yo=np.append(Yo,Y_t);\n",
        "        Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "        average_P_score=average_precision_score(Y_t,Y_score)\n",
        "        LP=np.load(path+complexnameT+'_Loss.npy''_Epoch'+str (epoch)+'.npy')\n",
        "        AUC_epoch=np.load(path+ complexnameT+'_AUC_list.npy''_Epoch'+str (epoch)+'.npy')\n",
        "        PR_epoch=np.load (path+ complexnameT+'_PR_list.npy''_Epoch'+str (epoch)+'.npy')\n",
        "        All_loss.extend(LP);AUC_list.extend(AUC_epoch); PR_list.extend(PR_epoch)\n",
        "        ######\n",
        "        fig=plt.figure()\n",
        "        plt.plot(LP,color='k',marker=',',label='Loss_'+complexnameT)\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        fig .savefig(path+complexnameT+\"_Loss.pdf\", bbox_inches='tight')\n",
        "        ########\n",
        "        Y_t=np.array(Y_t)\n",
        "        #print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "        print(complexnameT,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1))#,\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "        AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "        #####External\n",
        "        Externalpath='/content/PPI-Inhibitors/Data/External data/'\n",
        "        #Resultdict,Tscores=PredictScorefromFile(Externalpath+'External1.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "        Resultdict,Tscores=PredictScorefromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "        print(\"Tscores\",Tscores)\n",
        "        1/0\n",
        "        #import pdb;pdb.set_trace()\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr Over all complexes \\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])))#,\"\\n\")\n",
        "#######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUCROC PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "\n",
        "np.save(path+'All_loss.npy',All_loss)\n",
        "np.save (path+ 'AUC_list.npy',AUC_list)\n",
        "np.save (path+ 'PR_list.npy',PR_list)\n",
        "####\n",
        "fig=plt.figure()\n",
        "plt.plot(All_loss,color='k',marker=',',label='Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "fig .savefig(path+\"PPI Inhibitors loss over all complexes 30 epochs.pdf\", bbox_inches='tight')\n",
        "############\n",
        "fig=plt.figure()\n",
        "plt.plot(All_loss,color='k',marker=',',label='Loss')\n",
        "plt.plot( AUC_list,color='b',marker=',',label='AUC')\n",
        "plt.plot( PR_list,color='m',marker=',',label='PR')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "fig .savefig(path+\"PPI Inhibitors loss-PR over all complexes.pdf\", bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aUb8qPAKwnH"
      },
      "outputs": [],
      "source": [
        "Externalpath='/content/PPI-Inhibitors/Data/External data/'\n",
        "Result=PredictScorefromFile(Externalpath+'External1.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "print(Result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FEIviWOTlUN"
      },
      "outputs": [],
      "source": [
        "#For End to End LEarning\n",
        "\"\"\"\n",
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "#########\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "\"\"\"\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "#DBD5_ProteinData_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "Pos_ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "\"\"\"\n",
        "All_ProteinData_dict=dict( list (Pos_ProteinData_dict.items())+list (DBD5_ProteinData_dict.items()))\n",
        "for d in All_ProteinData_dict:\n",
        "  data=All_ProteinData_dict[d]\n",
        "  All_ProteinData_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "\"\"\"\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[]; PR_list=[];\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateRandomNegative(trainindex,CC,complex_ligand_dict,Ldict)\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    train_SMILES=[Ldict[t[1]] for t in train]\n",
        "    traincomp=[t[0] for t in train]\n",
        "    train=list (zip(traincomp,train_SMILES))\n",
        "    poslabel=1.0*np.ones(len(trainPos));neglabel=-1.0*np.ones(len(trainNeg))\n",
        "    y_train=np.append(poslabel,neglabel )\n",
        "    ###########\n",
        "    testPos,testNeg=GenerateRandomNegative(testindex,CC,complex_ligand_dict,Ldict)\n",
        "    test=np.vstack((testPos,testNeg))\n",
        "    testSMILES=[Ldict[t[1]] for t in test]\n",
        "    testcomp=[t[0] for t in test]\n",
        "    test=list (zip(testcomp,testSMILES))\n",
        "    poslabel=1.0*np.ones(len(testPos));neglabel=-1.0*np.ones(len(testNeg))\n",
        "    y_test=np.append(poslabel,neglabel )\n",
        "    #All examples passed for negative examples generation\n",
        "    ######################################3\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #############\n",
        "    GNN_model=GNN().cuda()\n",
        "    #####\n",
        "    \"\"\"\n",
        "    complexname,epoch='3UVW','0'\n",
        "    IPPI_Net.load_state_dict(torch.load(path+'IPPI_Net_'+ complexname+'_Epoch'+str (epoch)))\n",
        "    GNN_model.load_state_dict(torch.load(path+'GNN_model_'+ complexname+'_Epoch'+str (epoch)))\n",
        "    \"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(list (IPPI_Net.parameters()) + list( GNN_model.parameters()),lr=0.01,weight_decay=0.0001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    ####\n",
        "    print (\"test complex \",testPos[0][0],\"test complex[-1]\",testPos[-1][0])\n",
        "    #Done=['3UVW','2B4J','2RNY','4AJY','1YCR','4QC3','1NW9','2E3K','4GQ6','4YY6']\n",
        "    #3UVW 2B4J 2RNY 4QC3 1YCR 4AJY 1NW9 4GQ6 2E3K 4YY6 1BXL 4ESG 2FLU 1YCQ 3D9T 2XA0 3WN7 1Z92 3DAB 3TDU 1F47 1BKD\n",
        "    Done=['4QC3','1NW9','2E3K','4GQ6','3WN7','1YCQ','2XA0','1Z92', '1F47','4ESG','1BKD','4YY6','3D9T','3DAB','1BXL','2FLU','3TDU']#'3UVW','2B4J','2RNY','4AJY','1YCR',\n",
        "    if test[0][0] in Done:\n",
        "      continue\n",
        "    for epoch in range(100):\n",
        "      total_preds = torch.Tensor()\n",
        "      total_labels = torch.Tensor()\n",
        "      for n in range(len(train)):\n",
        "        complexname,Ligandname =train[n]\n",
        "        #if complexname=='3UVW':\n",
        "          #continue\n",
        "        #print(\"n\",n)\n",
        "        output=IPPI_Net(GNN_model,All_ProteinData_dict,complexname,Ligandname)\n",
        "        #print(\"output\",output)\n",
        "        if output!=0.0:\n",
        "          total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
        "          total_labels = torch.cat((total_labels, y_train[n].view(-1, 1).cpu()), 0)\n",
        "      #output_list=torch.FloatTensor(output_list,requires_grad=True).cuda\n",
        "      #target=torch.FloatTensor(target).cuda\n",
        "      loss = criterion(total_preds, total_labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      params = list(IPPI_Net.parameters())\n",
        "      print(\"Epoch\",epoch,\"loss\",loss.cpu().data.numpy())\n",
        "      #############\n",
        "      Y_t,Y_score=[],[]\n",
        "      if epoch%3==0:\n",
        "        for nt in range(len(test)):\n",
        "          complexname,Ligandname =test[nt]\n",
        "          test_score=IPPI_Net(GNN_model,All_ProteinData_dict,complexname,Ligandname)\n",
        "          #print(\"nt\",nt,\"test_score\",test_score)\n",
        "          if test_score!=0.0:\n",
        "            Y_score.append(test_score.cpu().data.numpy())\n",
        "            Y_t.append(y_test[nt])\n",
        "        auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "        average_PR=average_precision_score(Y_t,Y_score)\n",
        "        print(\"test comp\",testPos[0][0],\"Epoch\",epoch,\"auc\",\"PR\",auc_roc.round(3),average_PR.round(3),\"loss\",loss.cpu().data.numpy().round(3))\n",
        "  #            print(\"Fauc_roc\",auc_roc)\n",
        "        AUC_list.append(auc_roc);PR_list.append(average_PR)\n",
        "        Y_t,Y_score=[],[]\n",
        "        LP.append(loss.cpu().data.numpy())\n",
        "    #if (loss.cpu().data.numpy()<0.22):\n",
        "      #break;\n",
        "    #Testing\n",
        "    for nt in range(len(test)):\n",
        "      complexname,Ligandname =test[nt]\n",
        "      #print(\"n\",n)\n",
        "      test_score=IPPI_Net(GNN_model,All_ProteinData_dict,complexname,Ligandname)\n",
        "      if test_score!=0.0:\n",
        "          Y_score.extend(test_score.cpu().data.numpy())\n",
        "          Y_t.append(y_test[nt])\n",
        "\n",
        "    print (complexname,\"test complex \",testPos[0][0],testPos[-1][0])\n",
        "    TestComplex=testPos[0][0]\n",
        "    Z=np.append(Z,Y_score);Yo=np.append(Yo,Y_t);\n",
        "    Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "    average_P_score=average_precision_score(Y_t,Y_score)\n",
        "    np.save(path+TestComplex+'_Loss.npy''_Epoch'+str (epoch),LP)\n",
        "    np.save (path+ TestComplex+'_AUC_list.npy''_Epoch'+str (epoch),AUC_list)\n",
        "    np.save (path+ TestComplex+'_PR_list.npy''_Epoch'+str (epoch),PR_list)\n",
        "    ####\n",
        "    file_model = path+'/IPPI_Net_Random_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "    torch.save(IPPI_Net.state_dict(), file_model)\n",
        "    file_model = path+'/GNN_model_Random_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "    torch.save(GNN_model.state_dict(), file_model)\n",
        "    Y_t=np.array(Y_t)\n",
        "    #print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "    print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "#######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "plt.figure()\n",
        "np.save(path+'Loss.npy',LP)\n",
        "np.save (path+ 'AUC_listEpoch5.npy',AUC_list)\n",
        "plt.plot(LP)\n",
        "plt.plot( AUC_list)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGgOYmALVz87"
      },
      "outputs": [],
      "source": [
        "Y_t=np.array(Y_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV16DB36e1b2"
      },
      "outputs": [],
      "source": [
        "LP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGsrZRE8Wgbc"
      },
      "outputs": [],
      "source": [
        " for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt]\n",
        "  test_score=IPPI_Net(GNN_model,ProteinData_dict,complexname,Ligandname)\n",
        "  print(\"nt\",nt,\"test_score\",test_score)\n",
        "  if test_score!=0.0:\n",
        "    Y_score.append(test_score.cpu().data.numpy())\n",
        "    Y_t.append(y_test[nt])\n",
        "auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqmWn8j5Qfz-"
      },
      "outputs": [],
      "source": [
        " for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt]\n",
        "  test_score=IPPI_Net(GNN_model,ProteinData_dict,complexname,Ligandname)\n",
        "  print(\"nt\",nt,\"test_score\",test_score)\n",
        "  if output!=0.0:\n",
        "    Y_score.append(test_score.cpu().data.numpy())\n",
        "    Y_t.append(y_test[nt])\n",
        "auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u11BxFkpPorl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiQb0RWpLRZl"
      },
      "outputs": [],
      "source": [
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt]\n",
        "  print(\"n\",n)\n",
        "  test_score=IPPI_Net(GNN_model,ProteinData_dict,complexname,Ligandname)\n",
        "  print(\"test_score\",test_score)\n",
        "  Y_score.append(test_score.cpu().data.numpy())\n",
        "  Y_t.append(y_test[nt])\n",
        "auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPUhwU8nMJiq"
      },
      "outputs": [],
      "source": [
        "test_score=IPPI_Net(GNN_model,ProteinData_dict,complexname,Ligandname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4nGcD1FMNpf"
      },
      "outputs": [],
      "source": [
        "test_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2VPZxDcLbw7"
      },
      "outputs": [],
      "source": [
        "Y_t.append(y_test[nt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpw-ZdGJGw6M"
      },
      "outputs": [],
      "source": [
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt]\n",
        "  #print(\"n\",n)\n",
        "  test_score=IPPI_Net(GNN_model,ProteinData_dict,complexname,Ligandname)\n",
        "  Y_score.extend(test_score.cpu().data.numpy())\n",
        "  Y_t.extend(y_test[nt].cpu().data.numpy())\n",
        "  auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "  print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())\n",
        "#            print(\"Fauc_roc\",auc_roc)\n",
        "  AUC_list.append(auc_roc)\n",
        "  Y_t,Y_score=[],[]\n",
        "  LP.append(loss.cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rsg8kXhPFRzj"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rguvzuf2FUir"
      },
      "outputs": [],
      "source": [
        "total_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHJWOcomEVTn"
      },
      "outputs": [],
      "source": [
        "torch.cat(output_list,output,dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t3czIdXEbhV"
      },
      "outputs": [],
      "source": [
        "output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wljkK-gJ-VDS"
      },
      "outputs": [],
      "source": [
        "output_list=torch.FloatTensor(output_list).cuda\n",
        "target=torch.FloatTensor(target).cuda\n",
        "loss = criterion(output_list, y_train)\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "params = list(IPPI_Net.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44sJjsQA-b3k"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mf424Jc8hCu"
      },
      "outputs": [],
      "source": [
        "output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW-UCyq98lK9"
      },
      "outputs": [],
      "source": [
        "output.cpu().detach().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8mAzmS_12n9"
      },
      "outputs": [],
      "source": [
        "trainPos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vduv6hc616mV"
      },
      "outputs": [],
      "source": [
        "getFP(Ligandname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ1017d21ZVe"
      },
      "outputs": [],
      "source": [
        "output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5LelXbrBT17"
      },
      "outputs": [],
      "source": [
        "output_list=torch.FloatTensor(output_list).cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBpdCfrFxbap"
      },
      "outputs": [],
      "source": [
        "loss = criterion(output_list, y_train[0:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6mYOFxxxDVZ"
      },
      "outputs": [],
      "source": [
        " y_train=np.append(1.0*np.ones(len(trainPos)),-1.0*np.ones(len(trainNeg)))\n",
        " y_train=torch.FloatTensor( y_train).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw-jVFOxvC3m"
      },
      "outputs": [],
      "source": [
        " leny_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5rlhqTWyH0p"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwEglSceYhRu"
      },
      "outputs": [],
      "source": [
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[];\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateNegativeBinderonly(trainindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    #All examples passed for negative examples generation\n",
        "#        posexamples=testindex\n",
        "    testPos,testNeg=GenerateNegativeBinderonly(testindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "\n",
        "    1/0\n",
        "#        AlltestExamples.extend(testPos);AlltestExamples.extend(testNeg);\n",
        "    ######\n",
        "    Ctr=[];Ptr=[];y_train=[]\n",
        "    for t in trainPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctr.append(getFP(Ldict[t[1]]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(1)\n",
        "    #####\n",
        "    for t in trainNeg:\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctr.append(getFP(t[1]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(-1);\n",
        "    #######\n",
        "    PC_train_Features=np.hstack((Ptr,Ctr))\n",
        "#        ######\n",
        "\n",
        "    Ctt=[];Ptt=[];y_test=[]\n",
        "    for t in testPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctt.append(getFP(Ldict[t[1]]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(1)\n",
        "    ####\n",
        "    for t in testNeg:\n",
        "#             if t[0]!='1TUE_A_2_B':\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctt.append(getFP(t[1]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(-1);\n",
        "    #######\n",
        "    PC_test_Features=np.hstack((Ptt,Ctt))\n",
        "    X_train,X_test=PC_train_Features,PC_test_Features\n",
        "    ######################################3\n",
        "\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #print(IPPI_MLP_Net())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(IPPI_Net.parameters(),lr=0.0001,weight_decay=0.001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
        "    X_train=torch.FloatTensor(X_train).cuda()\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    X_test=torch.FloatTensor(X_test).cuda()\n",
        "    y_test=torch.FloatTensor( y_test).cuda()\n",
        "    for epoch in range(100):\n",
        "      output = IPPI_Net(X_train)\n",
        "      output=torch.squeeze(output, 1)\n",
        "      target =   y_train\n",
        "      loss = criterion(output, target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      params = list(IPPI_Net.parameters())\n",
        "      test_score = IPPI_Net(X_test)\n",
        "      test_score=torch.squeeze(test_score, 1)\n",
        "      Y_score.extend(test_score.cpu().data.numpy())\n",
        "      Y_t.extend(y_test.cpu().data.numpy())\n",
        "      test_score.tolist()\n",
        "      y_test.tolist()\n",
        "      auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())\n",
        "#            print(\"Fauc_roc\",auc_roc)\n",
        "      AUC_list.append(auc_roc)\n",
        "      Y_t,Y_score=[],[]\n",
        "      LP.append(loss.cpu().data.numpy())\n",
        "    #Testing\n",
        "    test_score = IPPI_Net(X_test)\n",
        "    test_score=torch.squeeze(test_score, 1)\n",
        "    test_score=test_score.cpu().data.numpy()\n",
        "    y_test=y_test.cpu().data.numpy()\n",
        "    Z=np.append(Z,test_score);Yo=np.append(Yo,y_test);\n",
        "    Auc=roc_auc_score(np.array(y_test), np.array(test_score))\n",
        "    print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",t[0],\"Final auc_roc\",Auc)\n",
        "\n",
        "    average_P_score=average_precision_score(y_test,test_score)\n",
        "    print(testPos[0][0],\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",len(testPos),\"\\t\",len(testNeg),\"\\t\",round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "  ######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "#    ######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "plt.figure()\n",
        "plt.plot(LP)\n",
        "plt.plot( AUC_list)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEgZlfX3Tk5l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KVf0KqsTXsg"
      },
      "outputs": [],
      "source": [
        "testNeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3v-ZqVQ8dLy"
      },
      "outputs": [],
      "source": [
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[];\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateNegativeBinderonly(trainindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    #All examples passed for negative examples generation\n",
        "#        posexamples=testindex\n",
        "    testPos,testNeg=GenerateNegativeBinderonly(testindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "#        AlltestExamples.extend(testPos);AlltestExamples.extend(testNeg);\n",
        "    ######\n",
        "    Ctr=[];Ptr=[];y_train=[]\n",
        "    for t in trainPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctr.append(getFP(Ldict[t[1]]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(1)\n",
        "    #####\n",
        "    for t in trainNeg:\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctr.append(getFP(t[1]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(-1);\n",
        "    #######\n",
        "    PC_train_Features=np.hstack((Ptr,Ctr))\n",
        "#        ######\n",
        "\n",
        "    Ctt=[];Ptt=[];y_test=[]\n",
        "    for t in testPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctt.append(getFP(Ldict[t[1]]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(1)\n",
        "    ####\n",
        "    for t in testNeg:\n",
        "#             if t[0]!='1TUE_A_2_B':\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctt.append(getFP(t[1]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(-1);\n",
        "    #######\n",
        "    PC_test_Features=np.hstack((Ptt,Ctt))\n",
        "    X_train,X_test=PC_train_Features,PC_test_Features\n",
        "    ######################################3\n",
        "\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #print(IPPI_MLP_Net())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(IPPI_Net.parameters(),lr=0.0001,weight_decay=0.001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
        "    X_train=torch.FloatTensor(X_train).cuda()\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    X_test=torch.FloatTensor(X_test).cuda()\n",
        "    y_test=torch.FloatTensor( y_test).cuda()\n",
        "    for epoch in range(100):\n",
        "      output = IPPI_Net(X_train)\n",
        "      output=torch.squeeze(output, 1)\n",
        "      target =   y_train\n",
        "      loss = criterion(output, target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if loss<0.1:\n",
        "        break;\n",
        "      params = list(IPPI_Net.parameters())\n",
        "      test_score = IPPI_Net(X_test)\n",
        "      test_score=torch.squeeze(test_score, 1)\n",
        "      Y_score.extend(test_score.cpu().data.numpy())\n",
        "      Y_t.extend(y_test.cpu().data.numpy())\n",
        "      test_score.tolist()\n",
        "      y_test.tolist()\n",
        "      auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())\n",
        "#            print(\"Fauc_roc\",auc_roc)\n",
        "      AUC_list.append(auc_roc)\n",
        "      Y_t,Y_score=[],[]\n",
        "      LP.append(loss.cpu().data.numpy())\n",
        "    #Testing\n",
        "    test_score = IPPI_Net(X_test)\n",
        "    test_score=torch.squeeze(test_score, 1)\n",
        "    test_score=test_score.cpu().data.numpy()\n",
        "    y_test=y_test.cpu().data.numpy()\n",
        "    Z=np.append(Z,test_score);Yo=np.append(Yo,y_test);\n",
        "    Auc=roc_auc_score(np.array(y_test), np.array(test_score))\n",
        "    print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",t[0],\"Final auc_roc\",Auc)\n",
        "\n",
        "    average_P_score=average_precision_score(y_test,test_score)\n",
        "    print(testPos[0][0],\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",len(testPos),\"\\t\",len(testNeg),\"\\t\",round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "  ######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "#    ######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "plt.figure()\n",
        "plt.plot(LP)\n",
        "plt.plot( AUC_list)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faFfBkgmARO3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTk-3ha-ARny"
      },
      "outputs": [],
      "source": [
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[];\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateNegativeBinderonly(trainindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    #All examples passed for negative examples generation\n",
        "#        posexamples=testindex\n",
        "    testPos,testNeg=GenerateNegativeBinderonly(testindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "#        AlltestExamples.extend(testPos);AlltestExamples.extend(testNeg);\n",
        "    ######\n",
        "    Ctr=[];Ptr=[];y_train=[]\n",
        "    for t in trainPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctr.append(getFP(Ldict[t[1]]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(1)\n",
        "    #####\n",
        "    for t in trainNeg:\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctr.append(getFP(t[1]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(-1);\n",
        "    #######\n",
        "    PC_train_Features=np.hstack((Ptr,Ctr))\n",
        "#        ######\n",
        "\n",
        "    Ctt=[];Ptt=[];y_test=[]\n",
        "    for t in testPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctt.append(getFP(Ldict[t[1]]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(1)\n",
        "    ####\n",
        "    for t in testNeg:\n",
        "#             if t[0]!='1TUE_A_2_B':\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctt.append(getFP(t[1]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(-1);\n",
        "    #######\n",
        "    PC_test_Features=np.hstack((Ptt,Ctt))\n",
        "    X_train,X_test=PC_train_Features,PC_test_Features\n",
        "    ######################################3\n",
        "    Max_auc_roc=0.0\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #print(IPPI_MLP_Net())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(IPPI_Net.parameters(),lr=0.0001,weight_decay=0.001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
        "    X_train=torch.FloatTensor(X_train).cuda()\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    X_test=torch.FloatTensor(X_test).cuda()\n",
        "    y_test=torch.FloatTensor( y_test).cuda()\n",
        "    for epoch in range(100):\n",
        "      output = IPPI_Net(X_train)\n",
        "      output=torch.squeeze(output, 1)\n",
        "      target =   y_train\n",
        "      loss = criterion(output, target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      #if loss<0.15:\n",
        "        #break;\n",
        "      params = list(IPPI_Net.parameters())\n",
        "      test_score = IPPI_Net(X_test)\n",
        "      test_score=torch.squeeze(test_score, 1)\n",
        "      Y_score.extend(test_score.cpu().data.numpy())\n",
        "      Y_t.extend(y_test.cpu().data.numpy())\n",
        "      #test_score.tolist()\n",
        "      #y_test.tolist()\n",
        "      auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      #Max_auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      if (auc_roc-Max_auc_roc)>0:\n",
        "        Max_auc_roc=auc_roc\n",
        "        print(\"Max auc\",Max_auc_roc)\n",
        "      else:\n",
        "        break;\n",
        "      print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())\n",
        "#            print(\"Fauc_roc\",auc_roc)\n",
        "      AUC_list.append(auc_roc)\n",
        "      Y_t,Y_score=[],[]\n",
        "      LP.append(loss.cpu().data.numpy())\n",
        "    #Testing\n",
        "    test_score = IPPI_Net(X_test)\n",
        "    test_score=torch.squeeze(test_score, 1)\n",
        "    test_score=test_score.cpu().data.numpy()\n",
        "    y_test=y_test.cpu().data.numpy()\n",
        "    Z=np.append(Z,test_score);Yo=np.append(Yo,y_test);\n",
        "    Auc=roc_auc_score(np.array(y_test), np.array(test_score))\n",
        "    print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",t[0],\"Final auc_roc\",Auc)\n",
        "\n",
        "    average_P_score=average_precision_score(y_test,test_score)\n",
        "    print(testPos[0][0],\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",len(testPos),\"\\t\",len(testNeg),\"\\t\",round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "  ######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "#    ######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "plt.figure()\n",
        "plt.plot(LP)\n",
        "plt.plot( AUC_list)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VU1p0-iF7aW"
      },
      "outputs": [],
      "source": [
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[];\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateNegativeBinderonly(trainindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    #All examples passed for negative examples generation\n",
        "#        posexamples=testindex\n",
        "    testPos,testNeg=GenerateNegativeBinderonly(testindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "#        AlltestExamples.extend(testPos);AlltestExamples.extend(testNeg);\n",
        "    ######\n",
        "    Ctr=[];Ptr=[];y_train=[]\n",
        "    for t in trainPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctr.append(getFP(Ldict[t[1]]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(1)\n",
        "    #####\n",
        "    for t in trainNeg:\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctr.append(getFP(t[1]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(-1);\n",
        "    #######\n",
        "    PC_train_Features=np.hstack((Ptr,Ctr))\n",
        "#        ######\n",
        "\n",
        "    Ctt=[];Ptt=[];y_test=[]\n",
        "    for t in testPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctt.append(getFP(Ldict[t[1]]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(1)\n",
        "    ####\n",
        "    for t in testNeg:\n",
        "#             if t[0]!='1TUE_A_2_B':\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctt.append(getFP(t[1]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(-1);\n",
        "    #######\n",
        "    PC_test_Features=np.hstack((Ptt,Ctt))\n",
        "    X_train,X_test=PC_train_Features,PC_test_Features\n",
        "    ######################################3\n",
        "    Max_auc_roc=0.0\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #print(IPPI_MLP_Net())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(IPPI_Net.parameters(),lr=0.0001,weight_decay=0.001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
        "    X_train=torch.FloatTensor(X_train).cuda()\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    X_test=torch.FloatTensor(X_test).cuda()\n",
        "    y_test=torch.FloatTensor( y_test).cuda()\n",
        "    for epoch in range(100):\n",
        "      output = IPPI_Net(X_train)\n",
        "      output=torch.squeeze(output, 1)\n",
        "      target =   y_train\n",
        "      loss = criterion(output, target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      params = list(IPPI_Net.parameters())\n",
        "      test_score = IPPI_Net(X_test)\n",
        "      test_score=torch.squeeze(test_score, 1)\n",
        "      Y_score.extend(test_score.cpu().data.numpy())\n",
        "      Y_t.extend(y_test.cpu().data.numpy())\n",
        "      auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      #Max_auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      if (auc_roc-Max_auc_roc)>-0.01:\n",
        "        Max_auc_roc=auc_roc\n",
        "        print(\"Max auc\",Max_auc_roc)\n",
        "      else:\n",
        "        break;\n",
        "      print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())\n",
        "      AUC_list.append(auc_roc)\n",
        "      Y_t,Y_score=[],[]\n",
        "      LP.append(loss.cpu().data.numpy())\n",
        "    #Testing\n",
        "    test_score = IPPI_Net(X_test)\n",
        "    test_score=torch.squeeze(test_score, 1)\n",
        "    test_score=test_score.cpu().data.numpy()\n",
        "    y_test=y_test.cpu().data.numpy()\n",
        "    Z=np.append(Z,test_score);Yo=np.append(Yo,y_test);\n",
        "    Auc=roc_auc_score(np.array(y_test), np.array(test_score))\n",
        "    print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",t[0],\"Final auc_roc\",Auc)\n",
        "\n",
        "    average_P_score=average_precision_score(y_test,test_score)\n",
        "    print(testPos[0][0],\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",len(testPos),\"\\t\",len(testNeg),\"\\t\",round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "  ######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "#    ######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "plt.figure()\n",
        "plt.plot(LP)\n",
        "plt.plot( AUC_list)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w8lhNNxI605"
      },
      "outputs": [],
      "source": [
        "PdBloc='/content/PPI-Inhibitors/Data/Pdb'\n",
        "pos_Pid,pos_smiles,pos_label=readFile('/content/PPI-Inhibitors/Data/2p2iInhibitorsSMILES.txt')\n",
        "import pandas as pd\n",
        "fields=['(Complexname)','Binders SMILES']\n",
        "df=pd.read_csv('/content/PPI-Inhibitors/Data/BindersWithComplexname.csv', skipinitialspace=True, usecols=fields)\n",
        "# See the keys\n",
        "#print(df.keys())\n",
        "neg_Pidname,neg_smiles=df[df.keys()[0]].values,df[df.keys()[1]].values\n",
        "neg_Pid=[n.split('_')[0] for n in neg_Pidname]\n",
        "####\n",
        "neg_label=-1.0*(np.ones(len(neg_smiles)))\n",
        "#####\n",
        "compound_iso_smiles=np.append(pos_smiles,neg_smiles)\n",
        "smile_graph = {}\n",
        "for smile in compound_iso_smiles:\n",
        "    g = smile_to_graph(smile)\n",
        "    smile_graph[smile] = g\n",
        "########\n",
        "smiles_all,Pid_all,labels_all=np.array(np.append(pos_smiles,neg_smiles)),np.array(np.append(pos_Pid,neg_Pid)),np.array(np.append(pos_label,neg_label))\n",
        "Processed_data=PrepairDataset.process(smiles_all,Pid_all,labels_all,smile_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KoeCGb8FoTi"
      },
      "outputs": [],
      "source": [
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFnPHOfVFdbO"
      },
      "outputs": [],
      "source": [
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtJr0-RjdALV"
      },
      "outputs": [],
      "source": [
        "#GNN_data,GCN_data,label=Processed_data[0]\n",
        "GNN_data_all,GCN_data_all,labels=[],[],[]\n",
        "for i in range(len(Processed_data)):\n",
        "  GNN_data,GCN_data,label=Processed_data[i]\n",
        "  GNN_data_all.append(GNN_data);GCN_data_all.append(GCN_data);labels.append(label);\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];mini_batch=[];target_list=[]\n",
        "print(\"Total samples\",GNN_data_all)\n",
        "for p in range(0,10):#len(GNN_data_all)):\n",
        "  print(\"sample\",p)\n",
        "  result,PFeatures=model(GNN_data_all[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  print(PFeatures.shape)\n",
        "###############\n",
        "modeling = GCN\n",
        "model_st = modeling.__name__\n",
        "cuda_name = \"cuda:0\"\n",
        "print('cuda_name:', cuda_name)\n",
        "dataset='PPI-inhibitor'\n",
        "#############################################\n",
        "CUDA_LAUNCH_BLOCKING = \"1\"\n",
        "# training the model\n",
        "device = torch.device(cuda_name if torch.cuda.is_available() else \"cpu\")\n",
        "model = modeling().to(device)\n",
        "####\n",
        "LR = 0.0005\n",
        "LOG_INTERVAL = 20\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "best_epoch = -1\n",
        "NUM_EPOCHS=10\n",
        "model_file_name = 'model_' + model_st + '_' + dataset +  '.model'\n",
        "result_file_name = 'result_' + model_st + '_' + dataset +  '.csv'\n",
        "TRAIN_BATCH_SIZE,TEST_BATCH_SIZE=10,10\n",
        "TotalData=len(GCN_data_all)\n",
        "train_data,test_data=GCN_data_all[0:int (TotalData/2)],GCN_data_all[int (TotalData/2):]\n",
        "###################33\n",
        "# make data PyTorch mini-batch processing ready\n",
        "train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
        "#############################################\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train(model, device, train_loader, optimizer, epoch+1)\n",
        "    #1/0\n",
        "    ###\n",
        "    G,P,Sfeatures= predicting(model, device, test_loader,labels)\n",
        "    print(Sfeatures.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgyCP0gwrUOR"
      },
      "outputs": [],
      "source": [
        "G,P,Sfeatures= predicting(model, device, test_loader,labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1KdE_6Cwxp_"
      },
      "outputs": [],
      "source": [
        "Sfeatures[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgVftpkLZLPA"
      },
      "outputs": [],
      "source": [
        "# make data PyTorch Geometric ready\n",
        "dataset='ppi_inhibitor'\n",
        "print('preparing ', dataset + '_train.pt in pytorch format!')\n",
        "train_data = PrepairDataset(root='data', dataset=dataset+'_train', xd=train_drugs, xt=train_prots, y=train_Y,smile_graph=smile_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRtfFzZ4WXzl"
      },
      "outputs": [],
      "source": [
        "Processed_data=PrepairDataset.process(smiles_all,Pid_all,labels_all,smile_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JErYA7uLVA4a"
      },
      "outputs": [],
      "source": [
        "Processed_data=PrepairDataset.processProtein(Pid_all)#,\"/content/PPI-Inhibitors/Data/Pdb/*\")#labels_all,smile_graph,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtE98ci1UR08"
      },
      "outputs": [],
      "source": [
        "neg_Pid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTQKDaSq9Q56"
      },
      "outputs": [],
      "source": [
        "loc=glob.glob(\"/content/PPI-Inhibitors/Data/Pdb/*\")\n",
        "P1=loc[0]\n",
        "print(P1)\n",
        "parser = PDBParser()\n",
        "with warnings.catch_warnings(record=True) as w:\n",
        "  structure = parser.get_structure(\"\", P1)\n",
        "one_hot_atom=(atom1(structure))\n",
        "print(\"\\none_hot_atom.shape\",one_hot_atom.shape)\n",
        "#\n",
        "one_hot_res=(res1(structure))\n",
        "print(\"\\none_hot_atom.shape\",one_hot_res.shape)\n",
        "neigh_same_res,neigh_diff_res=(neigh1(structure))\n",
        "print(\"\\neigh_same_res.shape,neigh_diff_re.shape\",neigh_same_res.shape,\"\\n\",neigh_diff_res.shape)\n",
        "target=np.array(float (1.0))\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "#model.load_state_dict(torch.load(saved_model))\n",
        "result_list=[];target_list=[];mini_batch=[];target_list=[]\n",
        "#for p in range(0,len(one_hot_atom),2):\n",
        "result,PFeatures=model([torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "1/0\n",
        "#mini_batch.append(result)\n",
        "\n",
        "          # explain the flatten\n",
        "          #Result has a dimension of [N,1] and so we are trying to convert it to  a 1D tensor inorder to plot it\n",
        "          #So we are flattening the result and also the target\n",
        "result_list.append(torch.flatten(result.detach().cpu()).numpy())\n",
        "target_list.append(target)\n",
        "          #print(result_list)\n",
        "\n",
        "#result_list now consist of  a number o numpy arrays like [[<array 1>],[<array 2>], ..] but we need to flatten it\n",
        "# to plot it in a graph like [<array 1 contents>,<array 2 contents>,.... ]\n",
        "result_list=np.array(result_list).flatten()\n",
        "\n",
        "target_list=np.array(target_list).flatten()\n",
        "plt.plot(target_list,result_list,\".\")\n",
        "x = np.linspace(0,1,100)\n",
        "y=x\n",
        "plt.plot(x,y)\n",
        "plt.xlabel(\"Actual scores\")\n",
        "plt.ylabel(\"Predicted Scores\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLxf_vjhR6gW"
      },
      "outputs": [],
      "source": [
        "PFeatures.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyCR01HmSJBa"
      },
      "outputs": [],
      "source": [
        "Features=torch.squeeze(Features,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTfSi40fSOyc"
      },
      "outputs": [],
      "source": [
        "Features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6lRbj7WSHb1"
      },
      "outputs": [],
      "source": [
        "same_neigh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBfRTNqZWzkM"
      },
      "outputs": [],
      "source": [
        "torch.sum(same_neigh > -1).type(torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YsTVj98XRSU"
      },
      "outputs": [],
      "source": [
        "same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxQXEooLRKpD"
      },
      "outputs": [],
      "source": [
        "(same_neigh>-1).unsqueeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_s3_j_zRgDm"
      },
      "outputs": [],
      "source": [
        "same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG6dkxslQpKD"
      },
      "outputs": [],
      "source": [
        "same_neigh,diff_neigh=torch.tensor(neigh_same_res[p]).to(device).long(),torch.tensor(neigh_diff_res[p]).to(device).long()\n",
        "unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(1)\n",
        "unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U_EJGyCZhTp"
      },
      "outputs": [],
      "source": [
        "unsqueezed_same_neigh_indicator.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVjlekYmMC_-"
      },
      "outputs": [],
      "source": [
        "result=model([torch.tensor(one_hot_atom[p],dtype=torch.float32).to(device),torch.tensor(one_hot_res[p],dtype=torch.float32).to(device),torch.tensor(neigh_same_res[p]).to(device).long(),torch.tensor(neigh_diff_res[p]).to(device).long()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVzUq4okNTXn"
      },
      "outputs": [],
      "source": [
        "Z=torch.tensor(one_hot_atom[p:p+2],dtype=torch.float32).to(device)#.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kesdH9WmOQau"
      },
      "outputs": [],
      "source": [
        "Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs4_NBaeNWfA"
      },
      "outputs": [],
      "source": [
        "Z.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJUxautSLGkV"
      },
      "outputs": [],
      "source": [
        "torch.tensor(one_hot_atom[p]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imRUpRMYlMt8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import sampler\n",
        "import glob\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "#from QA.data_import import get_dataloader,data1\n",
        "import matplotlib.pyplot as plt\n",
        "#from QA.train import train\n",
        "#from QA.test import test\n",
        "seed=3\n",
        "torch.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRVez8JZc3Ms"
      },
      "outputs": [],
      "source": [
        "import py3Dmol\n",
        "#taken from https://william-dawson.github.io/using-py3dmol.html\n",
        "print(\"Target\")\n",
        "with open(\"QA/T0759-D1.pdb\") as ifile:\n",
        "    file_info_target = \"\".join([x for x in ifile])\n",
        "view = py3Dmol.view(width=400, height=300)\n",
        "view.addModelsAsFrames(file_info_target)\n",
        "view.setStyle({'model': -1}, {\"cartoon\": {'color': 'blue'}})\n",
        "view.zoomTo()\n",
        "view.show()\n",
        "print(\"Decoy\")\n",
        "with open(\"QA/T0759TS499_1-D1.pdb\") as ifile:\n",
        "    file_info_decoy = \"\".join([x for x in ifile])\n",
        "view = py3Dmol.view(width=400, height=300)\n",
        "view.addModelsAsFrames(file_info_decoy)\n",
        "view.setStyle({'model': -1}, {\"cartoon\": {'color': 'salmon'}})\n",
        "view.zoomTo()\n",
        "view.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxsFy6qkmgkP"
      },
      "source": [
        "#The Graph Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI8iz25ZnO21"
      },
      "source": [
        "### Input features\n",
        "\n",
        "1. One hot encoded atom information\n",
        "\n",
        "2. One hot encoded residue information\n",
        "\n",
        "### GCN update equations\n",
        "\n",
        "\n",
        "$$\n",
        "h_{i}^{(l)}=\\sigma\\left( W_{\\mathrm{center}}^{(l)} h_i^{(l-1)}  +\\frac{1}{|\\mathcal{N}_{\\mathrm{same}}(i)|}\\sum_{j\\in \\mathcal{N}_{\\mathrm{same}}(i)}{W_{\\mathrm{same}}^{(l)} h_j^{(l-1)}  } +\\frac{1}{|\\mathcal{N}_{\\mathrm{other}}(i)|}\\sum_{j\\in \\mathcal{N}_{\\mathrm{other}}(i)}{W_{\\mathrm{other}}^{(l)} h_j^{(l-1)} } \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "h_{i}^{(1)}=\\sigma\\left( W_{\\mathrm{center}}^{(0)} h_i^{(0)} + W_{\\mathrm{residue}} r_i +\\frac{1}{|\\mathcal{N}_{\\mathrm{same}}(i)|}\\sum_{j\\in \\mathcal{N}_{\\mathrm{same}}(i)}{W_{\\mathrm{same}}^{(0)} h_j^{(0)}  } +\\frac{1}{|\\mathcal{N}_{\\mathrm{other}}(i)|}\\sum_{j\\in \\mathcal{N}_{\\mathrm{other}}(i)}{W_{\\mathrm{other}}^{(0)} h_j^{(0)} } \\right)\n",
        "$$\n",
        "\n",
        "$h_i^{(0)}$: atom one-hot-encoding\n",
        "\n",
        "$r_i$ residue one-hot-encoding\n",
        "\n",
        "$\\mathcal{N}_{\\mathrm{same}}(i)$: the set of neighbouring atoms of atom $i$ that are within the same residue\n",
        "\n",
        "\n",
        "$\\mathcal{N}_{\\mathrm{other}}(i)$: the set of neighbouring atoms of atom $i$ that are from different residues\n",
        "\n",
        "$\\sigma$: the activation function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKWpBoJ2_Ni_"
      },
      "source": [
        "### Steps for computing GCN output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5xoap9pBIB1"
      },
      "source": [
        "In the following example we assume four different atom types: Carbon, Oxygen, Nitrogen, and Hydrogen.  Their one-hot-encoding is  [1,0,0,0] ,[0,1,0,0],[0,0,1,0] and [0,0,0,1], respectively.\n",
        "\n",
        "#### Computing the node signals, $W_{\\mathrm{center}}^{(0)} h^{(0)}$:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ__Lem1_WWl"
      },
      "outputs": [],
      "source": [
        "one_hot_encoded_atom=torch.tensor([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],dtype=torch.float32)\n",
        "Wv = torch.randn(4, 10)\n",
        "node_signals=one_hot_encoded_atom @ Wv\n",
        "node_signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8MjRCdDYfP3"
      },
      "source": [
        "#### Computing $\\frac{1}{|\\mathcal{N}_{i}|}\\sum_{j\\in \\mathcal{N}_{i}} W_{\\mathrm{neighbors}}^{(l)} h_j^{(l-1)} $\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBJglgIuBfmO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# neigh_info:\n",
        "# the indices of the closest neighbours of the atoms of a protein.\n",
        "# In this example we have 4 atoms with 3 neighbours per atom.\n",
        "# If no neighbour is present, the index is -1.\n",
        "neigh_info=torch.tensor(\n",
        "                         [\n",
        "                          [1,3,-1],   #neighbours of first atom\n",
        "                          [0,2,-1],   #neighbours of second atom\n",
        "                          [1,3,-1],   #neighbours of third atom\n",
        "                          [2,0,-1]    #neighbours of fourth atom\n",
        "                         ]\n",
        "                        )\n",
        "atom_feat=torch.randn((4,6))\n",
        "# atom_feat:\n",
        "# the features of all the atoms in a protein\n",
        "# Its dimensionality is [4,6] where  4 is the number of atoms and 6 is dimensionality\n",
        "# of the embedding space of each atom\n",
        "\n",
        "neigh_info.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tRgvftTCdzD"
      },
      "outputs": [],
      "source": [
        "print(f'Atom features \\n{atom_feat}')\n",
        "print(f'Neighbour information \\n{neigh_info}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EafQTIwKhNiO"
      },
      "source": [
        "Computing the neighbour signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yesrVFtyCjvF"
      },
      "outputs": [],
      "source": [
        "W_n = torch.randn((6,10),dtype=torch.float32)\n",
        "neigh_signals=atom_feat@W_n\n",
        "neigh_signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xz_eoAChp3V"
      },
      "source": [
        "Boolean matrix that indicates presence of a neighboring atom:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP-TwMuy-eOd"
      },
      "outputs": [],
      "source": [
        "neigh_indicator=(neigh_info>-1)\n",
        "neigh_indicator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE6U5eMpl3Oa"
      },
      "source": [
        "Reshaping it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBBtRU-QmqRp"
      },
      "outputs": [],
      "source": [
        "unsqueezed_neigh_indicator=neigh_indicator.unsqueeze(2)\n",
        "unsqueezed_neigh_indicator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyC5MBmR4e0K"
      },
      "source": [
        "Now its shape matches the shape of the neighbor signals:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUKfchWz3xOL"
      },
      "outputs": [],
      "source": [
        "unsqueezed_neigh_indicator.shape, neigh_signals[neigh_info].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RrIEdeAqvIi"
      },
      "source": [
        "Next, we compute the features of neighbouring atoms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7jc41l1lCCY"
      },
      "outputs": [],
      "source": [
        "neigh_features=neigh_signals[neigh_info]*unsqueezed_neigh_indicator\n",
        "print (neigh_features.shape)\n",
        "neigh_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYQ11eXSu1O2"
      },
      "source": [
        "Next step:  $\\sum_{j\\in \\mathcal{N}_{i}} W_{\\mathrm{neighbors}}^{(l)} h_j^{(l-1)} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-OV-4Iss_OC"
      },
      "outputs": [],
      "source": [
        "sum_neigh=torch.sum(neigh_features, 1)\n",
        "print(sum_neigh.shape)\n",
        "sum_neigh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwXwWjBlrkVs"
      },
      "source": [
        "We still need to divide by the number of neighbors.  First, find the total number of  neighbouring atoms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GiaOaVWnF_3"
      },
      "outputs": [],
      "source": [
        "num_neigh=torch.sum(neigh_info>-1,1)\n",
        "num_neigh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPBvwNRxy0MA"
      },
      "outputs": [],
      "source": [
        "num_neigh = num_neigh.unsqueeze(1)\n",
        "# To prevent divide by 0 error\n",
        "num_neigh[num_neigh==0]=1\n",
        "print(num_neigh.shape)\n",
        "num_neigh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ5ULPSt8K4w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFnbxxRp8UM_"
      },
      "source": [
        "Now its shape matches the shape of `sum_neigh`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw45KuWG79dE"
      },
      "outputs": [],
      "source": [
        "sum_neigh.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJal8PbtvPiy"
      },
      "source": [
        "Finally, we can compute $\\frac{1}{|\\mathcal{N}_{i}|}\\sum_{j\\in \\mathcal{N}_{i}} W_{\\mathrm{neighbors}}^{(l)} h_j^{(l-1)} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIRL7Du7r43C"
      },
      "outputs": [],
      "source": [
        "final_neigh_features=(sum_neigh/num_neigh)\n",
        "final_neigh_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13e7lii98CI1"
      },
      "source": [
        "\n",
        "## GNN code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqI1QAF1l9cM"
      },
      "outputs": [],
      "source": [
        "class GNN_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, v_feats, trainable=True, **kwargs):\n",
        "\n",
        "        super(GNN_Layer, self).__init__()\n",
        "        self.v_feats = v_feats\n",
        "        self.filters = filters\n",
        "\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device=device\n",
        "        self.Wsv = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "\n",
        "    def forward(self, x):\n",
        "        Z,same_neigh,diff_neigh = x\n",
        "        print(\"input\",x)\n",
        "        print(Z.shape)\n",
        "        node_signals = Z@self.Wsv\n",
        "        neigh_signals_same=Z@self.Wsr\n",
        "        neigh_signals_diff=Z@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal = (torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal = (torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "        final_res = torch.relu(node_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res,same_neigh,diff_neigh\n",
        "\n",
        "class GNN_First_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, trainable=True, **kwargs):\n",
        "\n",
        "        super(GNN_First_Layer, self).__init__()\n",
        "        self.filters = filters\n",
        "\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "        self.Wv = nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wr = nn.Parameter( torch.randn(21,self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        atoms, residues,same_neigh,diff_neigh = x\n",
        "        node_signals = atoms@self.Wv\n",
        "        residue_signals = residues@self.Wr\n",
        "        neigh_signals_same=atoms@self.Wsr\n",
        "        neigh_signals_diff=atoms@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal=(torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal=(torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "\n",
        "        final_res = torch.relu(node_signals+residue_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "        return final_res, same_neigh,diff_neigh\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, in_dims, out_dims, trainable=True, **kwargs):\n",
        "\n",
        "        super(Dense, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.out_dims = out_dims\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "\n",
        "        self.W = nn.Parameter( torch.randn(self.in_dims, self.out_dims, device=self.cuda_device,requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        Z = torch.sigmoid(torch.matmul(x, self.W))\n",
        "\n",
        "        return Z\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GNN_First_Layer(filters=128)\n",
        "        self.conv2 = GNN_Layer(v_feats=128, filters=256)\n",
        "        self.conv3 = GNN_Layer(v_feats=256, filters=512)\n",
        "        self.dense = Dense(in_dims=512, out_dims=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1=self.conv1(x)\n",
        "        x2=self.conv2(x1)\n",
        "        x3=self.conv3(x2)\n",
        "        x=x3[0]\n",
        "        x=torch.sum(x,axis=0).view(1,-1)\n",
        "        x = F.normalize(x)\n",
        "        x5=self.dense(x)\n",
        "        x6=torch.squeeze(x5,1)\n",
        "\n",
        "        return x6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwoSSfAPDOCo"
      },
      "source": [
        "## GNN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kMKuQYXWwfX"
      },
      "outputs": [],
      "source": [
        "data_loc=(\"Data/train_data/*/*\")\n",
        "label_loc=(\"Data/train_label/\")\n",
        "workers=0\n",
        "batch_size=10\n",
        "loader_info=data1(data_loc,label_loc,batch_size,workers)\n",
        "\n",
        "train(GNN(),get_dataloader(loader_info))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOjfQlabDQuJ"
      },
      "source": [
        "## GNN Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qotMtxQCZQ7p"
      },
      "outputs": [],
      "source": [
        "data_loc=(\"Data/test_data/*/*\")\n",
        "label_loc=(\"Data/test_label/\")\n",
        "workers=0\n",
        "batch_size=10\n",
        "temp=data1(data_loc,label_loc,batch_size,workers)\n",
        "test(GNN(),get_dataloader(temp),'modelGCNL2_Global_Basic1.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcpU9kAt9bJl"
      },
      "source": [
        "### GNN performance on the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1qcV5zutVmI"
      },
      "outputs": [],
      "source": [
        "data_loc=(\"Data/train_data/*/*\")\n",
        "label_loc=(\"Data/train_label/\")\n",
        "workers=0\n",
        "batch_size=10\n",
        "temp=data1(data_loc,label_loc,batch_size,workers)\n",
        "test(GNN(),get_dataloader(temp),'modelGCNL2_Global_Basic1.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaWVorauHHDN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HxsFy6qkmgkP",
        "OI8iz25ZnO21",
        "UKWpBoJ2_Ni_",
        "f5xoap9pBIB1",
        "E8MjRCdDYfP3",
        "13e7lii98CI1",
        "VwoSSfAPDOCo",
        "HOjfQlabDQuJ",
        "AcpU9kAt9bJl"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}