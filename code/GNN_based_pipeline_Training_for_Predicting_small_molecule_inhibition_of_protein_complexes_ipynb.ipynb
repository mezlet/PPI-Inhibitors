{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mezlet/PPI-Inhibitors/blob/main/code/GNN_based_pipeline_Training_for_Predicting_small_molecule_inhibition_of_protein_complexes_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zy8pv8_EQa"
      },
      "source": [
        "**Set the Runtime->Change Runtime Type to GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLKfwNHlwnSE"
      },
      "source": [
        "# Protein 3d structure assessment with graph neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uOhzY0qAj-0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE6yIRsQ7C5Y",
        "outputId": "3bdabff8-ff98-4e22-e2be-16df5801cda2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.12/dist-packages (2025.9.1)\n",
            "Requirement already satisfied: biopython==1.81 in /usr/local/lib/python3.12/dist-packages (1.81)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision==0.17.2 in /usr/local/lib/python3.12/dist-packages (0.17.2)\n",
            "Requirement already satisfied: torchaudio==2.2.2 in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch-geometric==2.5.3 in /usr/local/lib/python3.12/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm==4.66.2 in /usr/local/lib/python3.12/dist-packages (4.66.2)\n",
            "Requirement already satisfied: pandas==2.1.4 in /usr/local/lib/python3.12/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn==1.3.2 in /usr/local/lib/python3.12/dist-packages (1.3.2)\n",
            "Requirement already satisfied: matplotlib==3.8.3 in /usr/local/lib/python3.12/dist-packages (3.8.3)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: networkx==3.2.1 in /usr/local/lib/python3.12/dist-packages (3.2.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.2) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (1.16.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (3.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (2.32.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (3.2.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.2) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (25.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.6.85)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric==2.5.3) (3.10)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric==2.5.3) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric==2.5.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric==2.5.3) (2025.10.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install rdkit biopython==1.81 torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 torch-geometric==2.5.3 tqdm==4.66.2 pandas==2.1.4 numpy==1.26.4 scikit-learn==1.3.2 matplotlib==3.8.3 seaborn==0.13.2 networkx==3.2.1 gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_3U6PueCCgv",
        "outputId": "aae65515-2c18-42be-ef76-21036b2d1a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'PPI-Inhibitors': No such file or directory\n",
            "Cloning into 'PPI-Inhibitors'...\n",
            "remote: Enumerating objects: 1341, done.\u001b[K\n",
            "remote: Counting objects: 100% (536/536), done.\u001b[K\n",
            "remote: Compressing objects: 100% (404/404), done.\u001b[K\n",
            "remote: Total 1341 (delta 223), reused 395 (delta 129), pack-reused 805 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1341/1341), 2.59 GiB | 25.74 MiB/s, done.\n",
            "Resolving deltas: 100% (405/405), done.\n",
            "Updating files: 100% (605/605), done.\n"
          ]
        }
      ],
      "source": [
        "#!rm -r Data\n",
        "!rm -r PPI-Inhibitors\n",
        "# stay in Colab's /content directory (temporary storage)\n",
        "!pwd\n",
        "!git clone https://github.com/adibayaseen/PPI-Inhibitors.git\n",
        "%cd /content/PPI-Inhibitors\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PPI-Inhibitors\n",
        "!mkdir -p Data\n",
        "\n",
        "%cd Data\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/01ad4975fb9133825b1bf9e71b64fcdaaa5e4d8b/Data/2p2iComplexPairs.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/01ad4975fb9133825b1bf9e71b64fcdaaa5e4d8b/Data/2p2iInhibitorsSMILES.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/2d6bd03422602ec19147870c487e64018b52660f/Data/WriteAllexamplesRandomBindersIdsAll_24JAN_Binary.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/b1e45884f61f792399abad2e4492f48083ab1093/Data/BindersWithComplexname.csv\n",
        "%cd ..\n"
      ],
      "metadata": {
        "id": "0ESIfTCMdFyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive'\n",
        "\n",
        "# Make folders\n",
        "!mkdir -p GNN-PPI-Inhibitor\n",
        "!gdown --id 1goeDiPZSKT1Xx3j00eNG9xlqYkLLv1gW -O GNN-PPI-Inhibitor/ProteinData_dict.pickle\n",
        "!gdown --id 1GOYEKLQCoGea9QQ72kujy0rdJKbUSYAE -O GNN-PPI-Inhibitor/DBD5_ProteinData_dict.pickle"
      ],
      "metadata": {
        "id": "68vjaK6ddQh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kjV5-TPeGt-0"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan  9 16:43:15 2024\n",
        "\n",
        "@author: u1876024\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "class BalancedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class that creates a balanced dataset from imbalanced data.\n",
        "    This dataset calculates sample weights inversely proportional to class frequencies,\n",
        "    which can be used with a WeightedRandomSampler to achieve balanced batches.\n",
        "\n",
        "    NOTE: As it involves stochastic sampling, there is a chance that a few training examples are actually never selected.\n",
        "\n",
        "    Attributes:\n",
        "        data (array-like): The input data. Can be a list, NumPy array, or PyTorch tensor.\n",
        "        labels (array-like): The labels corresponding to the data. Should be a 1D array-like object.\n",
        "        sample_weights (torch.Tensor): Weights for each sample, inversely proportional to class frequencies.\n",
        "\n",
        "    Methods:\n",
        "        __len__: Returns the number of samples in the dataset.\n",
        "        __getitem__(idx): Returns the sample and its corresponding label at the given index.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "        # Count the number of examples in each class\n",
        "        class_counts = np.bincount(self.labels)\n",
        "        # Assign weight inversely proportional to class frequency\n",
        "        weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "        # Create a weight list for each sample\n",
        "        self.sample_weights = weights[labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "def create_balanced_loader(data, labels, batch_size=32):\n",
        "    \"\"\"\n",
        "    Creates a DataLoader with balanced batches for a given dataset.\n",
        "    This function is useful for training models on imbalanced datasets.\n",
        "\n",
        "    Args:\n",
        "        data (array-like): The input data. Can be a list, NumPy array, or PyTorch tensor.\n",
        "        labels (array-like): The labels corresponding to the data. Should be a 1D array-like object.\n",
        "        batch_size (int, optional): The size of each batch. Default is 32.\n",
        "\n",
        "    Returns:\n",
        "        DataLoader: A PyTorch DataLoader that yields balanced batches.\n",
        "\n",
        "    Usage Example:\n",
        "        >>> data = [features1, features2, ...]  # Replace with your data features\n",
        "        >>> labels = [label1, label2, ...]     # Replace with your data labels\n",
        "        >>> balanced_loader = create_balanced_loader(data, labels, batch_size=32)\n",
        "        >>> for batch_data, batch_labels in balanced_loader:\n",
        "        >>>     # Train your model using the balanced batches\n",
        "    \"\"\"\n",
        "    dataset = BalancedDataset(data, labels)\n",
        "    # WeightedRandomSampler will take care of the balancing\n",
        "    sampler = WeightedRandomSampler(weights=dataset.sample_weights, num_samples=len(dataset.sample_weights), replacement=True)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
        "    return loader\n",
        "\n",
        "\n",
        "class BinaryBalancedSampler(Sampler):\n",
        "    \"\"\"\n",
        "    A PyTorch Sampler that returns batches with an equal number of positive and negative examples.\n",
        "    The sampler oversamples from the minority class to balance the majority class, ensuring that\n",
        "    each batch contains 50% positive and 50% negative examples.\n",
        "\n",
        "    NOTE: It leads to more examples in single iteration through the data loader than in one epoch\n",
        "\n",
        "    Attributes:\n",
        "        class_vector (list or numpy array): class labels.\n",
        "        batch_size (int): The size of each batch.\n",
        "        n_splits (int): The number of batches/splits in the dataset.\n",
        "        equivalent_epochs (float): The number of times the sampler goes over the minority class\n",
        "                                   in one complete iteration of the DataLoader.\n",
        "\n",
        "    Methods:\n",
        "        gen_sample_array: Yields indices for each batch ensuring class balance.\n",
        "        __iter__: Returns an iterator over batch indices.\n",
        "        __len__: Returns the number of batches in the sampler.\n",
        "    \"\"\"\n",
        "    def __init__(self, class_vector, batch_size = 10):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        class_vector : torch tensor\n",
        "            a vector of class labels\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.class_vector = class_vector\n",
        "        YY = np.array(self.class_vector)\n",
        "        U, C = np.unique(YY, return_counts=True)\n",
        "        M = U[np.argmax(C)]        #find majority class\n",
        "        Midx = np.nonzero(YY==M)[0] #indices of majority class\n",
        "        midx = np.nonzero(YY!=M)[0] #indices of minority class\n",
        "        midx_ = np.random.choice(midx,size=len(Midx))     #oversample minority indices so they are equal to majority ones\n",
        "        self.YY = np.array(list(YY[Midx])+list(YY[midx_]))\n",
        "        self.idx = np.array(list(Midx)+list(midx_))\n",
        "        self.n_splits = int(np.ceil(len(self.idx)/self.batch_size))\n",
        "        self.equivalent_epochs = len(self.idx)/len(self.class_vector)\n",
        "        print('Equivalent epochs in one iteration of data loader',self.equivalent_epochs)\n",
        "\n",
        "    def gen_sample_array(self):\n",
        "        from sklearn.model_selection import StratifiedKFold\n",
        "        skf = StratifiedKFold(n_splits= self.n_splits,shuffle=True)\n",
        "        for tridx,ttidx in skf.split(self.idx,self.YY):\n",
        "            yield np.array(self.idx[ttidx])\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.gen_sample_array())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_splits\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    E = [(str(p_i), str(-1*c_i)) for p_i, c_i in zip(range(100), range(100))]  # Replace with your data\n",
        "    Y = np.random.randint(0, 2, size=100)  # Replace with your labels\n",
        "    batch_size = 10\n",
        "\n",
        "    dataset = CustomDataset(E, Y)\n",
        "    batch_sampler = BinaryBalancedSampler(Y, batch_size)\n",
        "    data_loader = DataLoader(dataset, batch_sampler=batch_sampler)\n",
        "\n",
        "    for batch in data_loader:\n",
        "        print(batch)\n",
        "\n",
        "    # Example usage of create_balanced_loader\n",
        "    balanced_loader = create_balanced_loader(E, Y, batch_size)\n",
        "\n",
        "    # Iterate over the DataLoader\n",
        "    L = []\n",
        "    for (pid,cid),label in balanced_loader:\n",
        "        # Process your batches\n",
        "        L.extend(pid)\n",
        "    print(L)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hn0xj2yWfAYq"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan  9 13:06:21 2024\n",
        "\n",
        "@author: u1876024\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import glob\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import glob\n",
        "from Bio.PDB import *\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "from Bio.PDB.NeighborSearch import NeighborSearch\n",
        "from tqdm import tqdm as tqdm\n",
        "import pickle\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score\n",
        "import pickle\n",
        "from rdkit import Chem\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "from torch.autograd import Variable\n",
        "def cuda(v):\n",
        "    if USE_CUDA:\n",
        "        return v.cuda()\n",
        "    return v\n",
        "def toTensor(v,dtype = torch.float,requires_grad = False):\n",
        "    return cuda(Variable(torch.tensor(v)).type(dtype).requires_grad_(requires_grad))\n",
        "def toNumpy(v):\n",
        "    if USE_CUDA:\n",
        "        return v.detach().cpu().numpy()\n",
        "    return v.detach().numpy()\n",
        "\n",
        "'''\n",
        "Using Sklearn One hot encoder to encode the atoms\n",
        "Output is of size N*M where N is the total number of atoms and M is the total number of encoded features\n",
        "'''\n",
        "def atom1(structure):\n",
        "    atomslist=np.array(sorted(np.array(['C', 'CA', 'CB', 'CG', 'CH2', 'N','NH2',  'OG','OH', 'O1', 'O2', 'SE','1']))).reshape(-1,1)\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc.fit(atomslist)\n",
        "    atom_list=[]\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_name() in atomslist:\n",
        "            atom_list.append(atom.get_name())\n",
        "        else:\n",
        "            atom_list.append(\"1\")\n",
        "    atoms_onehot=enc.transform(np.array(atom_list).reshape(-1,1)).toarray()\n",
        "    return atoms_onehot\n",
        "##############\n",
        "'''\n",
        "One hot encoded residue infomration using SKlearn Library\n",
        "\n",
        "Output is N*M where N is the total number of atoms and M is the encoded features of the residues.\n",
        "Any unknown  residue is mapped to 1\n",
        "'''\n",
        "\n",
        "\n",
        "def res1(structure):\n",
        "    residuelist=np.array(sorted(np.array(['ALA', 'ARG', 'ASN', 'ASP', 'GLN', 'GLU', 'GLY', 'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL', 'CYS', 'HIS','1']))).reshape(-1,1)\n",
        "    encr = OneHotEncoder(handle_unknown='ignore')\n",
        "    encr.fit(residuelist)\n",
        "    residue_list=[]\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_parent().get_resname() in residuelist:\n",
        "            residue_list.append((atom.get_parent()).get_resname())\n",
        "        else:\n",
        "            residue_list.append(\"1\")\n",
        "\n",
        "    res_onehot=encr.transform(np.array(residue_list).reshape(-1,1)).toarray()\n",
        "\n",
        "    return res_onehot\n",
        "###########\n",
        "\n",
        "'''\n",
        "It calculates the neighbours of each atom i.e. 10 distinct neighbours\n",
        "Output is  in the form of a ditionary representing an  adjacency list where each source atom and neighbouring atom is represented bby its sequence index .\n",
        "'''\n",
        "\n",
        "\n",
        "def neigh1(structure):\n",
        "    #atom_list is a numpy array  that   contains all the atoms of the pdb file in atom object\n",
        "    atom_list=np.array([atom for atom in structure.get_atoms()])\n",
        "\n",
        "    #for atom in structure.get_atoms():\n",
        "    #    atom_list.append(atom)\n",
        "    #neighbour_list contains all the  neighbour atomic pairs  i.e. like if N has neighbours O and C then it is stored as [[N,C],[N,O]] i.e. has dimension N*2 where N is the total number of possible neighbours all the atoms have in an unsorted manner and it stores in the form of  atom object\n",
        "\n",
        "\n",
        "    p4=NeighborSearch(atom_list)\n",
        "    neighbour_list=p4.search_all(6,level=\"A\")\n",
        "    neighbour_list=np.array(neighbour_list)\n",
        "\n",
        "    #dist is the distance between the neighbour and the source atom  i.e. dimension is N*1\n",
        "    dist=np.array(neighbour_list[:,0]-neighbour_list[:,1])\n",
        "    #sorting in ascending order\n",
        "    place=np.argsort(dist)\n",
        "    sorted_neighbour_list=neighbour_list[place]\n",
        "\n",
        "    #old_atom_number is used for  storing atom id of the original protein before sorting\n",
        "    #old_residue_number is used for storing residue number of the original protein before sorting\n",
        "    source_vertex_list_atom_object=np.array(sorted_neighbour_list[:,0])\n",
        "    len_source_vertex=len(source_vertex_list_atom_object)\n",
        "    neighbour_vertex_with_respect_each_source_atom_object=np.array(sorted_neighbour_list[:,1])\n",
        "    old_atom_number=[]\n",
        "    old_residue_number=[]\n",
        "    for i in atom_list:\n",
        "        old_atom_number.append(i.get_serial_number())\n",
        "        old_residue_number.append(i.get_parent().get_id()[1])\n",
        "    old_atom_number=np.array(old_atom_number)\n",
        "    old_residue_number=np.array(old_residue_number)\n",
        "    req_no=len(neighbour_list)\n",
        "    total_atoms=len(atom_list)\n",
        "    #neigh_same_res is the 2D numpy array to store the indices of the  neighbours of  same residue and is of the shape N*10 where N is the total number of atoms\n",
        "    #neigh_diff_res is 2D numpy array to store  the indices of the  neighbours of different residue\n",
        "    #same_flag is used to restrict the neighbours belonging to same residue  to 10\n",
        "    #diff_flag is used to restrict the neighbours belonging to different residue to 10\n",
        "    neigh_same_res=np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    neigh_diff_res=np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    same_flag=[0]*total_atoms\n",
        "    diff_flag=[0]*total_atoms\n",
        "    for i in range(len_source_vertex):\n",
        "        source_atom_id=source_vertex_list_atom_object[i].get_serial_number()\n",
        "        neigh_atom_id=neighbour_vertex_with_respect_each_source_atom_object[i].get_serial_number()\n",
        "        source_atom_res=source_vertex_list_atom_object[i].get_parent().get_id()[1]\n",
        "        neigh_atom_res=neighbour_vertex_with_respect_each_source_atom_object[i].get_parent().get_id()[1]\n",
        "        #finding out index of the source and neighbouring atoms from the original atom array with respect to their residue id and atom id\n",
        "        temp_index1=np.where(source_atom_id==old_atom_number)[0]\n",
        "\n",
        "        temp_index2=np.where(neigh_atom_id==old_atom_number)[0]\n",
        "        for i1 in temp_index1:\n",
        "            if old_residue_number[i1]==source_atom_res:\n",
        "                source_index=i1\n",
        "                break\n",
        "        for i1 in temp_index2:\n",
        "            if old_residue_number[i1]==neigh_atom_res:\n",
        "                neigh_index=i1\n",
        "                break\n",
        "        #if both the residues are same\n",
        "\n",
        "        if source_atom_res==neigh_atom_res :\n",
        "\n",
        "            #limiting the number of neighbours of same residue to 10\n",
        "\n",
        "            if int(same_flag[source_index])< 10:\n",
        "                neigh_same_res[source_index][same_flag[source_index]]=neigh_index\n",
        "                same_flag[source_index]+=1\n",
        "\n",
        "            if int(same_flag[neigh_index])< 10:\n",
        "                neigh_same_res[neigh_index][same_flag[neigh_index]]=source_index\n",
        "                same_flag[neigh_index]+=1\n",
        "\n",
        "        # if both the residues are different\n",
        "        elif source_atom_res!=neigh_atom_res :\n",
        "\n",
        "            #limiting the number of neighbours of different residues to 10\n",
        "\n",
        "            if int(diff_flag[source_index])< 10:\n",
        "                neigh_diff_res[source_index][diff_flag[source_index]]=neigh_index\n",
        "                diff_flag[source_index]+=1\n",
        "\n",
        "\n",
        "            if int(diff_flag[neigh_index])< 10:\n",
        "\n",
        "                neigh_diff_res[neigh_index][diff_flag[neigh_index]]=source_index\n",
        "                diff_flag[neigh_index]+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return neigh_same_res,neigh_diff_res\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available. Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n",
        "class GNN_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, v_feats, trainable=True, **kwargs):\n",
        "        #pdb.set_trace()\n",
        "        super(GNN_Layer, self).__init__()\n",
        "        self.v_feats = v_feats\n",
        "        self.filters = filters\n",
        "\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device=device\n",
        "        self.Wsv = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "        #print(\"Wsv shape\",self.Wsv.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        Z,same_neigh,diff_neigh = x\n",
        "        node_signals = Z@self.Wsv\n",
        "        neigh_signals_same=Z@self.Wsr\n",
        "        neigh_signals_diff=Z@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal = (torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal = (torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "        final_res = torch.relu(node_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res,same_neigh,diff_neigh\n",
        "\n",
        "class GNN_First_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, trainable=True, **kwargs):\n",
        "\n",
        "        super(GNN_First_Layer, self).__init__()\n",
        "        self.filters = filters\n",
        "        #pdb.set_trace()\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "        self.Wv = nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wr = nn.Parameter( torch.randn(21,self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        atoms, residues,same_neigh,diff_neigh = x\n",
        "        node_signals = atoms@self.Wv\n",
        "        residue_signals = residues@self.Wr\n",
        "        neigh_signals_same=atoms@self.Wsr\n",
        "        neigh_signals_diff=atoms@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        \"\"\"\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(1)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(1)\n",
        "        \"\"\"\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        #print(\"same norm\",same_neigh > -1, 1)\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        same_norm = torch.sum(same_neigh > -1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1).type(torch.float)\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal=(torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal=(torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "\n",
        "        final_res = torch.relu(node_signals+residue_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res, same_neigh,diff_neigh\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, in_dims, out_dims, trainable=True, **kwargs):\n",
        "        #pdb.set_trace()\n",
        "        super(Dense, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.out_dims = out_dims\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "\n",
        "        self.W = nn.Parameter( torch.randn(self.in_dims, self.out_dims, device=self.cuda_device,requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        Z = torch.sigmoid(torch.matmul(x, self.W))\n",
        "\n",
        "        return Z\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GNN_First_Layer(filters=512)\n",
        "        self.conv2 = GNN_Layer(v_feats=512, filters=1024)\n",
        "        self.conv3 = GNN_Layer(v_feats=1024, filters=512)\n",
        "        self.dense = Dense(in_dims=512, out_dims=1)\n",
        "    def forward(self, x):\n",
        "        x1=self.conv1(x)\n",
        "        x2=self.conv2(x1)\n",
        "        x3=self.conv3(x2)\n",
        "        x=x3[0]\n",
        "        x=torch.sum(x,axis=0).view(1,-1)\n",
        "        x = F.normalize(x)\n",
        "        return x\n",
        "\n",
        "    def processProtein(UniqueProtein, PdBloc):#, PdBloc):\n",
        "        data_list = []\n",
        "\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        PData_dict={}\n",
        "        for i in range(len(UniqueProtein)):\n",
        "            #print('Converting PDB to Graph: {}/{}'.format(i+1, len(UniqueProtein)))\n",
        "            UniqueProtein[i]=UniqueProtein[i].split('.pdb')[0]\n",
        "            P1=PdBloc+UniqueProtein[i]+'.pdb'\n",
        "            #if P1 in UniqueProtein:\n",
        "            parser = PDBParser()\n",
        "            with warnings.catch_warnings(record=True) as w:\n",
        "              structure = parser.get_structure(\"\", P1)\n",
        "            one_hot_atom=(atom1(structure))\n",
        "\n",
        "            one_hot_res=(res1(structure))\n",
        "            neigh_same_res,neigh_diff_res=(neigh1(structure))\n",
        "            # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "            one_hot_atom=torch.tensor(one_hot_atom,dtype=torch.float32).to(device)\n",
        "            one_hot_res=torch.tensor(one_hot_res,dtype=torch.float32).to(device)\n",
        "            neigh_same_res=torch.tensor(neigh_same_res).to(device).long()\n",
        "            neigh_diff_res=torch.tensor(neigh_diff_res).to(device).long()\n",
        "            GNNData = [one_hot_atom,one_hot_res,neigh_same_res,neigh_diff_res]\n",
        "            PData_dict[UniqueProtein[i]]= GNNData\n",
        "        return PData_dict\n",
        "def readFile(filename):\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  Name=[];PdbId=[];Ligandnames=[];SMILES=[];labels=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      #if len(d)==6:\n",
        "      name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "      Name.append(name);PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);labels.append(float (y));\n",
        "  return  PdbId,Ligandnames,SMILES,labels\n",
        "class IPPI_MLP_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IPPI_MLP_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2840, 1024)#4096)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 100)\n",
        "        self.fc6 = nn.Linear(100, 1)\n",
        "    def forward(self, PFeatures,LigandFeatures,ProteinInterfaceF):\n",
        "          Cfeatures=LigandFeatures\n",
        "          P_all_Features=torch.hstack((PFeatures,ProteinInterfaceF))\n",
        "          PC_Features=torch.hstack((P_all_Features,Cfeatures))\n",
        "          x = torch.tanh(self.fc1(PC_Features))\n",
        "          x = torch.tanh(self.fc2(x))\n",
        "          x = torch.relu(self.fc3(x))\n",
        "          x = self.fc6(x)\n",
        "          return x\n",
        "\n",
        "#path,githubpath='/content/drive/MyDrive/GNN-PPI-Inhibitor/','/content/PPI-Inhibitors/'\n",
        "path,githubpath='/content/drive/MyDrive/GNN-PPI-Inhibitor/','/content/PPI-Inhibitors/'\n",
        "\"\"\"\n",
        "path = location of pkl files from these links:\n",
        "https://drive.google.com/file/d/1goeDiPZSKT1Xx3j00eNG9xlqYkLLv1gW/view\n",
        "https://drive.google.com/file/d/1GOYEKLQCoGea9QQ72kujy0rdJKbUSYAE/view\n",
        "\n",
        "githubpath =  location of the directory containing the github repo PPI-Inhibitors\n",
        "obtained using\n",
        "git clone https://github.com/adibayaseen/PPI-Inhibitors\n",
        "\"\"\"\n",
        "Ubench5InterfaceandSeq_dict=pickle.load(open(githubpath+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "Pos_seqandInterfaceF_dict=pickle.load(open(githubpath+'Features/Pos_seqandInterfaceF_dict.npy',\"rb\"))\n",
        "Complex_AllFeatures_dict=dict( list (Pos_seqandInterfaceF_dict.items())+list (Ubench5InterfaceandSeq_dict.items()))\n",
        "##############\n",
        "ComplexInterfaceFeatures={}\n",
        "for key in Complex_AllFeatures_dict:\n",
        "  if len(key.split('_'))>1:\n",
        "    compname=key.split('_')[0]\n",
        "    ComplexInterfaceFeatures[compname]=Complex_AllFeatures_dict[key]#.cuda()\n",
        "  else:\n",
        "    ComplexInterfaceFeatures[key]=Complex_AllFeatures_dict[key]#.cuda()\n",
        "CompoundFingerprintFeaturesDict=pickle.load(open(githubpath+'Features/Compound_Fingerprint_Features_Dict.npy',\"rb\"))\n",
        "#Load Protein data for GNN\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "ProteinDataGNN_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "DBD5_ProteinDataGNN_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "All_ProteinData_dict=dict( list (ProteinDataGNN_dict.items())+list (DBD5_ProteinDataGNN_dict.items()))\n",
        "for d in All_ProteinData_dict:\n",
        "  data=All_ProteinData_dict[d]\n",
        "  All_ProteinData_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "#########\n",
        "with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN_Binary.txt') as f:\n",
        "#with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN.txt') as f:\n",
        "    D = f.readlines()\n",
        "Labels=[];Ligandnames=[];Complexs=[];TestPoscomplexes=[];#SMILESlist=[];\n",
        "for d in tqdm(D):\n",
        "  if len(d.split())==4:\n",
        "      TestPoscomp,Complexname,Ligandname,label = d.split()\n",
        "  else:\n",
        "      TestPoscomp,Complexname,Ligandname,label = d.split()[0],d.split()[1],(' ').join(d.split()[2:-1]),d.split()[-1]\n",
        "  TestPoscomplexes.append(TestPoscomp),Ligandnames.append(Ligandname);Complexs.append(Complexname);Labels.append(float (label))\n",
        "#########Make dictionary, Rootcomplexname=(complexname,compoundname),label\n",
        "Allexamples=dict (zip(zip(TestPoscomplexes,zip(Complexs,Ligandnames)),Labels))\n",
        "#Group kfold\n",
        "Alldata=list (Allexamples.keys())\n",
        "KK=[k[0].split('_')[0] for k in Alldata]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "AUC_ROC_final=[];Avg_P_final=[];Z=[];Yo=[];Y_t=[];Y_score=[];\n",
        "from os import listdir\n",
        "AlltestExamples=[];Externallabels=[];ExternalscoresLOCO=[];covid19_Externallabels=[];covid19_ExternalscoresLOCO=[];Y_score=[];Y_t=[];classratio_dict={};\n",
        "AUC_ROC_final=[];Avg_P_final=[];\n",
        "Complexs,Ligandnames, Labels=np.array(Complexs),np.array(Ligandnames),np.array(Labels)\n",
        "Alldata=np.array(Alldata, dtype=object)\n",
        "classratio_dict=pickle.load(open(githubpath+'Features/Classratio_GNNdict.npy','rb'))\n",
        "\n",
        "#%% Cross-validation\n",
        "Done=set(KK).difference(['3D9T','1BKD','4ESG','2FLU','1YCQ','2XA0','3TDU','3D9T','2B4J','3DAB','3UVW','2RNY','4AJY', '1F47','1YCR','4QC3','1NW9','2E3K','4YY6','4GQ6','3WN7','1BXL','1Z92'])\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    train,test=Alldata[trainindex],Alldata[testindex]\n",
        "\n",
        "    if test[0][0].split('_')[0] in Done:\n",
        "      continue\n",
        "\n",
        "    Ctr=[];Ptr=[];y_train=[];Ctrname=[];Ptrname=[];Xtr=[];G=[];Cttname=[];Ctt=[];y_test=[];Ptt=[];Pttname=[];\n",
        "    #Split train and test\n",
        "    for t in train:\n",
        "        Ctrname.append(t[1][1]);Ctr.append(CompoundFingerprintFeaturesDict[t[1][1]]);\n",
        "        #change this only for GNN Complex_AllFeatures_dict with All_ProteinData_dic and t\n",
        "        #####\n",
        "        GNNcomp=t[1][0].split('_')[0]#t[1][0].split('_')[0]\n",
        "        Ptrname.append(GNNcomp);Ptr.append(ComplexInterfaceFeatures[GNNcomp]);\n",
        "        y_train.append(Allexamples[t[0],t[1]])\n",
        "    #Split train and test\n",
        "    for t in test:\n",
        "        GNNcomp=t[1][0].split('_')[0]\n",
        "        Cttname.append(t[1][1]);Ctt.append(CompoundFingerprintFeaturesDict[t[1][1]]);\n",
        "        Pttname.append(GNNcomp);Ptt.append(ComplexInterfaceFeatures[GNNcomp]);\n",
        "        y_test.append(Allexamples[t[0],t[1]])\n",
        "    #standarization\n",
        "    Pscaler = StandardScaler().fit(Ptr)\n",
        "    Cscaler = StandardScaler().fit(Ctr)\n",
        "    Ctr = Cscaler.transform(Ctr)\n",
        "    Ptr=Pscaler.transform(Ptr)\n",
        "    Ptt=Pscaler.transform(Ptt)\n",
        "    Ptrdict=dict (zip(Ptrname,torch.FloatTensor(Ptr).cuda()))\n",
        "    Ctrdict=dict (zip (Ctrname,torch.FloatTensor( Ctr).cuda()))\n",
        "    Ctt = Cscaler.transform(Ctt)\n",
        "    Cttdict=dict (zip (Cttname,torch.FloatTensor( Ctt).cuda()))\n",
        "    Pttdict=dict (zip(Pttname,torch.FloatTensor(Ptt).cuda()))\n",
        "\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "\n",
        "    GNN_model=GNN().cuda()\n",
        "    Mcomplexname=test[0][0].split('_')[0]\n",
        "    criterion  = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(list (IPPI_Net.parameters()) + list( GNN_model.parameters()),lr=0.001,weight_decay=0.0)#001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "    bsize = 1024\n",
        "\n",
        "\n",
        "\n",
        "    dataset = CustomDataset(train[:,1], y_train.astype('int'))\n",
        "    batch_sampler = BinaryBalancedSampler(y_train.astype('int'), bsize)\n",
        "    loader = DataLoader(dataset, batch_sampler=batch_sampler) # data loader that selects equal number of positive and negative examples\n",
        "\n",
        "    test_dataset = CustomDataset(test[:,1], np.array(y_test).astype('int'))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bsize, shuffle=False)\n",
        "\n",
        "\n",
        "    #y_train=torch.FloatTensor( y_train).cuda()\n",
        "    ####\n",
        "    print (\"test complex \", Mcomplexname)\n",
        "\n",
        "    Loss = [] #save loss values for plotting\n",
        "    E = [] #save examples\n",
        "    L = [] #save labels\n",
        "    terminated = False\n",
        "    best_result = 0.0\n",
        "    best_model = None\n",
        "    counter = 0\n",
        "    early_stop_count = 0\n",
        "    Zlist,Ylist=[],[]\n",
        "    for iters in tqdm(range(5)):\n",
        "        for (batch_pids,batch_cids),batch_labels in tqdm(loader):\n",
        "            GNN_model.train()\n",
        "            IPPI_Net.train()\n",
        "            E.extend(zip(batch_pids,batch_cids))\n",
        "            L.append(batch_labels)\n",
        "            pids = [p.split('_')[0] for p in batch_pids]\n",
        "            G_dict = {p:GNN_model(All_ProteinData_dict[p]) for p in set(pids)} #pass each unique complex through the GNN once\n",
        "            GNN_features = torch.vstack([G_dict[p] for p in pids]) #append to make examples\n",
        "            del G_dict #clear up memory\n",
        "            interface_features = torch.vstack([Ptrdict[p] for p in pids])\n",
        "            compound_features = torch.vstack([Ctrdict[c] for c in batch_cids])\n",
        "            #[GNN_model(All_ProteinData_dict[p]) for p in set_pids]\n",
        "            output = IPPI_Net(GNN_features,compound_features,interface_features)\n",
        "            V = np.min(list(classratio_dict.values()))\n",
        "            weights = toTensor(np.array([classratio_dict[p]/V if batch_labels[i]==1 else 1.0 for i,p in enumerate(pids)  ]))\n",
        "            criterion  = nn.BCEWithLogitsLoss(weight = None)\n",
        "            loss = criterion(output.flatten(), batch_labels.float().cuda())\n",
        "            Loss.append(loss.item())\n",
        "            #if np.median(Loss[-10:])<1e-1: terminated = True\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            early_stop_count += 1\n",
        "            #%% Validation/Testing (saves the best model in every 10 iterations over the validation set)\n",
        "            GNN_model.eval()\n",
        "            IPPI_Net.eval()\n",
        "            Z, Y = [], []\n",
        "            with torch.no_grad():\n",
        "                for (batch_pids,batch_cids),batch_labels in test_loader:\n",
        "                    pids = [p.split('_')[0] for p in batch_pids]\n",
        "                    G_dict = {p:GNN_model(All_ProteinData_dict[p]) for p in set(pids)} #pass each unique complex through the GNN once\n",
        "                    GNN_features = torch.vstack([G_dict[p] for p in pids]) #append to make examples\n",
        "                    del G_dict #clear up memory\n",
        "                    interface_features = torch.vstack([Pttdict[p] for p in pids])\n",
        "                    compound_features = torch.vstack([Cttdict[c] for c in batch_cids])\n",
        "                    output = IPPI_Net(GNN_features,compound_features,interface_features)\n",
        "                    Z.extend(output.cpu().flatten().numpy())\n",
        "                    Y.extend(batch_labels.cpu().flatten().numpy())\n",
        "                aucroc = roc_auc_score(np.array(Y), np.array(Z))\n",
        "                aucpr = average_precision_score(Y,Z)\n",
        "                if aucroc>best_result:\n",
        "                    early_stop_count = 0\n",
        "                    best_result = aucroc\n",
        "                    best_model = (GNN_model.state_dict(),IPPI_Net.state_dict())\n",
        "                    IPPI_Net.load_state_dict(best_model[1])#path+'/newIPPI_Net_'+test[0][0].split('_')[0]+'_AUC_'+str (round (best_result,3)))#torch.load(path+'IPPI_Net_'+ Mcomplexname)[1])\n",
        "                    GNN_model.load_state_dict(best_model[0])\n",
        "                    GNN_model.eval()\n",
        "                    IPPI_Net.eval()\n",
        "                    Zb, Yb = [], []\n",
        "                    for (batch_pids,batch_cids),batch_labels in test_loader:\n",
        "                      pids = [p.split('_')[0] for p in batch_pids]\n",
        "                      G_dict = {p:GNN_model(All_ProteinData_dict[p]) for p in set(pids)} #pass each unique complex through the GNN once\n",
        "                      GNN_features = torch.vstack([G_dict[p] for p in pids]) #append to make examples\n",
        "                      del G_dict #clear up memory\n",
        "                      interface_features = torch.vstack([Pttdict[p] for p in pids])\n",
        "                      compound_features = torch.vstack([Cttdict[c] for c in batch_cids])\n",
        "                      bestmodeloutput=IPPI_Net(GNN_features,compound_features,interface_features)\n",
        "                      #torch.save(Loss, path+'/Loss_'+test[0][0].split('_')[0])\n",
        "                      Zb.extend(bestmodeloutput.cpu().flatten().numpy())\n",
        "                      Yb.extend(batch_labels.cpu().flatten().numpy())\n",
        "                    aucrocb = roc_auc_score(np.array(Yb), np.array(Zb))\n",
        "                    aucprb = average_precision_score(Yb,Zb)\n",
        "                    print('LOADED BEST AUCROC',aucrocb,'AUCPR',aucprb)#,'best aucroc')\n",
        "                    aucpr = average_precision_score(Y,Z)\n",
        "                    print('AUCROC',aucroc,'AUCPR',aucpr,'best aucroc',best_result)\n",
        "    ###Load best model\n",
        "    print (\"OUTSIDE LOOP AUC of Best\")\n",
        "    torch.save(best_model[1], path+'/GNN-based-pipeline_IPPI_Net_'+ test[0][0].split('_')[0])\n",
        "    torch.save(best_model[0], path+'/GNN-based-pipeline_GNN_model_'+ test[0][0].split('_')[0])\n",
        "    Zlist.extend(Zb);Ylist.extend(Yb)\n",
        "    np.save(path+test[0][0].split('_')[0]+'Scores',Zb)\n",
        "    np.save(path+test[0][0].split('_')[0]+'Targets',Yb)\n",
        "    aucrocb = roc_auc_score(np.array(Yb), np.array(Zb))\n",
        "    aucprb = average_precision_score(Yb,Zb)\n",
        "    print('Complex name',test[0][0].split('_')[0],'AUCROC',aucrocb,'AUCPR',aucprb)#,'best aucroc')\n",
        "fpr, tpr, thresholds = roc_curve(Ylist, Zlist)\n",
        "Auc = roc_auc_score(Ylist, Zlist)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Zlist=np.array(Zlist);Yo=np.array(Ylist);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Ylist, Zlist)\n",
        "aucpr=average_precision_score (Ylist, Zlist)\n",
        "########\n",
        "np.save(path+'GNN-pipeline_Targets.npy',Ylist)\n",
        "np.save(path+'GNN-pipeline_Scores.npy',Zlist)\n",
        "######+\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"GNN-pipeline AUC-PR for PPI Inhibitors.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr Over all complexes \\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])))#,\"\\n\")\n",
        "#######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"GNN-pipeline AUCROC for vPPI Inhibitors.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",p.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlgI8EnYxPoS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score,precision_recall_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "path,githubpath='/content/drive/MyDrive/GNN-PPI-Inhibitor/','/content/PPI-Inhibitors/'\n",
        "Z_GearNet=np.load(path+'onlyGearnet_Scores.npy')\n",
        "Yo_GearNet=np.load(path+'onlyGearnet_Targets.npy')\n",
        "####\n",
        "fpr_GearNet, tpr_GearNet, thresholds_GearNet = roc_curve(Yo_GearNet, Z_GearNet)\n",
        "Auc_GearNet = roc_auc_score(Yo_GearNet, Z_GearNet)\n",
        "Auc_GearNet=(Auc_GearNet).round(2)\n",
        "# calculate precision-recall curve\n",
        "precision_GearNet, recall_GearNet, thresholds = precision_recall_curve(Yo_GearNet, Z_GearNet)\n",
        "aucpr_GearNet=auc(recall_GearNet,precision_GearNet)\n",
        "aucpr_GearNet=(aucpr_GearNet).round(2)\n",
        "#######\n",
        "Yo_SVM=np.load(path+'All_SVM_Targets.npy')\n",
        "Z_SVM=np.load(path+'All_SVM_Scores.npy')\n",
        "####\n",
        "fpr_SVM, tpr_SVM, thresholds_SVM = roc_curve(Yo_SVM, Z_SVM)\n",
        "Auc_SVM = roc_auc_score(Yo_SVM, Z_SVM)\n",
        "Auc_SVM=(Auc_SVM).round(2)\n",
        "# calculate precision-recall curve\n",
        "precision_SVM, recall_SVM, thresholds = precision_recall_curve(Yo_SVM, Z_SVM)\n",
        "aucpr_SVM=auc(recall_SVM,precision_SVM)\n",
        "aucpr_SVM=(aucpr_SVM).round(2)\n",
        "#####Change this\n",
        "Yo_GNN=np.load(path+'GNN-pipeline_Targets.npy')\n",
        "Z_GNN=np.load(path+'GNN-pipeline_Scores.npy')\n",
        "##########\n",
        "fpr_GNN, tpr_GNN, thresholds_GNN = roc_curve(Yo_GNN, Z_GNN)\n",
        "Auc_GNN= roc_auc_score(Yo_GNN, Z_GNN)\n",
        "Auc_GNN=(Auc_GNN).round(2)\n",
        "# calculate precision-recall curve\n",
        "precision_GNN, recall_GNN, thresholds = precision_recall_curve(Yo_GNN, Z_GNN)\n",
        "aucpr_GNN=auc(recall_GNN,precision_GNN)\n",
        "aucpr_GNN=(aucpr_GNN).round(2)\n",
        "###### GNN LOCO average 0.8576 ± 0.0923 0.4366 ± 0.2003\n",
        "##### SVM LOCO average 0.7445 ± 0.1958 0.3312 ± 0.2017\n",
        "fig = plt.figure()\n",
        "Auc_GNN_std,PR_GNN_std,Auc_SVM_std,PR_SVM_std=0.0923,0.01,0.16,0.18\n",
        "Auc_GearNet_std,PR_GearNet_std=0.1,0.19\n",
        "#text=\"There is an upcoming task in %d days at %d cluster!\" %a %cluster\n",
        "plt.plot(fpr_GNN,tpr_GNN,color='m',marker=',',markersize=2,label = ('GNN AUC-ROC : $ {} ± {}$').format(round(Auc_GNN,2), round(Auc_GNN_std,2)))\n",
        "plt.plot(fpr_SVM,tpr_SVM,color='b',marker=',',markersize=2,label=('SVM AUC-ROC : $ {} ± {}$').format(round(Auc_SVM,2), round(Auc_SVM_std,2)))\n",
        "plt.plot(fpr_GearNet,tpr_GearNet,color='k',marker='.', markersize=3,label=('GearNet AUC-ROC : $ {} ± {}$').format(round(Auc_GearNet,2),round(Auc_GearNet_std,2)))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"Comaprison of AUCROC SVM and GNN-base model PPI Inhibitors Random and Binders combine Negative.pdf\", bbox_inches='tight')\n",
        "#########\n",
        "fig = plt.figure()\n",
        "plt.plot(recall_GNN,precision_GNN,color='m',marker=',',markersize=2,label=('GNN AUC-PR : $ {} ± {}$').format(round(aucpr_GNN,2), round(PR_GNN_std,2)))\n",
        "plt.plot(recall_SVM,precision_SVM,color='b',marker=',',markersize=2,label=('SVM AUC-PR: $ {} ± {}$').format(round(aucpr_SVM,2), round(PR_SVM_std,2)))\n",
        "plt.plot(recall_GearNet,precision_GearNet,color='k',marker='.', markersize=3,label=('GearNet AUC-PR: $ {} ± {}$').format(round(aucpr_GearNet,2), round(PR_GearNet_std,2)))\n",
        "plt.title('AUC-PR');plt.xlabel('Recall');plt.ylabel('Precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"Comaprison of AUC-PR SVM and GNN-base model PPI Inhibitors  Random and Binders combine.pdf\", bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HNk6-cjzYxrO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}