{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mezlet/PPI-Inhibitors/blob/main/code/GNN_based_pipeline_Training_for_Predicting_small_molecule_inhibition_of_protein_complexes_ipynb_end_to_end.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zy8pv8_EQa"
      },
      "source": [
        "**Set the Runtime->Change Runtime Type to GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLKfwNHlwnSE"
      },
      "source": [
        "# Protein 3d structure assessment with graph neural networks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fusermount -u /content/drive\n",
        "!rm -rf /content/drive\n",
        "%cd /content\n"
      ],
      "metadata": {
        "id": "E-cGEcP1jsvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_3U6PueCCgv",
        "outputId": "8b9b12b8-9115-425c-fbb1-5b4fa9a66a5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'PPI-Inhibitors': No such file or directory\n",
            "Cloning into 'PPI-Inhibitors'...\n",
            "remote: Enumerating objects: 1341, done.\u001b[K\n",
            "remote: Counting objects: 100% (536/536), done.\u001b[K\n",
            "remote: Compressing objects: 100% (404/404), done.\u001b[K\n",
            "remote: Total 1341 (delta 223), reused 395 (delta 129), pack-reused 805 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1341/1341), 2.59 GiB | 16.62 MiB/s, done.\n",
            "Resolving deltas: 100% (405/405), done.\n",
            "Updating files: 100% (605/605), done.\n"
          ]
        }
      ],
      "source": [
        "#!rm -r Data\n",
        "!rm -r PPI-Inhibitors\n",
        "!git clone https://github.com/adibayaseen/PPI-Inhibitors\n",
        "#!pip install py3Dmol"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install rdkit biopython==1.81 torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 torch-geometric==2.5.3 tqdm==4.66.2 pandas==2.1.4 numpy==1.26.4 scikit-learn==1.3.2 matplotlib==3.8.3 seaborn==0.13.2 networkx==3.2.1 gdown\n"
      ],
      "metadata": {
        "id": "SOpJqASVkFMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PPI-Inhibitors\n",
        "!mkdir -p Data\n",
        "\n",
        "%cd Data\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/01ad4975fb9133825b1bf9e71b64fcdaaa5e4d8b/Data/2p2iComplexPairs.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/01ad4975fb9133825b1bf9e71b64fcdaaa5e4d8b/Data/2p2iInhibitorsSMILES.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/2d6bd03422602ec19147870c487e64018b52660f/Data/WriteAllexamplesRandomBindersIdsAll_24JAN_Binary.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/b1e45884f61f792399abad2e4492f48083ab1093/Data/BindersWithComplexname.csv\n",
        "%cd .."
      ],
      "metadata": {
        "id": "6dliB1TskGg9",
        "outputId": "0e9fbd7a-7b89-4912-d347-207573d66f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PPI-Inhibitors\n",
            "/content/PPI-Inhibitors/Data\n",
            "/content/PPI-Inhibitors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6uOhzY0qAj-0",
        "outputId": "89885a35-33af-4101-913e-b9b1bdc21b8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive'\n",
        "\n",
        "# Make folders\n",
        "!mkdir -p GNN-PPI-Inhibitor\n",
        "!gdown --id 1goeDiPZSKT1Xx3j00eNG9xlqYkLLv1gW -O GNN-PPI-Inhibitor/ProteinData_dict.pickle\n",
        "!gdown --id 1GOYEKLQCoGea9QQ72kujy0rdJKbUSYAE -O GNN-PPI-Inhibitor/DBD5_ProteinData_dict.pickle"
      ],
      "metadata": {
        "id": "VRBJi-5YkPqM",
        "outputId": "32e7bf7b-a12d-4649-ca2e-56b5fe8a6d15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1goeDiPZSKT1Xx3j00eNG9xlqYkLLv1gW\n",
            "To: /content/drive/MyDrive/GNN-PPI-Inhibitor/ProteinData_dict.pickle\n",
            "100% 34.6M/34.6M [00:00<00:00, 109MB/s] \n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1GOYEKLQCoGea9QQ72kujy0rdJKbUSYAE\n",
            "From (redirected): https://drive.google.com/uc?id=1GOYEKLQCoGea9QQ72kujy0rdJKbUSYAE&confirm=t&uuid=6ccbb09b-857e-4fe6-94d0-dfc975bccd1a\n",
            "To: /content/drive/MyDrive/GNN-PPI-Inhibitor/DBD5_ProteinData_dict.pickle\n",
            "100% 336M/336M [00:03<00:00, 95.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjV5-TPeGt-0",
        "outputId": "7d6824bf-a0f7-4bfd-de5d-09557e25f4fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equivalent epochs in one iteration of data loader 1.04\n",
            "[[('46', '47', '59', '95', '99', '6', '32', '43', '71', '29'), ('-46', '-47', '-59', '-95', '-99', '-6', '-32', '-43', '-71', '-29')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('1', '7', '54', '60', '87', '58', '75', '5', '17', '13'), ('-1', '-7', '-54', '-60', '-87', '-58', '-75', '-5', '-17', '-13')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('2', '4', '30', '69', '82', '62', '98', '75', '17', '44'), ('-2', '-4', '-30', '-69', '-82', '-62', '-98', '-75', '-17', '-44')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('39', '48', '70', '72', '86', '43', '5', '53', '36', '97'), ('-39', '-48', '-70', '-72', '-86', '-43', '-5', '-53', '-36', '-97')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('15', '33', '50', '52', '68', '31', '10', '13', '91', '97'), ('-15', '-33', '-50', '-52', '-68', '-31', '-10', '-13', '-91', '-97')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('20', '34', '61', '63', '90', '58', '18', '26', '98'), ('-20', '-34', '-61', '-63', '-90', '-58', '-18', '-26', '-98')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0])]\n",
            "[[('0', '56', '73', '81', '94', '19', '71', '80', '8'), ('0', '-56', '-73', '-81', '-94', '-19', '-71', '-80', '-8')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0])]\n",
            "[[('45', '67', '84', '92', '93', '32', '17', '26', '28'), ('-45', '-67', '-84', '-92', '-93', '-32', '-17', '-26', '-28')], tensor([1, 1, 1, 1, 1, 0, 0, 0, 0])]\n",
            "[[('12', '24', '51', '66', '6', '13', '25', '85', '23'), ('-12', '-24', '-51', '-66', '-6', '-13', '-25', '-85', '-23')], tensor([1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('38', '57', '77', '89', '5', '79', '9', '31', '44'), ('-38', '-57', '-77', '-89', '-5', '-79', '-9', '-31', '-44')], tensor([1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "[[('40', '42', '55', '78', '21', '19', '19', '23', '53'), ('-40', '-42', '-55', '-78', '-21', '-19', '-19', '-23', '-53')], tensor([1, 1, 1, 1, 0, 0, 0, 0, 0])]\n",
            "['6', '46', '92', '78', '15', '92', '1', '38', '64', '90', '30', '44', '26', '53', '70', '60', '75', '12', '46', '77', '39', '36', '17', '94', '6', '9', '31', '19', '57', '47', '10', '92', '21', '41', '9', '79', '56', '52', '51', '62', '47', '35', '38', '31', '1', '3', '7', '25', '23', '64', '45', '22', '50', '29', '28', '87', '50', '32', '49', '64', '42', '2', '43', '43', '55', '63', '95', '49', '55', '71', '22', '54', '13', '60', '94', '74', '11', '73', '69', '35', '28', '50', '93', '0', '62', '48', '2', '63', '43', '83', '1', '82', '13', '56', '80', '91', '41', '63', '52', '70']\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan  9 16:43:15 2024\n",
        "\n",
        "@author: u1876024\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "class BalancedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class that creates a balanced dataset from imbalanced data.\n",
        "    This dataset calculates sample weights inversely proportional to class frequencies,\n",
        "    which can be used with a WeightedRandomSampler to achieve balanced batches.\n",
        "\n",
        "    NOTE: As it involves stochastic sampling, there is a chance that a few training examples are actually never selected.\n",
        "\n",
        "    Attributes:\n",
        "        data (array-like): The input data. Can be a list, NumPy array, or PyTorch tensor.\n",
        "        labels (array-like): The labels corresponding to the data. Should be a 1D array-like object.\n",
        "        sample_weights (torch.Tensor): Weights for each sample, inversely proportional to class frequencies.\n",
        "\n",
        "    Methods:\n",
        "        __len__: Returns the number of samples in the dataset.\n",
        "        __getitem__(idx): Returns the sample and its corresponding label at the given index.\n",
        "    \"\"\"\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "        # Count the number of examples in each class\n",
        "        class_counts = np.bincount(self.labels)\n",
        "        # Assign weight inversely proportional to class frequency\n",
        "        weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "        # Create a weight list for each sample\n",
        "        self.sample_weights = weights[labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "def create_balanced_loader(data, labels, batch_size=32):\n",
        "    \"\"\"\n",
        "    Creates a DataLoader with balanced batches for a given dataset.\n",
        "    This function is useful for training models on imbalanced datasets.\n",
        "\n",
        "    Args:\n",
        "        data (array-like): The input data. Can be a list, NumPy array, or PyTorch tensor.\n",
        "        labels (array-like): The labels corresponding to the data. Should be a 1D array-like object.\n",
        "        batch_size (int, optional): The size of each batch. Default is 32.\n",
        "\n",
        "    Returns:\n",
        "        DataLoader: A PyTorch DataLoader that yields balanced batches.\n",
        "\n",
        "    Usage Example:\n",
        "        >>> data = [features1, features2, ...]  # Replace with your data features\n",
        "        >>> labels = [label1, label2, ...]     # Replace with your data labels\n",
        "        >>> balanced_loader = create_balanced_loader(data, labels, batch_size=32)\n",
        "        >>> for batch_data, batch_labels in balanced_loader:\n",
        "        >>>     # Train your model using the balanced batches\n",
        "    \"\"\"\n",
        "    dataset = BalancedDataset(data, labels)\n",
        "    # WeightedRandomSampler will take care of the balancing\n",
        "    sampler = WeightedRandomSampler(weights=dataset.sample_weights, num_samples=len(dataset.sample_weights), replacement=True)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
        "    return loader\n",
        "\n",
        "\n",
        "class BinaryBalancedSampler(Sampler):\n",
        "    \"\"\"\n",
        "    A PyTorch Sampler that returns batches with an equal number of positive and negative examples.\n",
        "    The sampler oversamples from the minority class to balance the majority class, ensuring that\n",
        "    each batch contains 50% positive and 50% negative examples.\n",
        "\n",
        "    NOTE: It leads to more examples in single iteration through the data loader than in one epoch\n",
        "\n",
        "    Attributes:\n",
        "        class_vector (list or numpy array): class labels.\n",
        "        batch_size (int): The size of each batch.\n",
        "        n_splits (int): The number of batches/splits in the dataset.\n",
        "        equivalent_epochs (float): The number of times the sampler goes over the minority class\n",
        "                                   in one complete iteration of the DataLoader.\n",
        "\n",
        "    Methods:\n",
        "        gen_sample_array: Yields indices for each batch ensuring class balance.\n",
        "        __iter__: Returns an iterator over batch indices.\n",
        "        __len__: Returns the number of batches in the sampler.\n",
        "    \"\"\"\n",
        "    def __init__(self, class_vector, batch_size = 10):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        class_vector : torch tensor\n",
        "            a vector of class labels\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.class_vector = class_vector\n",
        "        YY = np.array(self.class_vector)\n",
        "        U, C = np.unique(YY, return_counts=True)\n",
        "        M = U[np.argmax(C)]        #find majority class\n",
        "        Midx = np.nonzero(YY==M)[0] #indices of majority class\n",
        "        midx = np.nonzero(YY!=M)[0] #indices of minority class\n",
        "        midx_ = np.random.choice(midx,size=len(Midx))     #oversample minority indices so they are equal to majority ones\n",
        "        self.YY = np.array(list(YY[Midx])+list(YY[midx_]))\n",
        "        self.idx = np.array(list(Midx)+list(midx_))\n",
        "        self.n_splits = int(np.ceil(len(self.idx)/self.batch_size))\n",
        "        self.equivalent_epochs = len(self.idx)/len(self.class_vector)\n",
        "        print('Equivalent epochs in one iteration of data loader',self.equivalent_epochs)\n",
        "\n",
        "    def gen_sample_array(self):\n",
        "        from sklearn.model_selection import StratifiedKFold\n",
        "        skf = StratifiedKFold(n_splits= self.n_splits,shuffle=True)\n",
        "        for tridx,ttidx in skf.split(self.idx,self.YY):\n",
        "            yield np.array(self.idx[ttidx])\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.gen_sample_array())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_splits\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "if __name__=='__main__':\n",
        "\n",
        "    E = [(str(p_i), str(-1*c_i)) for p_i, c_i in zip(range(100), range(100))]  # Replace with your data\n",
        "    Y = np.random.randint(0, 2, size=100)  # Replace with your labels\n",
        "    batch_size = 10\n",
        "\n",
        "    dataset = CustomDataset(E, Y)\n",
        "    batch_sampler = BinaryBalancedSampler(Y, batch_size)\n",
        "    data_loader = DataLoader(dataset, batch_sampler=batch_sampler)\n",
        "\n",
        "    for batch in data_loader:\n",
        "        print(batch)\n",
        "\n",
        "    # Example usage of create_balanced_loader\n",
        "    balanced_loader = create_balanced_loader(E, Y, batch_size)\n",
        "\n",
        "    # Iterate over the DataLoader\n",
        "    L = []\n",
        "    for (pid,cid),label in balanced_loader:\n",
        "        # Process your batches\n",
        "        L.extend(pid)\n",
        "    print(L)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import DataStructs\n",
        "import numpy as np\n",
        "import math\n",
        "from itertools import product\n",
        "from scipy import spatial\n",
        "from os import listdir\n",
        "from Bio import SeqIO\n",
        "from Bio.SeqIO import FastaIO\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from sklearn.preprocessing import normalize\n",
        "from Bio.Data import IUPACData\n",
        "from Bio.PDB.Polypeptide import *\n",
        "from Bio.PDB import PDBParser\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "from tqdm import tqdm as tqdm\n",
        "# (Assuming PrepairDataset is a custom module you have)\n",
        "# import PrepairDataset\n",
        "\n",
        "def getFP(s, r=3, nBits=2048):\n",
        "    compound = Chem.MolFromSmiles(s.strip())\n",
        "    if compound is not None:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(compound, r, nBits=nBits)\n",
        "        #fp = pat.GetAvalonCountFP(compound,nBits=nBits)\n",
        "        m = np.zeros((0,), dtype=np.int8)\n",
        "        DataStructs.ConvertToNumpyArray(fp, m)\n",
        "        return m\n",
        "\n",
        "def twomerFromSeq(s):\n",
        "    k=2\n",
        "    groups={'A':'1','V':'1','G':'1','I':'2','L':'2','F':'2','P':'2','Y':'3',\n",
        "            'M':'3','T':'3','S':'3','H':'4','N':'4','Q':'4','W':'4',\n",
        "            'R':'5','K':'5','D':'6','E':'6','C':'7'}\n",
        "    crossproduct=[''.join (i) for i in product(\"1234567\",repeat=k)]\n",
        "    for i in range (0,len(crossproduct)): crossproduct[i]=int(crossproduct[i])\n",
        "    ind=[]\n",
        "    for i in range (0,len(crossproduct)): ind.append(i)\n",
        "    combinations=dict(zip(crossproduct,ind))\n",
        "    V=np.zeros(int((math.pow(7,k))))      #defines a vector of 343 length with zero entries\n",
        "    try:\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                c+=groups[kmer[l]]\n",
        "            V[combinations[int(c)]]+=1\n",
        "    except:\n",
        "        count={'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0}\n",
        "        for q in range(0,len(s)):\n",
        "            if s[q]=='A' or s[q]=='V' or s[q]=='G':\n",
        "                count['1']+=1\n",
        "            if s[q]=='I' or s[q]=='L'or s[q]=='F' or s[q]=='P':\n",
        "                count['2']+=1\n",
        "            if s[q]=='Y' or s[q]=='M'or s[q]=='T' or s[q]=='S':\n",
        "                count['3']+=1\n",
        "            if s[q]=='H' or s[q]=='N'or s[q]=='Q' or s[q]=='W':\n",
        "                count['4']+=1\n",
        "            if s[q]=='R' or s[q]=='K':\n",
        "                count['5']+=1\n",
        "            if s[q]=='D' or s[q]=='E':\n",
        "                count['6']+=1\n",
        "            if s[q]=='C':\n",
        "                count['7']+=1\n",
        "        val=list(count.values()  )           #[ 0,0,0,0,0,0,0]\n",
        "        key=list(count.keys()     )           #['1', '2', '3', '4', '5', '6', '7']\n",
        "        m=0\n",
        "        ind=0\n",
        "        for t in range(0,len(val)):     #find maximum value from val\n",
        "            if m<val[t]:\n",
        "                m=val[t]\n",
        "                ind=t\n",
        "        m=key [ind]                     # m=group number of maximum occuring group alphabets in protein\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                if kmer[l] not in groups:\n",
        "                    c+=m\n",
        "                else:\n",
        "                    c+=groups[kmer[l]]\n",
        "            V[combinations[int(c)]]+=1\n",
        "    V=V/(len(s)-1)\n",
        "    return np.array(V)\n",
        "\n",
        "def chainLabel(Cname_T,xl_T,Cname,xl):\n",
        "    \"\"\"\n",
        "    Cname_T: Target chain Name\n",
        "    xl_T: Target chain co-ordinates\n",
        "    Cname: Off Target chain Name\n",
        "    xl: Off Target chain co-ordinates\n",
        "    \"\"\"\n",
        "    tc = getCoords(xl_T)\n",
        "    nc = getCoords(xl)\n",
        "    D = getDist(tc, nc, thr = 8.0)\n",
        "    feats=extract_feats(generate_pair_features(D,xl_T,xl))\n",
        "    return feats\n",
        "\n",
        "def generate_pair_features(dist_info,xl,xr):\n",
        "    prot_dic=make_dic()\n",
        "    #    pdb.set_trace()\n",
        "    for rec in dist_info:\n",
        "        try:\n",
        "            l_letter= three_to_one(xl[rec[0]].get_resname())\n",
        "            r_letter= three_to_one(xr[rec[1]].get_resname())\n",
        "            #            print(l_letter,l_letter)\n",
        "            if (l_letter,r_letter) in prot_dic.keys():\n",
        "                prot_dic[(l_letter,r_letter)]+=1\n",
        "            elif (r_letter,l_letter) in prot_dic.keys():\n",
        "                prot_dic[(r_letter,l_letter)]+=1\n",
        "        except:\n",
        "            prot_dic[('_','_')]+=1\n",
        "    return prot_dic\n",
        "\n",
        "def getCoords(R):\n",
        "    \"\"\"\n",
        "    Get atom coordinates given a list of biopython residues\n",
        "    \"\"\"\n",
        "    Coords = []\n",
        "    for (idx, r) in enumerate(R):\n",
        "        v = [ak.get_coord() for ak in r.get_list()]\n",
        "        Coords.append(v)\n",
        "    return Coords\n",
        "\n",
        "def InterfaceFeatures(Complexs,pdbloc):\n",
        "    Found =  listdir(pdbloc)\n",
        "    InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "    comp_id=list(set(Complexs))\n",
        "    for ids in range(len(comp_id)):\n",
        "        if comp_id[ids]+'.pdb' in Found:\n",
        "            stx=pdbloc+'/'+comp_id[ids]+'.pdb'#'/2XA0.pdb'\n",
        "            chains=Struct2chain(stx)\n",
        "            for j in range(len(chains)):\n",
        "                Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "                for k in range(j,len(chains)):\n",
        "                    Cname,seq,L,xl=chains[k]\n",
        "                    #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                    name=comp_id[ids]#+'_'+Cname_T+'_2_'+Cname\n",
        "                    Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                    InterfaceF=np.array(Interface)\n",
        "                    InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                    if name not in InterfaceFeatures.keys():\n",
        "                        InterfaceFeatures[name]=Interface\n",
        "    #pickle.dump(InterfaceFeatures, open(path+Filename+\"_InterfaceFeatures.npy\", \"wb\"))\n",
        "    return InterfaceFeatures\n",
        "\n",
        "def getDist(C0, C1, thr=np.inf):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    N0 = []\n",
        "    N1 = []\n",
        "    for i in range(len(C0)):\n",
        "        for j in range(len(C1)):\n",
        "            d = spatial.distance.cdist(C0[i], C1[j]).min()\n",
        "            # dji=spatial.distance.cdist(C1[j], C0[i]).min()\n",
        "            #d=min(dij,dji)\n",
        "            #print d\n",
        "            if (d < thr):  # and not np.isnan(self.Phi[i]) and not np.isnan(self.Phi[j])\n",
        "                N0.append((i, j, d))\n",
        "                N1.append((j, i, d))\n",
        "    return (N0, N1)\n",
        "\n",
        "def prot_feats_seq(seq):\n",
        "    #Interfacedict=pickle.load(open(path+\"InhibitorNewModel2022/InterfaceFeatures2chainsSVM.npy\",\"rb\"))\n",
        "    #InterfaceF=Interfacedict[complexname]\n",
        "    aa=['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "    f=[]\n",
        "    X = ProteinAnalysis(str(seq))\n",
        "    X.molecular_weight() #throws an error if 'X' in sequence. we skip such sequences\n",
        "    p=X.get_amino_acids_percent()\n",
        "    dp=[]\n",
        "    for a in aa:\n",
        "        dp.append(p[a])\n",
        "    dp=np.array(dp)\n",
        "    dp=normalize(np.atleast_2d(dp), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "    f.extend(dp[0])\n",
        "    tm=np.array(twomerFromSeq(str(seq)))\n",
        "    tm=normalize(np.atleast_2d(tm), norm='l2', copy=True, axis=1,return_norm=False)\n",
        "    f.extend(tm[0])\n",
        "    return np.array(f)\n",
        "\n",
        "def Struct2chain(stx):\n",
        "    \"\"\"\n",
        "    Seq: sequence of the chain\n",
        "    seq_L:sequence Length\n",
        "    \"\"\"\n",
        "    p = PDBParser()\n",
        "    L=[]\n",
        "    stx=p.get_structure('X',stx)\n",
        "    for model in stx:\n",
        "        for C in model:\n",
        "            RL=[]\n",
        "            for R in C:\n",
        "                RL.append(R)\n",
        "            pp=PPBuilder().build_peptides(C)\n",
        "            if len(pp)==0:\n",
        "                pp=CaPPBuilder().build_peptides(C)\n",
        "            seq=''.join([str(p.get_sequence()) for p in pp])\n",
        "            #seq=''.join([p.get_sequence().tostring() for p in pp])\n",
        "            seq_L=len(seq)\n",
        "            L.append((C.full_id[2],seq,seq_L,RL))\n",
        "    return L\n",
        "\n",
        "def extract_feats(dic):\n",
        "    feats=[]\n",
        "    key_list=np.load('/content/PPI-Inhibitors/Features/'+'prote_letter_pair_keys.npy')#to keep features order same\n",
        "    for key in key_list:\n",
        "        #        pdb.set_trace()\n",
        "        feats.append(dic[(key[0].decode('utf-8'),key[1].decode('utf-8'))])\n",
        "    return feats\n",
        "\n",
        "def make_dic():\n",
        "    prot_dic={}\n",
        "    letters=IUPACData.protein_letters\n",
        "    for i in range(len(letters)):\n",
        "        for j in range(i,len(letters)):\n",
        "            prot_dic[(letters[i],letters[j])]=0.0\n",
        "    prot_dic[('_','_')]=0.0# for Amino acids other than 20 natural\n",
        "    return prot_dic\n",
        "\n",
        "def LoadProtein_SVM_Features(UniqueProtein,Pdbloc):\n",
        "    pdbname=listdir(Pdbloc)\n",
        "    InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "    SequenceFeatures=[];SequenceFeatures=dict(SequenceFeatures)\n",
        "    AllFeatures=[];AllFeatures=dict(AllFeatures)\n",
        "    for  b in range(len(UniqueProtein)):\n",
        "        if UniqueProtein[b]+'.pdb'in pdbname:\n",
        "            stx=Pdbloc+UniqueProtein[b]+'.pdb'#directory+'/2XA0.pdb'\n",
        "            chains=Struct2chain(stx)\n",
        "            #########Interface Features\n",
        "            for j in range(len(chains)):\n",
        "                Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "                for k in range(j,len(chains)):\n",
        "                    Cname,seq,L,xl=chains[k]\n",
        "                    #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                    name=UniqueProtein[b]#+'_'+Cname_T+'_2_'+Cname\n",
        "                    Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                    seq_TF=prot_feats_seq(seq_T)\n",
        "                    seq_NTF=prot_feats_seq(seq)\n",
        "                    SeQFeatures=(seq_TF+seq_NTF)/2\n",
        "                    InterfaceF=np.array(Interface)\n",
        "                    InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                    if name not in InterfaceFeatures.keys():\n",
        "                        InterfaceFeatures[name]=Interface\n",
        "                        SequenceFeatures[name]=SeQFeatures\n",
        "                        AllFeatures[name]=np.append(SeQFeatures,Interface)\n",
        "    return InterfaceFeatures,SequenceFeatures,AllFeatures\n",
        "\n",
        "def External_GenerateRandomNegative(posexamples):\n",
        "    NegtiveRatio=1\n",
        "    ###SuperDrugbank###Names\n",
        "    SuperdrugNames=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#'approved_drugs_chemical_structure_identifiers.xlsx'\n",
        "    SuperdrugNames=SuperdrugNames[1:]\n",
        "    SuperdrugNames = np.array([s[0] for s in SuperdrugNames])\n",
        "    ###############SuperDrugbank\n",
        "    df_Superdrug=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"C\").values\n",
        "    df_Superdrug=df_Superdrug[1:]\n",
        "    ###\n",
        "    df_Superdrug_Compounds=np.array([c[0] for c in df_Superdrug])#3638\n",
        "    SuperDrug_dict=dict (zip (SuperdrugNames,df_Superdrug_Compounds))\n",
        "    ################\n",
        "    #path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "    DBD5_ProteinData_dict=pickle.load(open('/content/PPI-Inhibitors/Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "    Ubench5CompNames=list (set (list (DBD5_ProteinData_dict.keys())))\n",
        "    ####\n",
        "    AllNeg=[];AllPos=[];complex_ligand_dict={};\n",
        "    for key,val in  posexamples:\n",
        "        #print(key,val,posexamples[key,val][1])\n",
        "        if key not in complex_ligand_dict:\n",
        "            complex_ligand_dict[key]=posexamples[key,val][1]\n",
        "        else:\n",
        "            #print(\"else\",key,val)\n",
        "            complex_ligand_dict[key]=np.append( complex_ligand_dict.get(key, ()) ,posexamples[key,val][1])\n",
        "\n",
        "    Complexnames=list (complex_ligand_dict.keys())\n",
        "    totalcomp=list (set (complex_ligand_dict.keys()))\n",
        "\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "        #print(origanlL)\n",
        "        #print(\"complexname=\",everycomp,\"origanlInhibitors\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        #print(pos)\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        AllPos.extend(pos)\n",
        "        while (len(negs)<NN):# and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            LigandR = random.choice(SuperdrugNames)\n",
        "            LigandR_smile=SuperDrug_dict[LigandR]\n",
        "            Npair=((everycomp,LigandR_smile))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(LigandR_smile) is not None:\n",
        "                negs.append(Npair)\n",
        "                #                print(\"Npair SuperdrugNames\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "\n",
        "    #print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    #print(\"second method Cr\")\n",
        "\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "        #        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):\n",
        "            for everyL in origanlL:\n",
        "                ComplexR = random.choice(Ubench5CompNames)\n",
        "                Npair=((ComplexR,everyL))\n",
        "                if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "                    #                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                    negs.append(Npair)\n",
        "        AllNeg.extend(negs)\n",
        "    ###################\n",
        "    #print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    return np.array(AllPos),np.array(AllNeg),SuperDrug_dict\n",
        "\n",
        "def PredictScorefromFile(filename,Pdbloc,Pscaler,Cscaler,trainedModel_IPPI,train_GNN,LOCOcomplexname):\n",
        "    githubpath='/content/PPI-Inhibitors/'\n",
        "    #filename,Pdbloc,Pscaler,Cscaler,trainedModel_IPPI,train_GNN=githubpath+'Data/External data/2dyh_all.txt',githubpath+'Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model\n",
        "    with open(filename) as f:\n",
        "        D = f.readlines()\n",
        "\n",
        "    InhibitedComp=[];PdbId=[];Ligandnames=[];SMILES=[];targets=[];\n",
        "    All_data_list=[]\n",
        "\n",
        "    #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "    for d in tqdm(D):\n",
        "        Pdbid,inhibtedc,Ligandid,smiles = d.split()\n",
        "        if getFP(smiles) is not None:\n",
        "            PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);InhibitedComp.append(inhibtedc);#labels.append(float (y));\n",
        "\n",
        "    ################\n",
        "    \"\"\"\n",
        "    Result_dict={}\n",
        "    pos=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "    Pos,Negs,SuperDrug_dict=External_GenerateRandomNegative(pos)\n",
        "    poslabel=1.0*np.ones(len(Pos));neglabel=-1.0*np.ones(len(Negs));targets=np.append(poslabel,neglabel )\n",
        "    All_examples=[];All_examples.extend(Pos);All_examples.extend(Negs)\n",
        "    #Write File for External\n",
        "    External_All_Examples=open('/content/drive/MyDrive/GNN-PPI-Inhibitor/_'+LOCOcomplexname+'_'+filename.split('.txt')[0].split('/')[-1]+'_External_All_Examples.txt',\"w\")\n",
        "    \"\"\"\n",
        "    complexnames=[];SMILES=[];targets=[];\n",
        "    #/content/PPI-Inhibitors/Data/External data/2dyh_all_External_All_Examples.txt\n",
        "    with open('/content/PPI-Inhibitors/Data/External data/'+filename.split('.txt')[0].split('/')[-1]+'_External_All_Examples.txt') as f:\n",
        "        D = f.readlines()\n",
        "\n",
        "    for d in tqdm(D):\n",
        "        complexname,smiles,target= d.split()\n",
        "        complexnames.append(complexname);SMILES.append(smiles);targets.append(target);#All_examples.append()\n",
        "\n",
        "    pdbname=listdir(Pdbloc);mypdb=[]\n",
        "    for p in pdbname:\n",
        "        if p.split('.pdb')[0] in Pdbid:\n",
        "            mypdb.append(p)\n",
        "\n",
        "    UniqueProtein=list (set (mypdb))\n",
        "    External_Protein_GNN_Data_dict=processProtein(UniqueProtein,Pdbloc)\n",
        "\n",
        "    ##########for Seq+interface features\n",
        "    #pdbname=listdir(Pdbloc)\n",
        "    s,i,External_ProteinSeqandInterfaceData_dict=LoadProtein_SVM_Features(UniqueProtein,Pdbloc)\n",
        "\n",
        "    ##############3 for sequence fedatures of DBD5 pdb's\n",
        "    Ubench5InterfaceandSeq_dict=pickle.load(open(githubpath+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "    All_External_ProteinSeqandInterfaceData_dict=dict( list (External_ProteinSeqandInterfaceData_dict.items())+list (Ubench5InterfaceandSeq_dict.items()))\n",
        "\n",
        "    #Testing\n",
        "    DBD5_Protein_GNN_Data_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "    #Pos_ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "    All_Protein_GNN_Data_dict=dict( list (External_Protein_GNN_Data_dict.items())+list (DBD5_Protein_GNN_Data_dict.items()))\n",
        "\n",
        "    for d in All_Protein_GNN_Data_dict:\n",
        "        data=All_Protein_GNN_Data_dict[d]\n",
        "        All_Protein_GNN_Data_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "\n",
        "    #####################\n",
        "    Cttname=[];Ctt=[];Pttname=[];Ptt=[];\n",
        "    for (complexname,ligandsmile) in zip(complexnames,SMILES):#All_examples:\n",
        "        Cttname.append(ligandsmile);Ctt.append(getFP(ligandsmile));\n",
        "        Pttname.append(complexname);Ptt.append(All_External_ProteinSeqandInterfaceData_dict[complexname]);\n",
        "\n",
        "    #standarization\n",
        "    Ctt = Cscaler.transform(Ctt)\n",
        "    Cttdict=dict (zip (Cttname,torch.FloatTensor( Ctt).cuda()))\n",
        "    Ptt = Pscaler.transform(Ptt)\n",
        "    Pttdict=dict (zip (Pttname,torch.FloatTensor( Ptt).cuda()))\n",
        "\n",
        "    #########\n",
        "    Y_t,Z,Targets=[],[],[]\n",
        "\n",
        "    # Set models to evaluation mode for inference\n",
        "    trainedModel_IPPI.eval()\n",
        "    train_GNN.eval()\n",
        "\n",
        "    # with torch.no_grad(): # Disable gradient calculations\n",
        "    #     for target,(complexname,ligandsmile) in zip(targets,zip(complexnames,SMILES)):\n",
        "\n",
        "    #         # 1. Get GNN features first by running the GNN model\n",
        "    #         GNN_features = train_GNN(All_Protein_GNN_Data_dict[complexname])\n",
        "\n",
        "    #         # 2. Pass the GNN features (not the model) to the MLP\n",
        "    #         test_score = trainedModel_IPPI(GNN_features, Cttdict[ligandsmile], Pttdict[complexname])\n",
        "\n",
        "    #         test_score=test_score.cpu().data.numpy()[0]\n",
        "    #         Z.append(test_score);Targets.append(float (target))\n",
        "\n",
        "    # return Z,Targets\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for target,(complexname,ligandsmile) in zip(targets,zip(complexnames,SMILES)):\n",
        "\n",
        "            # 1. Get GNN features (already 2D: [1, 512])\n",
        "            GNN_features = train_GNN(All_Protein_GNN_Data_dict[complexname])\n",
        "\n",
        "            # 2. Get Ligand and Interface features (which are 1D)\n",
        "            ligand_feats_1d = Cttdict[ligandsmile]\n",
        "            interface_feats_1d = Pttdict[complexname]\n",
        "\n",
        "            # 3. Add a batch dimension (unsqueeze) to make them 2D\n",
        "            ligand_feats_2d = ligand_feats_1d.unsqueeze(0)       # Shape becomes [1, 2048]\n",
        "            interface_feats_2d = interface_feats_1d.unsqueeze(0)   # Shape becomes [1, 280]\n",
        "\n",
        "            # 4. Pass all 2D tensors to the MLP\n",
        "            test_score = trainedModel_IPPI(GNN_features, ligand_feats_2d, interface_feats_2d)\n",
        "\n",
        "            test_score=test_score.cpu().data.numpy()[0]\n",
        "            Z.append(test_score);Targets.append(float (target))\n",
        "    return Z, Targets"
      ],
      "metadata": {
        "id": "4pYOXb0eXcR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn0xj2yWfAYq",
        "outputId": "1e451129-48cd-4e15-a896-df9b3c16fd9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Number of GPUs: 1\n",
            "GPU 0: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15695/15695 [00:00<00:00, 846944.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equivalent epochs in one iteration of data loader 1.8249820678348192\n",
            "test complex  2XA0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/18 [00:09<02:46,  9.79s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.3580858085808581 AUCPR 0.0034102333336018123\n",
            "AUCROC 0.3580858085808581 AUCPR 0.0034102333336018123 best aucroc 0.3580858085808581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 11%|█         | 2/18 [00:17<02:19,  8.69s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.3987211221122112 AUCPR 0.004504695184553124\n",
            "AUCROC 0.3987211221122112 AUCPR 0.004504695184553124 best aucroc 0.3987211221122112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 17%|█▋        | 3/18 [00:27<02:16,  9.08s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.415016501650165 AUCPR 0.004019096244291531\n",
            "AUCROC 0.415016501650165 AUCPR 0.004019096244291531 best aucroc 0.415016501650165\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 22%|██▏       | 4/18 [00:35<02:05,  8.93s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.46637788778877887 AUCPR 0.005240305139192904\n",
            "AUCROC 0.46637788778877887 AUCPR 0.005240305139192904 best aucroc 0.46637788778877887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 28%|██▊       | 5/18 [00:45<01:59,  9.22s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.5678630363036304 AUCPR 0.0072882013501048126\n",
            "AUCROC 0.5678630363036304 AUCPR 0.0072882013501048126 best aucroc 0.5678630363036304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 33%|███▎      | 6/18 [00:55<01:51,  9.29s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.6377887788778878 AUCPR 0.008296095689460619\n",
            "AUCROC 0.6377887788778878 AUCPR 0.008296095689460619 best aucroc 0.6377887788778878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 39%|███▉      | 7/18 [01:05<01:44,  9.52s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.656559405940594 AUCPR 0.00897670380860726\n",
            "AUCROC 0.656559405940594 AUCPR 0.00897670380860726 best aucroc 0.656559405940594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 44%|████▍     | 8/18 [01:13<01:32,  9.22s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.6588283828382839 AUCPR 0.008337016103396676\n",
            "AUCROC 0.6588283828382839 AUCPR 0.008337016103396676 best aucroc 0.6588283828382839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 50%|█████     | 9/18 [01:22<01:22,  9.21s/it]\u001b[A\n",
            " 56%|█████▌    | 10/18 [01:31<01:13,  9.14s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.6619224422442243 AUCPR 0.006301176738802801\n",
            "AUCROC 0.6619224422442243 AUCPR 0.006301176738802801 best aucroc 0.6619224422442243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 61%|██████    | 11/18 [01:42<01:06,  9.48s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.6941006600660066 AUCPR 0.006002898627570544\n",
            "AUCROC 0.6941006600660066 AUCPR 0.006002898627570544 best aucroc 0.6941006600660066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 67%|██████▋   | 12/18 [01:52<00:58,  9.71s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.7337046204620461 AUCPR 0.006095993601647377\n",
            "AUCROC 0.7337046204620461 AUCPR 0.006095993601647377 best aucroc 0.7337046204620461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 72%|███████▏  | 13/18 [02:01<00:47,  9.56s/it]\u001b[A\n",
            " 78%|███████▊  | 14/18 [02:12<00:39,  9.83s/it]\u001b[A\n",
            " 83%|████████▎ | 15/18 [02:20<00:28,  9.50s/it]\u001b[A\n",
            " 89%|████████▉ | 16/18 [02:29<00:18,  9.14s/it]\u001b[A\n",
            " 94%|█████████▍| 17/18 [02:39<00:09,  9.48s/it]\u001b[A\n",
            "100%|██████████| 18/18 [02:49<00:00,  9.40s/it]\n",
            " 20%|██        | 1/5 [02:49<11:16, 169.16s/it]\n",
            "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/18 [00:09<02:36,  9.20s/it]\u001b[A\n",
            " 11%|█         | 2/18 [00:18<02:26,  9.18s/it]\u001b[A\n",
            " 17%|█▋        | 3/18 [00:28<02:22,  9.52s/it]\u001b[A\n",
            " 22%|██▏       | 4/18 [00:38<02:14,  9.64s/it]\u001b[A\n",
            " 28%|██▊       | 5/18 [00:47<02:03,  9.51s/it]\u001b[A\n",
            " 33%|███▎      | 6/18 [00:57<01:55,  9.62s/it]\u001b[A\n",
            " 39%|███▉      | 7/18 [01:06<01:43,  9.40s/it]\u001b[A\n",
            " 44%|████▍     | 8/18 [01:15<01:33,  9.34s/it]\u001b[A\n",
            " 50%|█████     | 9/18 [01:23<01:21,  9.10s/it]\u001b[A\n",
            " 56%|█████▌    | 10/18 [01:32<01:11,  8.92s/it]\u001b[A\n",
            " 61%|██████    | 11/18 [01:43<01:06,  9.43s/it]\u001b[A\n",
            " 67%|██████▋   | 12/18 [01:53<00:57,  9.63s/it]\u001b[A\n",
            " 72%|███████▏  | 13/18 [02:03<00:48,  9.73s/it]\u001b[A\n",
            " 78%|███████▊  | 14/18 [02:11<00:37,  9.37s/it]\u001b[A\n",
            " 83%|████████▎ | 15/18 [02:20<00:27,  9.21s/it]\u001b[A\n",
            " 89%|████████▉ | 16/18 [02:30<00:19,  9.56s/it]\u001b[A\n",
            " 94%|█████████▍| 17/18 [02:39<00:09,  9.16s/it]\u001b[A\n",
            "100%|██████████| 18/18 [02:48<00:00,  9.36s/it]\n",
            " 40%|████      | 2/5 [05:37<08:26, 168.81s/it]\n",
            "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/18 [00:09<02:43,  9.65s/it]\u001b[A\n",
            " 11%|█         | 2/18 [00:18<02:24,  9.03s/it]\u001b[A\n",
            " 17%|█▋        | 3/18 [00:28<02:23,  9.57s/it]\u001b[A\n",
            " 22%|██▏       | 4/18 [00:37<02:13,  9.56s/it]\u001b[A\n",
            " 28%|██▊       | 5/18 [00:48<02:07,  9.82s/it]\u001b[A\n",
            " 33%|███▎      | 6/18 [00:57<01:56,  9.73s/it]\u001b[A\n",
            " 39%|███▉      | 7/18 [01:07<01:46,  9.71s/it]\u001b[A\n",
            " 44%|████▍     | 8/18 [01:15<01:32,  9.29s/it]\u001b[A\n",
            " 50%|█████     | 9/18 [01:25<01:24,  9.40s/it]\u001b[A\n",
            " 56%|█████▌    | 10/18 [01:34<01:14,  9.27s/it]\u001b[A\n",
            " 61%|██████    | 11/18 [01:44<01:05,  9.34s/it]\u001b[A\n",
            " 67%|██████▋   | 12/18 [01:52<00:54,  9.04s/it]\u001b[A\n",
            " 72%|███████▏  | 13/18 [02:00<00:44,  8.88s/it]\u001b[A\n",
            " 78%|███████▊  | 14/18 [02:11<00:37,  9.31s/it]\u001b[A\n",
            " 83%|████████▎ | 15/18 [02:20<00:28,  9.42s/it]\u001b[A\n",
            " 89%|████████▉ | 16/18 [02:30<00:18,  9.48s/it]\u001b[A\n",
            " 94%|█████████▍| 17/18 [02:40<00:09,  9.59s/it]\u001b[A\n",
            "100%|██████████| 18/18 [02:48<00:00,  9.38s/it]\n",
            " 60%|██████    | 3/5 [08:26<05:37, 168.83s/it]\n",
            "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/18 [00:09<02:45,  9.74s/it]\u001b[A\n",
            " 11%|█         | 2/18 [00:17<02:15,  8.48s/it]\u001b[A\n",
            " 17%|█▋        | 3/18 [00:26<02:13,  8.91s/it]\u001b[A\n",
            " 22%|██▏       | 4/18 [00:36<02:10,  9.31s/it]\u001b[A\n",
            " 28%|██▊       | 5/18 [00:46<02:04,  9.59s/it]\u001b[A\n",
            " 33%|███▎      | 6/18 [00:54<01:48,  9.01s/it]\u001b[A\n",
            " 39%|███▉      | 7/18 [01:03<01:37,  8.84s/it]\u001b[A\n",
            " 44%|████▍     | 8/18 [01:12<01:29,  8.94s/it]\u001b[A\n",
            " 50%|█████     | 9/18 [01:22<01:23,  9.27s/it]\u001b[A\n",
            " 56%|█████▌    | 10/18 [01:31<01:13,  9.18s/it]\u001b[A\n",
            " 61%|██████    | 11/18 [01:41<01:07,  9.62s/it]\u001b[A\n",
            " 67%|██████▋   | 12/18 [01:51<00:57,  9.64s/it]\u001b[A\n",
            " 72%|███████▏  | 13/18 [02:00<00:47,  9.50s/it]\u001b[A\n",
            " 78%|███████▊  | 14/18 [02:09<00:37,  9.29s/it]\u001b[A\n",
            " 83%|████████▎ | 15/18 [02:19<00:28,  9.46s/it]\u001b[A\n",
            " 89%|████████▉ | 16/18 [02:28<00:18,  9.44s/it]\u001b[A\n",
            " 94%|█████████▍| 17/18 [02:38<00:09,  9.66s/it]\u001b[A\n",
            "100%|██████████| 18/18 [02:48<00:00,  9.38s/it]\n",
            " 80%|████████  | 4/5 [11:15<02:48, 168.86s/it]\n",
            "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/18 [00:10<02:50, 10.00s/it]\u001b[A\n",
            " 11%|█         | 2/18 [00:19<02:36,  9.78s/it]\u001b[A\n",
            " 17%|█▋        | 3/18 [00:28<02:20,  9.34s/it]\u001b[A\n",
            " 22%|██▏       | 4/18 [00:37<02:10,  9.29s/it]\u001b[A\n",
            " 28%|██▊       | 5/18 [00:47<02:02,  9.42s/it]\u001b[A\n",
            " 33%|███▎      | 6/18 [00:56<01:54,  9.51s/it]\u001b[A\n",
            " 39%|███▉      | 7/18 [01:06<01:44,  9.52s/it]\u001b[A\n",
            " 44%|████▍     | 8/18 [01:16<01:35,  9.56s/it]\u001b[A\n",
            " 50%|█████     | 9/18 [01:25<01:24,  9.41s/it]\u001b[A\n",
            " 56%|█████▌    | 10/18 [01:34<01:15,  9.44s/it]\u001b[A\n",
            " 61%|██████    | 11/18 [01:45<01:08,  9.71s/it]\u001b[A\n",
            " 67%|██████▋   | 12/18 [01:53<00:56,  9.46s/it]\u001b[A\n",
            " 72%|███████▏  | 13/18 [02:03<00:47,  9.54s/it]\u001b[A\n",
            " 78%|███████▊  | 14/18 [02:13<00:38,  9.56s/it]\u001b[A\n",
            " 83%|████████▎ | 15/18 [02:22<00:28,  9.51s/it]\u001b[A\n",
            " 89%|████████▉ | 16/18 [02:31<00:18,  9.25s/it]\u001b[A\n",
            " 94%|█████████▍| 17/18 [02:40<00:09,  9.17s/it]\u001b[A\n",
            "100%|██████████| 18/18 [02:49<00:00,  9.41s/it]\n",
            "100%|██████████| 5/5 [14:04<00:00, 168.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OUTSIDE LOOP AUC of Best\n",
            "Complex name 2XA0 AUCROC 0.7337046204620461 AUCPR 0.006095993601647377\n",
            "Equivalent epochs in one iteration of data loader 1.8322724057795292\n",
            "test complex  3WN7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n",
            "  0%|          | 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "  6%|▌         | 1/18 [00:12<03:35, 12.67s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.939859391716338 AUCPR 0.2111478042786575\n",
            "AUCROC 0.939859391716338 AUCPR 0.2111478042786575 best aucroc 0.939859391716338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 11%|█         | 2/18 [00:22<02:54, 10.92s/it]\u001b[A\n",
            " 17%|█▋        | 3/18 [00:34<02:50, 11.38s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.9444444444444444 AUCPR 0.3197144551732254\n",
            "AUCROC 0.9444444444444444 AUCPR 0.3197144551732254 best aucroc 0.9444444444444444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 22%|██▏       | 4/18 [00:46<02:42, 11.60s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LOADED BEST AUCROC 0.946966223444903 AUCPR 0.3338039846841163\n",
            "AUCROC 0.946966223444903 AUCPR 0.3338039846841163 best aucroc 0.946966223444903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 28%|██▊       | 5/18 [00:57<02:30, 11.54s/it]\u001b[A\n",
            " 33%|███▎      | 6/18 [01:08<02:15, 11.33s/it]\u001b[A\n",
            " 39%|███▉      | 7/18 [01:18<01:59, 10.87s/it]\u001b[A\n",
            " 44%|████▍     | 8/18 [01:28<01:46, 10.70s/it]\u001b[A\n",
            " 50%|█████     | 9/18 [01:39<01:35, 10.62s/it]\u001b[A\n",
            " 56%|█████▌    | 10/18 [01:49<01:23, 10.45s/it]\u001b[A\n",
            " 61%|██████    | 11/18 [01:59<01:13, 10.47s/it]\u001b[A\n",
            " 67%|██████▋   | 12/18 [02:10<01:03, 10.64s/it]\u001b[A\n",
            " 72%|███████▏  | 13/18 [02:21<00:52, 10.60s/it]\u001b[A\n",
            " 78%|███████▊  | 14/18 [02:32<00:42, 10.69s/it]\u001b[A\n",
            " 83%|████████▎ | 15/18 [02:43<00:32, 10.71s/it]\u001b[A"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jan  9 13:06:21 2024\n",
        "\n",
        "@author: u1876024\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import glob\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import glob\n",
        "from Bio.PDB import *\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "from Bio.PDB.NeighborSearch import NeighborSearch\n",
        "from tqdm import tqdm as tqdm\n",
        "import pickle\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score\n",
        "import pickle\n",
        "from rdkit import Chem\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "from torch.autograd import Variable\n",
        "def cuda(v):\n",
        "    if USE_CUDA:\n",
        "        return v.cuda()\n",
        "    return v\n",
        "def toTensor(v,dtype = torch.float,requires_grad = False):\n",
        "    return cuda(Variable(torch.tensor(v)).type(dtype).requires_grad_(requires_grad))\n",
        "def toNumpy(v):\n",
        "    if USE_CUDA:\n",
        "        return v.detach().cpu().numpy()\n",
        "    return v.detach().numpy()\n",
        "\n",
        "'''\n",
        "Using Sklearn One hot encoder to encode the atoms\n",
        "Output is of size N*M where N is the total number of atoms and M is the total number of encoded features\n",
        "'''\n",
        "def atom1(structure):\n",
        "    atomslist=np.array(sorted(np.array(['C', 'CA', 'CB', 'CG', 'CH2', 'N','NH2',  'OG','OH', 'O1', 'O2', 'SE','1']))).reshape(-1,1)\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc.fit(atomslist)\n",
        "    atom_list=[]\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_name() in atomslist:\n",
        "            atom_list.append(atom.get_name())\n",
        "        else:\n",
        "            atom_list.append(\"1\")\n",
        "    atoms_onehot=enc.transform(np.array(atom_list).reshape(-1,1)).toarray()\n",
        "    return atoms_onehot\n",
        "##############\n",
        "'''\n",
        "One hot encoded residue infomration using SKlearn Library\n",
        "\n",
        "Output is N*M where N is the total number of atoms and M is the encoded features of the residues.\n",
        "Any unknown  residue is mapped to 1\n",
        "'''\n",
        "\n",
        "\n",
        "def res1(structure):\n",
        "    residuelist=np.array(sorted(np.array(['ALA', 'ARG', 'ASN', 'ASP', 'GLN', 'GLU', 'GLY', 'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL', 'CYS', 'HIS','1']))).reshape(-1,1)\n",
        "    encr = OneHotEncoder(handle_unknown='ignore')\n",
        "    encr.fit(residuelist)\n",
        "    residue_list=[]\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_parent().get_resname() in residuelist:\n",
        "            residue_list.append((atom.get_parent()).get_resname())\n",
        "        else:\n",
        "            residue_list.append(\"1\")\n",
        "\n",
        "    res_onehot=encr.transform(np.array(residue_list).reshape(-1,1)).toarray()\n",
        "\n",
        "    return res_onehot\n",
        "###########\n",
        "\n",
        "'''\n",
        "It calculates the neighbours of each atom i.e. 10 distinct neighbours\n",
        "Output is  in the form of a ditionary representing an  adjacency list where each source atom and neighbouring atom is represented bby its sequence index .\n",
        "'''\n",
        "\n",
        "\n",
        "def neigh1(structure):\n",
        "    #atom_list is a numpy array  that   contains all the atoms of the pdb file in atom object\n",
        "    atom_list=np.array([atom for atom in structure.get_atoms()])\n",
        "\n",
        "    #for atom in structure.get_atoms():\n",
        "    #    atom_list.append(atom)\n",
        "    #neighbour_list contains all the  neighbour atomic pairs  i.e. like if N has neighbours O and C then it is stored as [[N,C],[N,O]] i.e. has dimension N*2 where N is the total number of possible neighbours all the atoms have in an unsorted manner and it stores in the form of  atom object\n",
        "\n",
        "\n",
        "    p4=NeighborSearch(atom_list)\n",
        "    neighbour_list=p4.search_all(6,level=\"A\")\n",
        "    neighbour_list=np.array(neighbour_list)\n",
        "\n",
        "    #dist is the distance between the neighbour and the source atom  i.e. dimension is N*1\n",
        "    dist=np.array(neighbour_list[:,0]-neighbour_list[:,1])\n",
        "    #sorting in ascending order\n",
        "    place=np.argsort(dist)\n",
        "    sorted_neighbour_list=neighbour_list[place]\n",
        "\n",
        "    #old_atom_number is used for  storing atom id of the original protein before sorting\n",
        "    #old_residue_number is used for storing residue number of the original protein before sorting\n",
        "    source_vertex_list_atom_object=np.array(sorted_neighbour_list[:,0])\n",
        "    len_source_vertex=len(source_vertex_list_atom_object)\n",
        "    neighbour_vertex_with_respect_each_source_atom_object=np.array(sorted_neighbour_list[:,1])\n",
        "    old_atom_number=[]\n",
        "    old_residue_number=[]\n",
        "    for i in atom_list:\n",
        "        old_atom_number.append(i.get_serial_number())\n",
        "        old_residue_number.append(i.get_parent().get_id()[1])\n",
        "    old_atom_number=np.array(old_atom_number)\n",
        "    old_residue_number=np.array(old_residue_number)\n",
        "    req_no=len(neighbour_list)\n",
        "    total_atoms=len(atom_list)\n",
        "    #neigh_same_res is the 2D numpy array to store the indices of the  neighbours of  same residue and is of the shape N*10 where N is the total number of atoms\n",
        "    #neigh_diff_res is 2D numpy array to store  the indices of the  neighbours of different residue\n",
        "    #same_flag is used to restrict the neighbours belonging to same residue  to 10\n",
        "    #diff_flag is used to restrict the neighbours belonging to different residue to 10\n",
        "    neigh_same_res=np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    neigh_diff_res=np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    same_flag=[0]*total_atoms\n",
        "    diff_flag=[0]*total_atoms\n",
        "    for i in range(len_source_vertex):\n",
        "        source_atom_id=source_vertex_list_atom_object[i].get_serial_number()\n",
        "        neigh_atom_id=neighbour_vertex_with_respect_each_source_atom_object[i].get_serial_number()\n",
        "        source_atom_res=source_vertex_list_atom_object[i].get_parent().get_id()[1]\n",
        "        neigh_atom_res=neighbour_vertex_with_respect_each_source_atom_object[i].get_parent().get_id()[1]\n",
        "        #finding out index of the source and neighbouring atoms from the original atom array with respect to their residue id and atom id\n",
        "        temp_index1=np.where(source_atom_id==old_atom_number)[0]\n",
        "\n",
        "        temp_index2=np.where(neigh_atom_id==old_atom_number)[0]\n",
        "        for i1 in temp_index1:\n",
        "            if old_residue_number[i1]==source_atom_res:\n",
        "                source_index=i1\n",
        "                break\n",
        "        for i1 in temp_index2:\n",
        "            if old_residue_number[i1]==neigh_atom_res:\n",
        "                neigh_index=i1\n",
        "                break\n",
        "        #if both the residues are same\n",
        "\n",
        "        if source_atom_res==neigh_atom_res :\n",
        "\n",
        "            #limiting the number of neighbours of same residue to 10\n",
        "\n",
        "            if int(same_flag[source_index])< 10:\n",
        "                neigh_same_res[source_index][same_flag[source_index]]=neigh_index\n",
        "                same_flag[source_index]+=1\n",
        "\n",
        "            if int(same_flag[neigh_index])< 10:\n",
        "                neigh_same_res[neigh_index][same_flag[neigh_index]]=source_index\n",
        "                same_flag[neigh_index]+=1\n",
        "\n",
        "        # if both the residues are different\n",
        "        elif source_atom_res!=neigh_atom_res :\n",
        "\n",
        "            #limiting the number of neighbours of different residues to 10\n",
        "\n",
        "            if int(diff_flag[source_index])< 10:\n",
        "                neigh_diff_res[source_index][diff_flag[source_index]]=neigh_index\n",
        "                diff_flag[source_index]+=1\n",
        "\n",
        "\n",
        "            if int(diff_flag[neigh_index])< 10:\n",
        "\n",
        "                neigh_diff_res[neigh_index][diff_flag[neigh_index]]=source_index\n",
        "                diff_flag[neigh_index]+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return neigh_same_res,neigh_diff_res\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA is available. Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")\n",
        "class GNN_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, v_feats, trainable=True, **kwargs):\n",
        "        #pdb.set_trace()\n",
        "        super(GNN_Layer, self).__init__()\n",
        "        self.v_feats = v_feats\n",
        "        self.filters = filters\n",
        "\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device=device\n",
        "        self.Wsv = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "        #print(\"Wsv shape\",self.Wsv.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        Z,same_neigh,diff_neigh = x\n",
        "        node_signals = Z@self.Wsv\n",
        "        neigh_signals_same=Z@self.Wsr\n",
        "        neigh_signals_diff=Z@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal = (torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal = (torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "        final_res = torch.relu(node_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res,same_neigh,diff_neigh\n",
        "\n",
        "class GNN_First_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, trainable=True, **kwargs):\n",
        "\n",
        "        super(GNN_First_Layer, self).__init__()\n",
        "        self.filters = filters\n",
        "        #pdb.set_trace()\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "        self.Wv = nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wr = nn.Parameter( torch.randn(21,self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        atoms, residues,same_neigh,diff_neigh = x\n",
        "        node_signals = atoms@self.Wv\n",
        "        residue_signals = residues@self.Wr\n",
        "        neigh_signals_same=atoms@self.Wsr\n",
        "        neigh_signals_diff=atoms@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        \"\"\"\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(1)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(1)\n",
        "        \"\"\"\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        #print(\"same norm\",same_neigh > -1, 1)\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        same_norm = torch.sum(same_neigh > -1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1).type(torch.float)\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal=(torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal=(torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "\n",
        "        final_res = torch.relu(node_signals+residue_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res, same_neigh,diff_neigh\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, in_dims, out_dims, trainable=True, **kwargs):\n",
        "        #pdb.set_trace()\n",
        "        super(Dense, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.out_dims = out_dims\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "\n",
        "        self.W = nn.Parameter( torch.randn(self.in_dims, self.out_dims, device=self.cuda_device,requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        Z = torch.sigmoid(torch.matmul(x, self.W))\n",
        "\n",
        "        return Z\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GNN_First_Layer(filters=512)\n",
        "        self.conv2 = GNN_Layer(v_feats=512, filters=1024)\n",
        "        self.conv3 = GNN_Layer(v_feats=1024, filters=512)\n",
        "        self.dense = Dense(in_dims=512, out_dims=1)\n",
        "    def forward(self, x):\n",
        "        x1=self.conv1(x)\n",
        "        x2=self.conv2(x1)\n",
        "        x3=self.conv3(x2)\n",
        "        x=x3[0]\n",
        "        x=torch.sum(x,axis=0).view(1,-1)\n",
        "        x = F.normalize(x)\n",
        "        return x\n",
        "\n",
        "    def processProtein(UniqueProtein, PdBloc):#, PdBloc):\n",
        "        data_list = []\n",
        "\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        PData_dict={}\n",
        "        for i in range(len(UniqueProtein)):\n",
        "            #print('Converting PDB to Graph: {}/{}'.format(i+1, len(UniqueProtein)))\n",
        "            UniqueProtein[i]=UniqueProtein[i].split('.pdb')[0]\n",
        "            P1=PdBloc+UniqueProtein[i]+'.pdb'\n",
        "            #if P1 in UniqueProtein:\n",
        "            parser = PDBParser()\n",
        "            with warnings.catch_warnings(record=True) as w:\n",
        "              structure = parser.get_structure(\"\", P1)\n",
        "            one_hot_atom=(atom1(structure))\n",
        "\n",
        "            one_hot_res=(res1(structure))\n",
        "            neigh_same_res,neigh_diff_res=(neigh1(structure))\n",
        "            # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "            one_hot_atom=torch.tensor(one_hot_atom,dtype=torch.float32).to(device)\n",
        "            one_hot_res=torch.tensor(one_hot_res,dtype=torch.float32).to(device)\n",
        "            neigh_same_res=torch.tensor(neigh_same_res).to(device).long()\n",
        "            neigh_diff_res=torch.tensor(neigh_diff_res).to(device).long()\n",
        "            GNNData = [one_hot_atom,one_hot_res,neigh_same_res,neigh_diff_res]\n",
        "            PData_dict[UniqueProtein[i]]= GNNData\n",
        "        return PData_dict\n",
        "def readFile(filename):\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  Name=[];PdbId=[];Ligandnames=[];SMILES=[];labels=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      #if len(d)==6:\n",
        "      name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "      Name.append(name);PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);labels.append(float (y));\n",
        "  return  PdbId,Ligandnames,SMILES,labels\n",
        "class IPPI_MLP_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IPPI_MLP_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2840, 1024)#4096)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 100)\n",
        "        self.fc6 = nn.Linear(100, 1)\n",
        "    def forward(self, PFeatures,LigandFeatures,ProteinInterfaceF):\n",
        "          Cfeatures=LigandFeatures\n",
        "          P_all_Features=torch.hstack((PFeatures,ProteinInterfaceF))\n",
        "          PC_Features=torch.hstack((P_all_Features,Cfeatures))\n",
        "          x = torch.tanh(self.fc1(PC_Features))\n",
        "          x = torch.tanh(self.fc2(x))\n",
        "          x = torch.relu(self.fc3(x))\n",
        "          x = self.fc6(x)\n",
        "          return x\n",
        "\n",
        "#path,githubpath='/content/drive/MyDrive/GNN-PPI-Inhibitor/','/content/PPI-Inhibitors/'\n",
        "path,githubpath='/content/drive/MyDrive/GNN-PPI-Inhibitor/','/content/PPI-Inhibitors/'\n",
        "\"\"\"\n",
        "path = location of pkl files from these links:\n",
        "https://drive.google.com/file/d/1goeDiPZSKT1Xx3j00eNG9xlqYkLLv1gW/view\n",
        "https://drive.google.com/file/d/1GOYEKLQCoGea9QQ72kujy0rdJKbUSYAE/view\n",
        "\n",
        "githubpath =  location of the directory containing the github repo PPI-Inhibitors\n",
        "obtained using\n",
        "git clone https://github.com/adibayaseen/PPI-Inhibitors\n",
        "\"\"\"\n",
        "Ubench5InterfaceandSeq_dict=pickle.load(open(githubpath+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "Pos_seqandInterfaceF_dict=pickle.load(open(githubpath+'Features/Pos_seqandInterfaceF_dict.npy',\"rb\"))\n",
        "Complex_AllFeatures_dict=dict( list (Pos_seqandInterfaceF_dict.items())+list (Ubench5InterfaceandSeq_dict.items()))\n",
        "##############\n",
        "ComplexInterfaceFeatures={}\n",
        "for key in Complex_AllFeatures_dict:\n",
        "  if len(key.split('_'))>1:\n",
        "    compname=key.split('_')[0]\n",
        "    ComplexInterfaceFeatures[compname]=Complex_AllFeatures_dict[key]#.cuda()\n",
        "  else:\n",
        "    ComplexInterfaceFeatures[key]=Complex_AllFeatures_dict[key]#.cuda()\n",
        "CompoundFingerprintFeaturesDict=pickle.load(open(githubpath+'Features/Compound_Fingerprint_Features_Dict.npy',\"rb\"))\n",
        "#Load Protein data for GNN\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "ProteinDataGNN_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "DBD5_ProteinDataGNN_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "All_ProteinData_dict=dict( list (ProteinDataGNN_dict.items())+list (DBD5_ProteinDataGNN_dict.items()))\n",
        "for d in All_ProteinData_dict:\n",
        "  data=All_ProteinData_dict[d]\n",
        "  All_ProteinData_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "#########\n",
        "with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN_Binary.txt') as f:\n",
        "#with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN.txt') as f:\n",
        "    D = f.readlines()\n",
        "Labels=[];Ligandnames=[];Complexs=[];TestPoscomplexes=[];#SMILESlist=[];\n",
        "for d in tqdm(D):\n",
        "  if len(d.split())==4:\n",
        "      TestPoscomp,Complexname,Ligandname,label = d.split()\n",
        "  else:\n",
        "      TestPoscomp,Complexname,Ligandname,label = d.split()[0],d.split()[1],(' ').join(d.split()[2:-1]),d.split()[-1]\n",
        "  TestPoscomplexes.append(TestPoscomp),Ligandnames.append(Ligandname);Complexs.append(Complexname);Labels.append(float (label))\n",
        "#########Make dictionary, Rootcomplexname=(complexname,compoundname),label\n",
        "Allexamples=dict (zip(zip(TestPoscomplexes,zip(Complexs,Ligandnames)),Labels))\n",
        "#Group kfold\n",
        "Alldata=list (Allexamples.keys())\n",
        "KK=[k[0].split('_')[0] for k in Alldata]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "AUC_ROC_final=[];Avg_P_final=[];Z=[];Yo=[];Y_t=[];Y_score=[];\n",
        "from os import listdir\n",
        "AlltestExamples=[];Externallabels=[];ExternalscoresLOCO=[];covid19_Externallabels=[];covid19_ExternalscoresLOCO=[];Y_score=[];Y_t=[];classratio_dict={};\n",
        "AUC_ROC_final=[];Avg_P_final=[];\n",
        "Complexs,Ligandnames, Labels=np.array(Complexs),np.array(Ligandnames),np.array(Labels)\n",
        "Alldata=np.array(Alldata, dtype=object)\n",
        "classratio_dict=pickle.load(open(githubpath+'Features/Classratio_GNNdict.npy','rb'))\n",
        "\n",
        "#%% Cross-validation\n",
        "# Done=set(KK).difference(['3D9T','1BKD','4ESG','2FLU','1YCQ','2XA0','3TDU','3D9T','2B4J','3DAB','3UVW','2RNY','4AJY', '1F47','1YCR','4QC3','1NW9','2E3K','4YY6','4GQ6','3WN7','1BXL','1Z92'])\n",
        "Done=set(KK).difference(['3D9T','1BKD','4ESG','2FLU','1YCQ'])\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    train,test=Alldata[trainindex],Alldata[testindex]\n",
        "    test_complex_name = test[0][0].split('_')[0]\n",
        "\n",
        "    # if test[0][0].split('_')[0] in Done:\n",
        "    if test_complex_name not in Done:\n",
        "      continue\n",
        "\n",
        "    Ctr=[];Ptr=[];y_train=[];Ctrname=[];Ptrname=[];Xtr=[];G=[];Cttname=[];Ctt=[];y_test=[];Ptt=[];Pttname=[];\n",
        "    #Split train and test\n",
        "    for t in train:\n",
        "        Ctrname.append(t[1][1]);Ctr.append(CompoundFingerprintFeaturesDict[t[1][1]]);\n",
        "        #change this only for GNN Complex_AllFeatures_dict with All_ProteinData_dic and t\n",
        "        #####\n",
        "        GNNcomp=t[1][0].split('_')[0]#t[1][0].split('_')[0]\n",
        "        Ptrname.append(GNNcomp);Ptr.append(ComplexInterfaceFeatures[GNNcomp]);\n",
        "        y_train.append(Allexamples[t[0],t[1]])\n",
        "    #Split train and test\n",
        "    for t in test:\n",
        "        GNNcomp=t[1][0].split('_')[0]\n",
        "        Cttname.append(t[1][1]);Ctt.append(CompoundFingerprintFeaturesDict[t[1][1]]);\n",
        "        Pttname.append(GNNcomp);Ptt.append(ComplexInterfaceFeatures[GNNcomp]);\n",
        "        y_test.append(Allexamples[t[0],t[1]])\n",
        "    #standarization\n",
        "    Pscaler = StandardScaler().fit(Ptr)\n",
        "    Cscaler = StandardScaler().fit(Ctr)\n",
        "    Ctr = Cscaler.transform(Ctr)\n",
        "    Ptr=Pscaler.transform(Ptr)\n",
        "    Ptt=Pscaler.transform(Ptt)\n",
        "    Ptrdict=dict (zip(Ptrname,torch.FloatTensor(Ptr).cuda()))\n",
        "    Ctrdict=dict (zip (Ctrname,torch.FloatTensor( Ctr).cuda()))\n",
        "    Ctt = Cscaler.transform(Ctt)\n",
        "    Cttdict=dict (zip (Cttname,torch.FloatTensor( Ctt).cuda()))\n",
        "    Pttdict=dict (zip(Pttname,torch.FloatTensor(Ptt).cuda()))\n",
        "\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "\n",
        "    GNN_model=GNN().cuda()\n",
        "    Mcomplexname=test[0][0].split('_')[0]\n",
        "    criterion  = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(list (IPPI_Net.parameters()) + list( GNN_model.parameters()),lr=0.001,weight_decay=0.0)#001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "\n",
        "    # bsize = 1024\n",
        "    bsize = 2048\n",
        "\n",
        "\n",
        "    dataset = CustomDataset(train[:,1], y_train.astype('int'))\n",
        "    batch_sampler = BinaryBalancedSampler(y_train.astype('int'), bsize)\n",
        "    loader = DataLoader(dataset, batch_sampler=batch_sampler) # data loader that selects equal number of positive and negative examples\n",
        "\n",
        "    test_dataset = CustomDataset(test[:,1], np.array(y_test).astype('int'))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bsize, shuffle=False)\n",
        "\n",
        "\n",
        "    #y_train=torch.FloatTensor( y_train).cuda()\n",
        "    ####\n",
        "    print (\"test complex \", Mcomplexname)\n",
        "\n",
        "    Loss = [] #save loss values for plotting\n",
        "    E = [] #save examples\n",
        "    L = [] #save labels\n",
        "    terminated = False\n",
        "    best_result = 0.0\n",
        "    best_model = None\n",
        "    counter = 0\n",
        "    early_stop_count = 0\n",
        "    Zlist,Ylist=[],[]\n",
        "    NUM_EPOCHS = 1\n",
        "    for iters in tqdm(range(NUM_EPOCHS)):\n",
        "        for (batch_pids,batch_cids),batch_labels in tqdm(loader):\n",
        "            GNN_model.train()\n",
        "            IPPI_Net.train()\n",
        "            E.extend(zip(batch_pids,batch_cids))\n",
        "            L.append(batch_labels)\n",
        "            pids = [p.split('_')[0] for p in batch_pids]\n",
        "            G_dict = {p:GNN_model(All_ProteinData_dict[p]) for p in set(pids)} #pass each unique complex through the GNN once\n",
        "            GNN_features = torch.vstack([G_dict[p] for p in pids]) #append to make examples\n",
        "            del G_dict #clear up memory\n",
        "            interface_features = torch.vstack([Ptrdict[p] for p in pids])\n",
        "            compound_features = torch.vstack([Ctrdict[c] for c in batch_cids])\n",
        "            #[GNN_model(All_ProteinData_dict[p]) for p in set_pids]\n",
        "            output = IPPI_Net(GNN_features,compound_features,interface_features)\n",
        "            V = np.min(list(classratio_dict.values()))\n",
        "            weights = toTensor(np.array([classratio_dict[p]/V if batch_labels[i]==1 else 1.0 for i,p in enumerate(pids)  ]))\n",
        "            criterion  = nn.BCEWithLogitsLoss(weight = None)\n",
        "            loss = criterion(output.flatten(), batch_labels.float().cuda())\n",
        "            Loss.append(loss.item())\n",
        "            #if np.median(Loss[-10:])<1e-1: terminated = True\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            early_stop_count += 1\n",
        "            #%% Validation/Testing (saves the best model in every 10 iterations over the validation set)\n",
        "            GNN_model.eval()\n",
        "            IPPI_Net.eval()\n",
        "            Z, Y = [], []\n",
        "            with torch.no_grad():\n",
        "                for (batch_pids,batch_cids),batch_labels in test_loader:\n",
        "                    pids = [p.split('_')[0] for p in batch_pids]\n",
        "                    G_dict = {p:GNN_model(All_ProteinData_dict[p]) for p in set(pids)} #pass each unique complex through the GNN once\n",
        "                    GNN_features = torch.vstack([G_dict[p] for p in pids]) #append to make examples\n",
        "                    del G_dict #clear up memory\n",
        "                    interface_features = torch.vstack([Pttdict[p] for p in pids])\n",
        "                    compound_features = torch.vstack([Cttdict[c] for c in batch_cids])\n",
        "                    output = IPPI_Net(GNN_features,compound_features,interface_features)\n",
        "                    Z.extend(output.cpu().flatten().numpy())\n",
        "                    Y.extend(batch_labels.cpu().flatten().numpy())\n",
        "                aucroc = roc_auc_score(np.array(Y), np.array(Z))\n",
        "                aucpr = average_precision_score(Y,Z)\n",
        "                if aucroc>best_result:\n",
        "                    early_stop_count = 0\n",
        "                    best_result = aucroc\n",
        "                    best_model = (GNN_model.state_dict(),IPPI_Net.state_dict())\n",
        "                    IPPI_Net.load_state_dict(best_model[1])#path+'/newIPPI_Net_'+test[0][0].split('_')[0]+'_AUC_'+str (round (best_result,3)))#torch.load(path+'IPPI_Net_'+ Mcomplexname)[1])\n",
        "                    GNN_model.load_state_dict(best_model[0])\n",
        "                    GNN_model.eval()\n",
        "                    IPPI_Net.eval()\n",
        "                    Zb, Yb = [], []\n",
        "                    for (batch_pids,batch_cids),batch_labels in test_loader:\n",
        "                      pids = [p.split('_')[0] for p in batch_pids]\n",
        "                      G_dict = {p:GNN_model(All_ProteinData_dict[p]) for p in set(pids)} #pass each unique complex through the GNN once\n",
        "                      GNN_features = torch.vstack([G_dict[p] for p in pids]) #append to make examples\n",
        "                      del G_dict #clear up memory\n",
        "                      interface_features = torch.vstack([Pttdict[p] for p in pids])\n",
        "                      compound_features = torch.vstack([Cttdict[c] for c in batch_cids])\n",
        "                      bestmodeloutput=IPPI_Net(GNN_features,compound_features,interface_features)\n",
        "                      #torch.save(Loss, path+'/Loss_'+test[0][0].split('_')[0])\n",
        "                      Zb.extend(bestmodeloutput.cpu().flatten().numpy())\n",
        "                      Yb.extend(batch_labels.cpu().flatten().numpy())\n",
        "                    aucrocb = roc_auc_score(np.array(Yb), np.array(Zb))\n",
        "                    aucprb = average_precision_score(Yb,Zb)\n",
        "                    print('LOADED BEST AUCROC',aucrocb,'AUCPR',aucprb)#,'best aucroc')\n",
        "                    aucpr = average_precision_score(Y,Z)\n",
        "                    print('AUCROC',aucroc,'AUCPR',aucpr,'best aucroc',best_result)\n",
        "    ###Load best model\n",
        "    print (\"OUTSIDE LOOP AUC of Best\")\n",
        "    torch.save(best_model[1], path+'/GNN-based-pipeline_IPPI_Net_'+ test[0][0].split('_')[0])\n",
        "    torch.save(best_model[0], path+'/GNN-based-pipeline_GNN_model_'+ test[0][0].split('_')[0])\n",
        "    Zlist.extend(Zb);Ylist.extend(Yb)\n",
        "    np.save(path+test[0][0].split('_')[0]+'Scores',Zb)\n",
        "    np.save(path+test[0][0].split('_')[0]+'Targets',Yb)\n",
        "    aucrocb = roc_auc_score(np.array(Yb), np.array(Zb))\n",
        "    aucprb = average_precision_score(Yb,Zb)\n",
        "    print('Complex name',test[0][0].split('_')[0],'AUCROC',aucrocb,'AUCPR',aucprb)#,'best aucroc')\n",
        "\n",
        "    # ========================================\n",
        "    # EXTERNAL VALIDATION (Enabled by Default)\n",
        "    # ========================================\n",
        "    try:\n",
        "        print(f\"\\nExternal validation for {test_complex_name}...\")\n",
        "\n",
        "        # Test on Recent Publications\n",
        "        External_score, External_labels = PredictScorefromFile(\n",
        "            githubpath + '/Data/External data/2dyh_all.txt',\n",
        "            githubpath + '/Data/External data/pdb/',\n",
        "            Pscaler, Cscaler, IPPI_Net, GNN_model, test_complex_name)\n",
        "\n",
        "        ExternalscoresLOCO.extend(External_score)\n",
        "        Externallabels.extend(External_labels)\n",
        "        External_Auc = roc_auc_score(External_labels, External_score)\n",
        "        External_AP = average_precision_score(External_labels, External_score)\n",
        "        print(f\"  Recent Pubs - AUC-ROC: {External_Auc:.3f}, AUC-PR: {External_AP:.3f}\")\n",
        "\n",
        "        # Test on COVID-19\n",
        "        Covid19_External_score, Covid19_External_labels = PredictScorefromFile(\n",
        "            githubpath + '/Data/External data/HansonACE2hits.txt',\n",
        "            githubpath + '/Data/External data/pdb/',\n",
        "            Pscaler, Cscaler, IPPI_Net, GNN_model, test_complex_name)\n",
        "\n",
        "        covid19_Externallabels.extend(Covid19_External_labels)\n",
        "        covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "        Covid19_External_Auc = roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "        Covid19_External_AP = average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "        print(f\"  COVID-19 - AUC-ROC: {Covid19_External_Auc:.3f}, AUC-PR: {Covid19_External_AP:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ⚠️  External validation failed: {e}\")\n",
        "        print(f\"     Continuing with cross-validation results...\")\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(Ylist, Zlist)\n",
        "Auc = roc_auc_score(Ylist, Zlist)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Zlist=np.array(Zlist);Yo=np.array(Ylist);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Ylist, Zlist)\n",
        "aucpr=average_precision_score (Ylist, Zlist)\n",
        "########\n",
        "np.save(path+'GNN-pipeline_Targets.npy',Ylist)\n",
        "np.save(path+'GNN-pipeline_Scores.npy',Zlist)\n",
        "######+\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"GNN-pipeline AUC-PR for PPI Inhibitors.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr Over all complexes \\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])))#,\"\\n\")\n",
        "#######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"GNN-pipeline AUCROC for vPPI Inhibitors.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGCpIm336IUs",
        "outputId": "a15f17cb-138e-40a9-ab81-475f963d388c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final average over all folds,Leave one complex out nan ± nan nan ± nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
            "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlgI8EnYxPoS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "0cb3ab3d-385a-42af-d773-7be8dab048df"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/GNN-PPI-Inhibitor/onlyGearnet_Scores.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2007139325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgithubpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/GNN-PPI-Inhibitor/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/content/PPI-Inhibitors/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mZ_GearNet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'onlyGearnet_Scores.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mYo_GearNet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'onlyGearnet_Targets.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/GNN-PPI-Inhibitor/onlyGearnet_Scores.npy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score,precision_recall_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "path,githubpath='/content/drive/MyDrive/GNN-PPI-Inhibitor/','/content/PPI-Inhibitors/'\n",
        "Z_GearNet=np.load(path+'onlyGearnet_Scores.npy')\n",
        "Yo_GearNet=np.load(path+'onlyGearnet_Targets.npy')\n",
        "####\n",
        "fpr_GearNet, tpr_GearNet, thresholds_GearNet = roc_curve(Yo_GearNet, Z_GearNet)\n",
        "Auc_GearNet = roc_auc_score(Yo_GearNet, Z_GearNet)\n",
        "Auc_GearNet=(Auc_GearNet).round(2)\n",
        "# calculate precision-recall curve\n",
        "precision_GearNet, recall_GearNet, thresholds = precision_recall_curve(Yo_GearNet, Z_GearNet)\n",
        "aucpr_GearNet=auc(recall_GearNet,precision_GearNet)\n",
        "aucpr_GearNet=(aucpr_GearNet).round(2)\n",
        "#######here svm\n",
        "# Yo_SVM=np.load(path+'All_SVM_Targets.npy')\n",
        "# Z_SVM=np.load(path+'All_SVM_Scores.npy')\n",
        "####here svm\n",
        "# fpr_SVM, tpr_SVM, thresholds_SVM = roc_curve(Yo_SVM, Z_SVM)\n",
        "# Auc_SVM = roc_auc_score(Yo_SVM, Z_SVM)\n",
        "# Auc_SVM=(Auc_SVM).round(2)\n",
        "# calculate precision-recall curve here svm\n",
        "# precision_SVM, recall_SVM, thresholds = precision_recall_curve(Yo_SVM, Z_SVM)\n",
        "# aucpr_SVM=auc(recall_SVM,precision_SVM)\n",
        "# aucpr_SVM=(aucpr_SVM).round(2)\n",
        "#####Change this\n",
        "Yo_GNN=np.load(path+'GNN-pipeline_Targets.npy')\n",
        "Z_GNN=np.load(path+'GNN-pipeline_Scores.npy')\n",
        "##########\n",
        "fpr_GNN, tpr_GNN, thresholds_GNN = roc_curve(Yo_GNN, Z_GNN)\n",
        "Auc_GNN= roc_auc_score(Yo_GNN, Z_GNN)\n",
        "Auc_GNN=(Auc_GNN).round(2)\n",
        "# calculate precision-recall curve\n",
        "precision_GNN, recall_GNN, thresholds = precision_recall_curve(Yo_GNN, Z_GNN)\n",
        "aucpr_GNN=auc(recall_GNN,precision_GNN)\n",
        "aucpr_GNN=(aucpr_GNN).round(2)\n",
        "###### GNN LOCO average 0.8576 ± 0.0923 0.4366 ± 0.2003\n",
        "##### SVM LOCO average 0.7445 ± 0.1958 0.3312 ± 0.2017\n",
        "fig = plt.figure()\n",
        "Auc_GNN_std,PR_GNN_std,Auc_SVM_std,PR_SVM_std=0.0923,0.01,0.16,0.18\n",
        "Auc_GearNet_std,PR_GearNet_std=0.1,0.19\n",
        "#text=\"There is an upcoming task in %d days at %d cluster!\" %a %cluster\n",
        "plt.plot(fpr_GNN,tpr_GNN,color='m',marker=',',markersize=2,label = ('GNN AUC-ROC : $ {} ± {}$').format(round(Auc_GNN,2), round(Auc_GNN_std,2)))\n",
        "#heresvm plt.plot(fpr_SVM,tpr_SVM,color='b',marker=',',markersize=2,label=('SVM AUC-ROC : $ {} ± {}$').format(round(Auc_SVM,2), round(Auc_SVM_std,2)))\n",
        "plt.plot(fpr_GearNet,tpr_GearNet,color='k',marker='.', markersize=3,label=('GearNet AUC-ROC : $ {} ± {}$').format(round(Auc_GearNet,2),round(Auc_GearNet_std,2)))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"Comaprison of AUCROC SVM and GNN-base model PPI Inhibitors Random and Binders combine Negative.pdf\", bbox_inches='tight')\n",
        "#########\n",
        "fig = plt.figure()\n",
        "plt.plot(recall_GNN,precision_GNN,color='m',marker=',',markersize=2,label=('GNN AUC-PR : $ {} ± {}$').format(round(aucpr_GNN,2), round(PR_GNN_std,2)))\n",
        "#heresvm plt.plot(recall_SVM,precision_SVM,color='b',marker=',',markersize=2,label=('SVM AUC-PR: $ {} ± {}$').format(round(aucpr_SVM,2), round(PR_SVM_std,2)))\n",
        "plt.plot(recall_GearNet,precision_GearNet,color='k',marker='.', markersize=3,label=('GearNet AUC-PR: $ {} ± {}$').format(round(aucpr_GearNet,2), round(PR_GearNet_std,2)))\n",
        "plt.title('AUC-PR');plt.xlabel('Recall');plt.ylabel('Precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"Comaprison of AUC-PR SVM and GNN-base model PPI Inhibitors  Random and Binders combine.pdf\", bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "Y_score = np.array(Y_score)\n",
        "Y_t = np.array(Y_t)\n",
        "\n",
        "fpr, tpr, _ = roc_curve(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Y_t, Y_score)\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(Y_t, Y_score)\n",
        "aucpr = average_precision_score(Y_t, Y_score)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# AUC-ROC\n",
        "ax1.plot(fpr, tpr, color='k', marker='d', markersize=3, label=f'AUC: {Auc:.2f}')\n",
        "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "ax1.set_xlabel('FPR', fontsize=12)\n",
        "ax1.set_ylabel('TPR', fontsize=12)\n",
        "ax1.set_title('AUC-ROC', fontsize=14, fontweight='bold')\n",
        "ax1.grid(alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# AUC-PR\n",
        "ax2.plot(recall, precision, color='m', marker=',', label=f'AUC-PR: {aucpr:.2f}')\n",
        "baseline = np.sum(Y_t) / len(Y_t)\n",
        "ax2.axhline(baseline, color='k', linestyle='--', alpha=0.3, label=f'Baseline: {baseline:.2f}')\n",
        "ax2.set_xlabel('Recall', fontsize=12)\n",
        "ax2.set_ylabel('Precision', fontsize=12)\n",
        "ax2.set_title('AUC-PR', fontsize=14, fontweight='bold')\n",
        "ax2.grid(alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ppi_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Results plotted\")"
      ],
      "metadata": {
        "id": "TaYf5rtlX-Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EXTERNAL VALIDATION SUMMARY (Optional)\n",
        "# ============================================================================\n",
        "if len(ExternalscoresLOCO) > 0:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXTERNAL VALIDATION - OVERALL RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Recent Publications\n",
        "    Overall_External_Auc = roc_auc_score(Externallabels, ExternalscoresLOCO)\n",
        "    Overall_External_AP = average_precision_score(Externallabels, ExternalscoresLOCO)\n",
        "\n",
        "    print(f\"\\nRecent Publications Dataset:\")\n",
        "    print(f\"  Examples: {len(Externallabels)}\")\n",
        "    print(f\"  AUC-ROC: {Overall_External_Auc:.3f}\")\n",
        "    print(f\"  AUC-PR:  {Overall_External_AP:.3f}\")\n",
        "    print(f\"  Expected: AUC-ROC ~0.82, AUC-PR ~0.45\")\n",
        "\n",
        "    # COVID-19\n",
        "    if len(covid19_ExternalscoresLOCO) > 0:\n",
        "        Overall_Covid19_Auc = roc_auc_score(covid19_Externallabels, covid19_ExternalscoresLOCO)\n",
        "        Overall_Covid19_AP = average_precision_score(covid19_Externallabels, covid19_ExternalscoresLOCO)\n",
        "\n",
        "        print(f\"\\nCOVID-19 ACE2 Inhibitors Dataset:\")\n",
        "        print(f\"  Examples: {len(covid19_Externallabels)}\")\n",
        "        print(f\"  AUC-ROC: {Overall_Covid19_Auc:.3f}\")\n",
        "        print(f\"  AUC-PR:  {Overall_Covid19_AP:.3f}\")\n",
        "        print(f\"  Expected: AUC-ROC ~0.78, AUC-PR ~0.42\")\n",
        "\n",
        "    # Plot external validation results\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Recent Publications\n",
        "    fpr, tpr, _ = roc_curve(Externallabels, ExternalscoresLOCO)\n",
        "    axes[0].plot(fpr, tpr, 'b-', linewidth=2,\n",
        "                 label=f'AUC-ROC: {Overall_External_Auc:.3f}')\n",
        "    axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "    axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
        "    axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
        "    axes[0].set_title('External: Recent Publications', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # COVID-19\n",
        "    if len(covid19_ExternalscoresLOCO) > 0:\n",
        "        fpr, tpr, _ = roc_curve(covid19_Externallabels, covid19_ExternalscoresLOCO)\n",
        "        axes[1].plot(fpr, tpr, 'r-', linewidth=2,\n",
        "                     label=f'AUC-ROC: {Overall_Covid19_Auc:.3f}')\n",
        "        axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "        axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
        "        axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
        "        axes[1].set_title('External: COVID-19 ACE2', fontsize=14, fontweight='bold')\n",
        "        axes[1].grid(alpha=0.3)\n",
        "        axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('external_validation_results.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n✓ External validation complete!\")\n",
        "else:\n",
        "    print(\"\\n⚠️  External validation was not run\")\n",
        "    print(\"   (This is optional - cross-validation results are the main results)\")"
      ],
      "metadata": {
        "id": "nAfaCtAKYEzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HNk6-cjzYxrO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}