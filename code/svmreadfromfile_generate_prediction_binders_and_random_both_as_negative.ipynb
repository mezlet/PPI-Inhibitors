{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mezlet/PPI-Inhibitors/blob/main/code/svmreadfromfile_generate_prediction_binders_and_random_both_as_negative.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zy8pv8_EQa"
      },
      "source": [
        "**Set the Runtime->Change Runtime Type to GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLKfwNHlwnSE"
      },
      "source": [
        "# Protein 3d structure assessment with graph neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_3U6PueCCgv",
        "outputId": "3fb58b86-bf29-4111-e49d-ca52748892bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'PPI-Inhibitors': No such file or directory\n",
            "Cloning into 'PPI-Inhibitors'...\n",
            "remote: Enumerating objects: 1341, done.\u001b[K\n",
            "remote: Total 1341 (delta 0), reused 0 (delta 0), pack-reused 1341 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1341/1341), 2.59 GiB | 42.45 MiB/s, done.\n",
            "Resolving deltas: 100% (417/417), done.\n",
            "Updating files: 100% (605/605), done.\n"
          ]
        }
      ],
      "source": [
        "#!rm -r Data\n",
        "!rm -r PPI-Inhibitors\n",
        "!git clone https://github.com/adibayaseen/PPI-Inhibitors\n",
        "#!pip install py3Dmol"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install rdkit biopython==1.81 torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 torch-geometric==2.5.3 tqdm==4.66.2 pandas==2.1.4 numpy==1.26.4 scikit-learn==1.3.2 matplotlib==3.8.3 seaborn==0.13.2 networkx==3.2.1 gdown\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hTn0aB_NgmsD",
        "outputId": "4d0a01c7-6cde-427d-b17e-4f88608fdad9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.2 setuptools-80.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "39a1f21531d94728bf8e77a2c9b30187"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting biopython==1.81\n",
            "  Downloading biopython-1.81-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision==0.17.2\n",
            "  Downloading torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.2.2\n",
            "  Downloading torchaudio-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torch-geometric==2.5.3\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
            "Collecting tqdm==4.66.2\n",
            "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting pandas==2.1.4\n",
            "  Downloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scikit-learn==1.3.2\n",
            "  Downloading scikit_learn-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting matplotlib==3.8.3\n",
            "  Downloading matplotlib-3.8.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Collecting networkx==3.2.1\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.2) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (1.16.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (3.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (2.32.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (3.2.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.2) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (25.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.6.85)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric==2.5.3) (3.11)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric==2.5.3) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric==2.5.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric==2.5.3) (2025.10.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Downloading biopython-1.81-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.2.2-cp312-cp312-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "Downloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m190.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m126.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m171.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m182.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m198.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m146.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m166.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading rdkit-2025.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (36.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m138.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, rdkit, pandas, nvidia-cusolver-cu12, nvidia-cudnn-cu12, biopython, torch, scikit-learn, matplotlib, torchvision, torchaudio, torch-geometric\n",
            "\u001b[2K  Attempting uninstall: tqdm\n",
            "\u001b[2K    Found existing installation: tqdm 4.67.1\n",
            "\u001b[2K    Uninstalling tqdm-4.67.1:\n",
            "\u001b[2K      Successfully uninstalled tqdm-4.67.1\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.0.2\n",
            "\u001b[2K    Uninstalling numpy-2.0.2:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K  Attempting uninstall: networkx\n",
            "\u001b[2K    Found existing installation: networkx 3.5\n",
            "\u001b[2K    Uninstalling networkx-3.5:\n",
            "\u001b[2K      Successfully uninstalled networkx-3.5\n",
            "\u001b[2K  Attempting uninstall: pandas\n",
            "\u001b[2K    Found existing installation: pandas 2.2.2\n",
            "\u001b[2K    Uninstalling pandas-2.2.2:\n",
            "\u001b[2K      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.8.0+cu126\n",
            "\u001b[2K    Uninstalling torch-2.8.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torch-2.8.0+cu126\n",
            "\u001b[2K  Attempting uninstall: scikit-learn\n",
            "\u001b[2K    Found existing installation: scikit-learn 1.6.1\n",
            "\u001b[2K    Uninstalling scikit-learn-1.6.1:\n",
            "\u001b[2K      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[2K  Attempting uninstall: matplotlib\n",
            "\u001b[2K    Found existing installation: matplotlib 3.10.0\n",
            "\u001b[2K    Uninstalling matplotlib-3.10.0:\n",
            "\u001b[2K      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[2K  Attempting uninstall: torchvision\n",
            "\u001b[2K    Found existing installation: torchvision 0.23.0+cu126\n",
            "\u001b[2K    Uninstalling torchvision-0.23.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "\u001b[2K  Attempting uninstall: torchaudio\n",
            "\u001b[2K    Found existing installation: torchaudio 2.8.0+cu126\n",
            "\u001b[2K    Uninstalling torchaudio-2.8.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/23\u001b[0m [torch-geometric]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "imbalanced-learn 0.14.0 requires scikit-learn<2,>=1.4.2, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "xarray 2025.10.1 requires pandas>=2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "datasets 4.0.0 requires tqdm>=4.66.3, but you have tqdm 4.66.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.2 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed biopython-1.81 matplotlib-3.8.3 networkx-3.2.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 pandas-2.1.4 rdkit-2025.9.1 scikit-learn-1.3.2 torch-2.2.2 torch-geometric-2.5.3 torchaudio-2.2.2 torchvision-0.17.2 tqdm-4.66.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "f8dafa30c5cb42f0bcaee5b31b526b07"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PPI-Inhibitors\n",
        "!mkdir -p Data\n",
        "\n",
        "%cd Data\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/01ad4975fb9133825b1bf9e71b64fcdaaa5e4d8b/Data/2p2iComplexPairs.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/01ad4975fb9133825b1bf9e71b64fcdaaa5e4d8b/Data/2p2iInhibitorsSMILES.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/2d6bd03422602ec19147870c487e64018b52660f/Data/WriteAllexamplesRandomBindersIdsAll_24JAN_Binary.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/b1e45884f61f792399abad2e4492f48083ab1093/Data/BindersWithComplexname.csv\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuLR1ISNgzd9",
        "outputId": "d621179a-ef39-4ce1-89ce-1b3ebe3a8ff2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PPI-Inhibitors\n",
            "/content/PPI-Inhibitors/Data\n",
            "/content/PPI-Inhibitors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJsLo_nfg4m2",
        "outputId": "979f4b44-fe3d-4cd5-fa36-d3b8ad7a668a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzwASCsDn1O2",
        "outputId": "08fac1f2-de09-42a8-c9d9-a37485f4abf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.2.2+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp312-cp312-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.2+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_sparse-0.6.18%2Bpt22cu121-cp312-cp312-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.2)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt22cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.2+cu121.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_cluster-1.6.3%2Bpt22cu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-cluster) (1.16.2)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt22cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.2+cu121.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt22cu121-cp312-cp312-linux_x86_64.whl (946 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m946.2/946.2 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt22cu121\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (1.16.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric) (3.11)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.10.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->torch-geometric) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->torch-geometric) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gglFcYgfcvGE",
        "outputId": "743481a1-24af-49a7-9ace-1c2bdf797de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kora\n",
            "  Downloading kora-0.9.20-py3-none-any.whl.metadata (703 bytes)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from kora) (7.34.0)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.12/dist-packages (from kora) (1.8.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from fastcore->kora) (25.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (80.9.0)\n",
            "Collecting jedi>=0.16 (from ipython->kora)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython->kora) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kora) (0.2.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython->kora) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython->kora) (0.7.0)\n",
            "Downloading kora-0.9.20-py3-none-any.whl (57 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, kora\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [kora]\n",
            "\u001b[1A\u001b[2KSuccessfully installed jedi-0.19.2 kora-0.9.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/kora/__init__.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  from pkg_resources import get_distribution\n"
          ]
        }
      ],
      "source": [
        "#Compound part\n",
        "!pip install kora\n",
        "import kora.install.rdkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qO- https://micro.mamba.pm/install.sh | bash\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvAqQnx8p_kP",
        "outputId": "313c805c-d90c-4048-fd2d-29101e0d2d9f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running `shell init`, which:\n",
            " - modifies RC file: \"/root/.bashrc\"\n",
            " - generates config for root prefix: \u001b[1m\"/root/micromamba\"\u001b[0m\n",
            " - sets mamba executable to: \u001b[1m\"/root/.local/bin/micromamba\"\u001b[0m\n",
            "The following has been added in your \"/root/.bashrc\" file\n",
            "\u001b[32m\n",
            "# >>> mamba initialize >>>\n",
            "# !! Contents within this block are managed by 'micromamba shell init' !!\n",
            "export MAMBA_EXE='/root/.local/bin/micromamba';\n",
            "export MAMBA_ROOT_PREFIX='/root/micromamba';\n",
            "__mamba_setup=\"$(\"$MAMBA_EXE\" shell hook --shell bash --root-prefix \"$MAMBA_ROOT_PREFIX\" 2> /dev/null)\"\n",
            "if [ $? -eq 0 ]; then\n",
            "    eval \"$__mamba_setup\"\n",
            "else\n",
            "    alias micromamba=\"$MAMBA_EXE\"  # Fallback on help from micromamba activate\n",
            "fi\n",
            "unset __mamba_setup\n",
            "# <<< mamba initialize <<<\n",
            "\u001b[0m\n",
            "Please restart your shell to activate micromamba or run the following:\\n\n",
            "  source ~/.bashrc (or ~/.zshrc, ~/.xonshrc, ~/.config/fish/config.fish, ...)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PATH\"] += \":/root/.local/bin:/root/micromamba/bin\"\n"
      ],
      "metadata": {
        "id": "fnuPP5eMrYs1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!micromamba --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEhCCrLerb_N",
        "outputId": "f1ceaba7-4247-44b7-bf9f-935de0a55d69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!micromamba create -y -n bioenv -c conda-forge -c bioconda biopython\n",
        "!micromamba run -n bioenv python -c \"import Bio; print('BioPython version:', Bio.__version__)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt4WAKWuqYuC",
        "outputId": "705c465b-0c90-4a82-df3d-6c1a4a63c234"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "bioconda/linux-64 ..  ⣾  \n",
            "bioconda/noarch (c..  ⣾  \n",
            "nodefaults/linux-6..  ⣾  \n",
            "nodefaults/noarch ..  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
            "nodefaults/linux-6..  ⣾  \n",
            "nodefaults/noarch ..  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
            "nodefaults/linux-6..  ⣾  \n",
            "nodefaults/noarch ..  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\n",
            "nodefaults/linux-6..  ⣾  \n",
            "nodefaults/noarch ..  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\n",
            "nodefaults/linux-6..  ⣾  \n",
            "nodefaults/noarch ..  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\n",
            "nodefaults/noarch ..  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "conda-forge/linux-64  ⣾  \n",
            "conda-forge/noarch    ⣾  \n",
            "bioconda/linux-64     ⣾  \n",
            "bioconda/noarch       ⣾  \n",
            "nodefaults/linux-64   ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
            "conda-forge/linux-64   1%\n",
            "conda-forge/noarch     2%\n",
            "bioconda/linux-64     11%\n",
            "bioconda/noarch       ⣾  \n",
            "nodefaults/linux-64   ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnodefaults/linux-64                               \n",
            "[+] 0.3s\n",
            "conda-forge/linux-64   6%\n",
            "conda-forge/noarch    15%\n",
            "bioconda/linux-64     68%\n",
            "bioconda/noarch       21%\n",
            "nodefaults/noarch     ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gbioconda/linux-64                                 \n",
            "[+] 0.4s\n",
            "conda-forge/linux-64  11%\n",
            "conda-forge/noarch    26%\n",
            "bioconda/noarch       73%\n",
            "nodefaults/noarch     ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gbioconda/noarch                                   \n",
            "[+] 0.5s\n",
            "conda-forge/linux-64  20%\n",
            "conda-forge/noarch    43%\n",
            "nodefaults/noarch     ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnodefaults/noarch                                 \n",
            "[+] 0.6s\n",
            "conda-forge/linux-64  30%\n",
            "conda-forge/noarch    65%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\n",
            "conda-forge/linux-64  41%\n",
            "conda-forge/noarch    88%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\n",
            "conda-forge/linux-64  47%\n",
            "conda-forge/noarch   100%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                \n",
            "[+] 0.9s\n",
            "conda-forge/linux-64  57%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\n",
            "conda-forge/linux-64  74%\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
            "conda-forge/linux-64  90%\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                              \n",
            "\u001b[?25h\n",
            "\n",
            "Transaction\n",
            "\n",
            "  Prefix: /root/micromamba/envs/bioenv\n",
            "\n",
            "  Updating specs:\n",
            "\n",
            "   - biopython\n",
            "\n",
            "\n",
            "  Package               Version  Build                 Channel          Size\n",
            "──────────────────────────────────────────────────────────────────────────────\n",
            "  Install:\n",
            "──────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "  \u001b[32m+ _libgcc_mutex   \u001b[0m        0.1  conda_forge           conda-forge       3kB\n",
            "  \u001b[32m+ _openmp_mutex   \u001b[0m        4.5  2_gnu                 conda-forge      24kB\n",
            "  \u001b[32m+ biopython       \u001b[0m       1.85  py314h5bd0f2a_2       conda-forge       4MB\n",
            "  \u001b[32m+ bzip2           \u001b[0m      1.0.8  hda65f42_8            conda-forge     260kB\n",
            "  \u001b[32m+ ca-certificates \u001b[0m  2025.10.5  hbd8a1cb_0            conda-forge     156kB\n",
            "  \u001b[32m+ ld_impl_linux-64\u001b[0m       2.44  ha97dd6f_2            conda-forge     747kB\n",
            "  \u001b[32m+ libblas         \u001b[0m      3.9.0  37_h4a7cf45_openblas  conda-forge      17kB\n",
            "  \u001b[32m+ libcblas        \u001b[0m      3.9.0  37_h0358290_openblas  conda-forge      17kB\n",
            "  \u001b[32m+ libexpat        \u001b[0m      2.7.1  hecca717_0            conda-forge      75kB\n",
            "  \u001b[32m+ libffi          \u001b[0m      3.4.6  h2dba641_1            conda-forge      57kB\n",
            "  \u001b[32m+ libgcc          \u001b[0m     15.2.0  h767d61c_7            conda-forge     823kB\n",
            "  \u001b[32m+ libgfortran     \u001b[0m     15.2.0  h69a702a_7            conda-forge      29kB\n",
            "  \u001b[32m+ libgfortran5    \u001b[0m     15.2.0  hcd61629_7            conda-forge       2MB\n",
            "  \u001b[32m+ libgomp         \u001b[0m     15.2.0  h767d61c_7            conda-forge     448kB\n",
            "  \u001b[32m+ liblapack       \u001b[0m      3.9.0  37_h47877c9_openblas  conda-forge      17kB\n",
            "  \u001b[32m+ liblzma         \u001b[0m      5.8.1  hb9d3cd8_2            conda-forge     113kB\n",
            "  \u001b[32m+ libmpdec        \u001b[0m      4.0.0  hb9d3cd8_0            conda-forge      91kB\n",
            "  \u001b[32m+ libopenblas     \u001b[0m     0.3.30  pthreads_h94d23a6_2   conda-forge       6MB\n",
            "  \u001b[32m+ libsqlite       \u001b[0m     3.50.4  h0c1763c_0            conda-forge     933kB\n",
            "  \u001b[32m+ libstdcxx       \u001b[0m     15.2.0  h8f9b012_7            conda-forge       4MB\n",
            "  \u001b[32m+ libuuid         \u001b[0m     2.41.2  he9a06e4_0            conda-forge      37kB\n",
            "  \u001b[32m+ libzlib         \u001b[0m      1.3.1  hb9d3cd8_2            conda-forge      61kB\n",
            "  \u001b[32m+ ncurses         \u001b[0m        6.5  h2d0b736_3            conda-forge     892kB\n",
            "  \u001b[32m+ numpy           \u001b[0m      2.3.3  py314h5d5eb18_0       conda-forge       9MB\n",
            "  \u001b[32m+ openssl         \u001b[0m      3.5.4  h26f9b46_0            conda-forge       3MB\n",
            "  \u001b[32m+ pip             \u001b[0m       25.2  pyh145f28c_0          conda-forge       1MB\n",
            "  \u001b[32m+ python          \u001b[0m     3.14.0  h5989046_101_cp314    conda-forge      37MB\n",
            "  \u001b[32m+ python_abi      \u001b[0m       3.14  8_cp314               conda-forge       7kB\n",
            "  \u001b[32m+ readline        \u001b[0m        8.2  h8c095d6_2            conda-forge     282kB\n",
            "  \u001b[32m+ tk              \u001b[0m     8.6.13  noxft_hd72426e_102    conda-forge       3MB\n",
            "  \u001b[32m+ tzdata          \u001b[0m      2025b  h78e105d_0            conda-forge     123kB\n",
            "  \u001b[32m+ zstd            \u001b[0m      1.5.7  hb8e6e7a_2            conda-forge     568kB\n",
            "\n",
            "  Summary:\n",
            "\n",
            "  Install: 32 packages\n",
            "\n",
            "  Total download: 74MB\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────────────\n",
            "\n",
            "\n",
            "\n",
            "Transaction starting\n",
            "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
            "Downloading        5%\n",
            "Extracting         0%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
            "Downloading  (5)   0%\n",
            "Extracting         0%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibstdcxx                                            3.9MB @  12.9MB/s  0.1s\n",
            "[+] 0.2s\n",
            "Downloading  (5)   8%\n",
            "Extracting   (1)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibopenblas                                          5.9MB @  16.9MB/s  0.2s\n",
            "[+] 0.3s\n",
            "Downloading  (5)  35%\n",
            "Extracting         4%\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnumpy                                                9.0MB @  27.7MB/s  0.3s\n",
            "tk                                                   3.3MB @  11.9MB/s  0.2s\n",
            "[+] 0.4s\n",
            "Downloading  (5)  65%\n",
            "Extracting   (3)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibgfortran5                                         1.6MB @  11.7MB/s  0.1s\n",
            "openssl                                              3.1MB @  17.5MB/s  0.1s\n",
            "biopython                                            3.5MB @   5.3MB/s  0.4s\n",
            "ncurses                                            891.6kB @  ??.?MB/s  0.1s\n",
            "pip                                                  1.2MB @   2.0MB/s  0.1s\n",
            "[+] 0.5s\n",
            "Downloading  (5)  84%\n",
            "Extracting   (5)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibsqlite                                          932.6kB @  ??.?MB/s  0.1s\n",
            "libgcc                                             822.6kB @  ??.?MB/s  0.1s\n",
            "ld_impl_linux-64                                   747.2kB @  ??.?MB/s  0.1s\n",
            "zstd                                               567.6kB @  ??.?MB/s  0.1s\n",
            "libgomp                                            447.9kB @  ??.?MB/s  0.1s\n",
            "[+] 0.6s\n",
            "Downloading  (4)  89%\n",
            "Extracting   (6)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Greadline                                           282.5kB @  ??.?MB/s  0.1s\n",
            "bzip2                                              260.3kB @  ??.?MB/s  0.1s\n",
            "[+] 0.7s\n",
            "Downloading  (5)  92%\n",
            "Extracting   (4)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gca-certificates                                    155.9kB @   1.1MB/s  0.1s\n",
            "tzdata                                             123.0kB @ 947.3kB/s  0.1s\n",
            "liblzma                                            112.9kB @ 928.5kB/s  0.1s\n",
            "libmpdec                                            91.2kB @  ??.?MB/s  0.1s\n",
            "python                                              36.7MB @  44.2MB/s  0.7s\n",
            "libuuid                                             37.1kB @  ??.?MB/s  0.0s\n",
            "libzlib                                             61.0kB @  ??.?MB/s  0.1s\n",
            "[+] 0.8s\n",
            "Downloading  (4) 100%\n",
            "Extracting   (4)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibffi                                              57.4kB @  ??.?MB/s  0.1s\n",
            "libgfortran                                         29.3kB @  ??.?MB/s  0.1s\n",
            "libexpat                                            74.8kB @  ??.?MB/s  0.1s\n",
            "_openmp_mutex                                       23.6kB @  ??.?MB/s  0.1s\n",
            "liblapack                                           17.5kB @ 304.6kB/s  0.1s\n",
            "libcblas                                            17.5kB @  ??.?MB/s  0.1s\n",
            "[+] 0.9s\n",
            "Downloading  (3) 100%\n",
            "Extracting   (1)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibblas                                             17.5kB @ 166.6kB/s  0.1s\n",
            "_libgcc_mutex                                        2.6kB @  ??.?MB/s  0.1s\n",
            "python_abi                                           7.0kB @  ??.?MB/s  0.1s\n",
            "[+] 1.0s\n",
            "Downloading      100%\n",
            "Extracting   (1)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
            "Downloading      100%\n",
            "Extracting   (1)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\n",
            "Downloading      100%\n",
            "Extracting   (1)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\n",
            "Downloading      100%\n",
            "Extracting   (1)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s\n",
            "Downloading      100%\n",
            "Extracting   (1)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s\n",
            "Downloading      100%\n",
            "Extracting   (1)  ⣾  \u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25hLinking python_abi-3.14-8_cp314\n",
            "Linking tzdata-2025b-h78e105d_0\n",
            "Linking ca-certificates-2025.10.5-hbd8a1cb_0\n",
            "Linking ld_impl_linux-64-2.44-ha97dd6f_2\n",
            "Linking libgomp-15.2.0-h767d61c_7\n",
            "Linking _libgcc_mutex-0.1-conda_forge\n",
            "Linking _openmp_mutex-4.5-2_gnu\n",
            "Linking libgcc-15.2.0-h767d61c_7\n",
            "Linking libmpdec-4.0.0-hb9d3cd8_0\n",
            "Linking libgfortran5-15.2.0-hcd61629_7\n",
            "Linking ncurses-6.5-h2d0b736_3\n",
            "Linking libzlib-1.3.1-hb9d3cd8_2\n",
            "Linking liblzma-5.8.1-hb9d3cd8_2\n",
            "Linking libffi-3.4.6-h2dba641_1\n",
            "Linking bzip2-1.0.8-hda65f42_8\n",
            "Linking openssl-3.5.4-h26f9b46_0\n",
            "Linking libuuid-2.41.2-he9a06e4_0\n",
            "Linking libexpat-2.7.1-hecca717_0\n",
            "Linking libstdcxx-15.2.0-h8f9b012_7\n",
            "Linking libgfortran-15.2.0-h69a702a_7\n",
            "Linking readline-8.2-h8c095d6_2\n",
            "Linking libsqlite-3.50.4-h0c1763c_0\n",
            "Linking tk-8.6.13-noxft_hd72426e_102\n",
            "Linking zstd-1.5.7-hb8e6e7a_2\n",
            "Linking libopenblas-0.3.30-pthreads_h94d23a6_2\n",
            "Linking python-3.14.0-h5989046_101_cp314\n",
            "Linking libblas-3.9.0-37_h4a7cf45_openblas\n",
            "Linking libcblas-3.9.0-37_h0358290_openblas\n",
            "Linking liblapack-3.9.0-37_h47877c9_openblas\n",
            "Linking pip-25.2-pyh145f28c_0\n",
            "Linking numpy-2.3.3-py314h5d5eb18_0\n",
            "Linking biopython-1.85-py314h5bd0f2a_2\n",
            "\n",
            "Transaction finished\n",
            "\n",
            "\n",
            "To activate this environment, use:\n",
            "\n",
            "    micromamba activate bioenv\n",
            "\n",
            "Or to execute a single command in this environment, use:\n",
            "\n",
            "    micromamba run -n bioenv mycommand\n",
            "\n",
            "BioPython version: 1.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "etHfJlkb6uhF"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Created on Thu Oct 11 16:31:29 2012\n",
        "@author: Afsar\n",
        "This module extracts information from the bound structures involved in a complex\n",
        "It performs mapping between the sequences of the bound and unbound sequences\n",
        "which are extracted from their structure files. This is done using global\n",
        "sequence alignment. This mapping is then used in extracting positive and\n",
        "negative training examples in getExamplesDBD.py\n",
        "\"\"\"\n",
        "#from BISEPutils import *\n",
        "#from myPDBUpdated import *\n",
        "import Bio.pairwise2\n",
        "def mapU2B(us, uS2Ri, ulR, bs, bS2Ri, blR):\n",
        "    \"\"\"\n",
        "    Get the mapping of indices between the bound and unbound residues\n",
        "    us: sequence of the unbound protein\n",
        "    uS2Ri: seq. to R index list for unbound protein\n",
        "    ulR: length of R list for the unbound protein\n",
        "    Similar for the bound protein\n",
        "    Return: u2b,b2u\n",
        "    #uR[i] corresponds to bR[u2b[i]] if u2b[i] is not nan\n",
        "    #bR[i] corresponds to uR[b2u[i]] if b2u[i] is not nan\n",
        "    \"\"\"\n",
        "    u2b = [np.nan for k in range(ulR)]\n",
        "    b2u = [np.nan for k in range(blR)]\n",
        "    aln = Bio.pairwise2.align.globalxs(us, bs, -1, -0.1)[0]\n",
        "\n",
        "    # print aln\n",
        "    ui = 0\n",
        "    bi = 0\n",
        "    for k in range(len(aln[0])):\n",
        "        uc = aln[0][k]\n",
        "        bc = aln[1][k]\n",
        "        if uc == bc:\n",
        "            u2b[uS2Ri[ui]] = bS2Ri[bi]\n",
        "            b2u[bS2Ri[bi]] = uS2Ri[ui]\n",
        "        ui = ui + (uc != '-')\n",
        "        bi = bi + (bc != '-')\n",
        "    pu2b = sum(np.isnan(u2b)) / float(len(u2b))\n",
        "    pb2u = sum(np.isnan(b2u)) / float(len(b2u))\n",
        "    #    if  pu2b > 0.01:\n",
        "    #        print \"Warning: \"+str(pu2b*100)+\" % of unbound residues not matched\"\n",
        "    #    if  pb2u > 0.01:\n",
        "    #        print \"Warning: \"+str(pb2u*100)+\" % of bound residues not matched\"\n",
        "    return (u2b, b2u)\n",
        "\n",
        "\n",
        "class myPDBComplex:\n",
        "    \"\"\"\n",
        "    Class responsible for extracting information from the bound structures of\n",
        "    the proteins.\n",
        "    Attributes:\n",
        "        fname: list of paths of files constituting the complex\n",
        "        N: length of fnames\n",
        "        cid: list of list of chain ids of each file\n",
        "        R: R[i] is the list of residues (biopython) for file i\n",
        "        Coords: Coords[i][j] is a list of coordinates of residue R[j] for file i\n",
        "        seq: seq[i] contains the combination of the peptide sequence\n",
        "        dthr: distance threshold\n",
        "        D: D[i][j] is the list of distances between files i and j\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fnames, dthr=6.0):\n",
        "\n",
        "        self.fnames = fnames\n",
        "        self.N = len(fnames)\n",
        "        self.dthr = dthr\n",
        "\n",
        "        self.Coords = []\n",
        "        self.R = []\n",
        "        self.D = [[[] for i in range(self.N)] for j in range(self.N)]\n",
        "        self.seq = []\n",
        "        # self.stx=[]\n",
        "        self.S2Ri = []\n",
        "        for (i, f) in enumerate(fnames):\n",
        "            (_, R, _, seq, S2Ri) = readPDB(f)  #(cid,stx,R,pp,seq,S2Ri)\n",
        "            #            self.cid.append(cid)\n",
        "            #self.stx.append(stx)\n",
        "            self.seq.append(seq)\n",
        "            self.R.append(R)\n",
        "            self.S2Ri.append(S2Ri)\n",
        "            self.Coords.append(getCoords(R))\n",
        "            for j in range(0, i):\n",
        "                (self.D[i][j], self.D[j][i]) = getDist(self.Coords[i], self.Coords[j], dthr)\n",
        "\n",
        "        \"\"\"\n",
        "        #Required for ASA computations\n",
        "        out_file = tempfile.NamedTemporaryFile(suffix='.pair.pdb')\n",
        "        out_file.close()\n",
        "        tmpfile=out_file.name\n",
        "        mergePDBFiles(fnames,tmpfile)\n",
        "\n",
        "        os.remove(tmpfile)\n",
        "        \"\"\"\n",
        "\n",
        "    def findNforU(self, ufnames):\n",
        "        \"\"\"\n",
        "        returns the distance object but changes the indices to represent the\n",
        "        mapping between bound and unbound structures\n",
        "        \"\"\"\n",
        "        D = [[[] for i in range(self.N)] for j in range(self.N)]\n",
        "        U2B = []\n",
        "        B2U = []\n",
        "        LenU = []\n",
        "        for (idx, f) in enumerate(ufnames):\n",
        "            (_, uR, _, us, uS2Ri) = readPDB(f)\n",
        "            (bR, bs, bS2Ri) = (self.R[idx], self.seq[idx], self.S2Ri[idx])\n",
        "            (u2b, b2u) = mapU2B(us, uS2Ri, len(uR), bs, bS2Ri, len(bR))\n",
        "            LenU.append(len(uR))\n",
        "            U2B.append(u2b)\n",
        "            B2U.append(b2u)\n",
        "        for i in range(self.N):\n",
        "            for j in range(i + 1, self.N):\n",
        "                oD = self.D[i][j]\n",
        "                for k in range(len(oD)):\n",
        "                    ov = oD[k]\n",
        "                    vij = (B2U[i][ov[0]], B2U[j][ov[1]], ov[2])\n",
        "                    vji = (B2U[j][ov[1]], B2U[i][ov[0]], ov[2])\n",
        "                    D[i][j].append(vij)\n",
        "                    D[j][i].append(vji)\n",
        "        return (D, LenU)\n",
        "import Bio\n",
        "from Bio.PDB import *\n",
        "from Bio import SeqIO\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "#from Bio.Alphabet import IUPAC\n",
        "import os\n",
        "def readPDB(fname,name=None):\n",
        "    \"\"\"\n",
        "    Extract info from a PDB file\n",
        "        fname: path of pdb file\n",
        "        name: name of the structure (default name of the file without extension)\n",
        "        return:: (stx,R,pp,seq,S2Ri)\n",
        "\n",
        "            stx: structure object\n",
        "            R: list of residues\n",
        "            pp: list of polypeptides in the structure\n",
        "            seq: combined sequence (for all polypeptides)\n",
        "            S2Ri: Sequence to R mapping index list, seq[i] corresponds to\n",
        "                R[S2Ri[i]]\n",
        "    \"\"\"\n",
        "    stxin=(type(fname)!=type(''))\n",
        "\n",
        "    if name is None:\n",
        "        if not stxin:\n",
        "            (_,name,_)=getFileParts(fname)\n",
        "        else:\n",
        "            name=fname.id\n",
        "    if not stxin:\n",
        "        stx=PDBParser() .get_structure(name,fname)\n",
        "    else:\n",
        "        stx=fname\n",
        "    if len(stx)>1:\n",
        "        stx2=Bio.PDB.Structure.Structure(stx.id)\n",
        "        print (stx2)\n",
        "        stx2.add(stx[0])\n",
        "        stx=stx2\n",
        "    if len(stx)!=1:\n",
        "        raise ValueError(\"Unexpected number of structures in \"+name)\n",
        "    #assert len(stx)==1 #there should be only one structure\n",
        "#    cid=[]\n",
        "#    for c in stx[0].get_list():\n",
        "#        cid.append(c.id)\n",
        "    R=Selection.unfold_entities(stx,'R') #list of residues\n",
        "    pp=PPBuilder().build_peptides(stx)\n",
        "    if len(pp)==0:\n",
        "        pp=CaPPBuilder().build_peptides(stx)\n",
        "    seq=''.join([p.get_sequence().tostring() for p in pp])\n",
        "    Rdict=dict(zip(R,range(len(R))))\n",
        "    S2Ri=[Rdict[r] for p in pp for r in p]\n",
        "    return (stx,R,pp,seq,S2Ri)\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Oct 04 01:30:15 2012\n",
        "@author: Afsar\n",
        "This module contains helper functions.\n",
        "\"\"\"\n",
        "import os\n",
        "#from add2path import *\n",
        "# import pdb\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import Bio\n",
        "from Bio.PDB import *\n",
        "from Bio import SeqIO\n",
        "import tempfile\n",
        "from scipy.sparse import lil_matrix\n",
        "\n",
        "from Bio.PDB.Polypeptide import one_to_three, three_to_one\n",
        "import urllib, os, traceback, pdb\n",
        "from time import time\n",
        "\n",
        "AA = 'ACDEFGHIKLMNPQRSTVWY-'\n",
        "to_one_letter_code = {}\n",
        "aaidx = dict(zip(AA, range(len(AA))))\n",
        "aa3idx = {}\n",
        "for __i__, __a__ in enumerate(AA):\n",
        "    try:\n",
        "        aa3idx[one_to_three(__a__)] = __i__\n",
        "        to_one_letter_code[one_to_three(__a__)] = __a__\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "def module_exists(module_name):\n",
        "    try:\n",
        "        __import__(module_name)\n",
        "    except ImportError:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\"\"\"\n",
        "def score_match(pair, matrix=MatrixInfo.blosum62):\n",
        "\n",
        "    #Given a tuple pair of amino acids, it returns the substitution matrix score\n",
        "\n",
        "\n",
        "    if pair not in matrix:\n",
        "        pair = tuple(reversed(pair))\n",
        "    if pair not in matrix:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return matrix[pair]\n",
        "\n",
        "def getSubMat(matrix=MatrixInfo.blosum62):\n",
        "\"\"\"\n",
        "    #Returns a dictionary representation of the columns of a substitution matrix\n",
        "\"\"\"\n",
        "    M = {}\n",
        "    for a in AA:\n",
        "        M[a] = []\n",
        "        for b in AA:\n",
        "            M[a].extend([score_match((a, b), matrix)])\n",
        "    return M\n",
        "\n",
        "\n",
        "BLOSUM62 = getSubMat()\n",
        "\"\"\"\n",
        "def getWPSSM(xpm, HWS=5):\n",
        "    \"\"\"\n",
        "    Given a np.array xpm this function creates the windowed representation\n",
        "    (for use in PSSM, PSFM etc)\n",
        "    \"\"\"\n",
        "    HWS = int(HWS)\n",
        "    (d, N) = xpm.shape\n",
        "    pm = np.hstack((np.zeros((d, HWS)), xpm, np.zeros((d, HWS))))\n",
        "    ws = 2 * HWS + 1\n",
        "    wpm = np.zeros((ws * d, N))\n",
        "    for i in range(N):\n",
        "        wpm[:, i] = pm[:, i:i + ws].flatten('F')\n",
        "    return wpm\n",
        "\n",
        "\n",
        "def getSubMatFeats(s, HWS=5):\n",
        "    \"\"\"\n",
        "    Given a sequence, this function returns the subsitution matrix representation\n",
        "    \"\"\"\n",
        "    HWS = int(HWS)\n",
        "    smat = np.zeros((len(AA), len(s)))\n",
        "    for (i, a) in enumerate(s):\n",
        "        try:\n",
        "            smat[:, i] = BLOSUM62[a]\n",
        "        except Exception as e:\n",
        "            print (e)\n",
        "            continue\n",
        "    return getWPSSM(smat, HWS)\n",
        "\n",
        "\n",
        "def renameChainPDB(ifile, oldChainId=' ', newChainId='_', ofile=None):\n",
        "    \"\"\"\n",
        "    Rename chains in pdb and save file\n",
        "    \"\"\"\n",
        "    if ofile is None:\n",
        "        ofile = ifile\n",
        "    (stx, _, _, _, _) = readPDB(ifile)\n",
        "    eids = [c.id for c in stx[0]]\n",
        "    if type(oldChainId) == type(''):\n",
        "        oldChainId = [oldChainId]\n",
        "        newChainId = [newChainId]\n",
        "\n",
        "    for i, oc in enumerate(oldChainId):\n",
        "        if oc in eids:\n",
        "            stx[0][oc].id = newChainId[i]\n",
        "        else:\n",
        "            print (\"Warning: Chain id \\'\" + oc + \"\\' Not found in\", ifile)\n",
        "\n",
        "    io = PDBIO()\n",
        "    io.set_structure(stx)\n",
        "    io.save(ofile)\n",
        "\n",
        "\n",
        "def fetchPDB(i, ofile=None,\n",
        "             url=\"http://www.rcsb.org/pdb/download/downloadFile.do?fileFormat=pdb&compression=NO&structureId=\"):\n",
        "    \"\"\"\n",
        "    Dowload\n",
        "    Fetches a pdb file for id \"i\" and puts it in bdir\n",
        "    \"\"\"\n",
        "    if ofile is None or not ofile:\n",
        "        ofile = './'\n",
        "    if os.path.isdir(ofile):\n",
        "        ofile = os.path.join(ofile, i + '.pdb')  # os.path.join(bdir,i+\".pdb\")\n",
        "    pdbid = url + str(i)\n",
        "    with open(ofile, \"w\") as fh:\n",
        "        fh.write(urllib.urlopen(pdbid).read())\n",
        "        fh.flush()\n",
        "    \"\"\"\n",
        "    while(not fh.closed):\n",
        "        pass\n",
        "\n",
        "    t0=time();\n",
        "    while(time()-t0<1.0): #Poll uptil 1 second and make sure that the file downloaded is readable\n",
        "        try:\n",
        "            with open(ofile,'rU'):\n",
        "                break\n",
        "        except IOError:\n",
        "            pass\n",
        "    \"\"\"\n",
        "    return ofile\n",
        "\n",
        "\n",
        "def downloadPDBList(pdblist, bdir):\n",
        "    \"\"\"\n",
        "    Download a list of pdb files\n",
        "    \"\"\"\n",
        "    for i in pdblist:\n",
        "        print (\"Downloading\", i)\n",
        "        try:\n",
        "            fetchPDB(i, bdir=bdir)\n",
        "        except Exception as e:\n",
        "            print ('-' * 60)\n",
        "            print ('###PROCESSSING FAILED FOR ', i, e)\n",
        "            traceback.print_exc(file=sys.stdout)\n",
        "            print ('-' * 60)\n",
        "\n",
        "\n",
        "def getDSSP(stx, fname):\n",
        "    \"\"\"\n",
        "    Biopython's dssp does not process broken chains (or missing residues). So\n",
        "    what we do here is to write a temp pdb file for each peptide and apply DSSP\n",
        "    on it. The return is a dictionary object in which the DSSP proeprties for\n",
        "    all residues for all chains have been merged.\n",
        "    \"\"\"\n",
        "\n",
        "    class pepSelect(Select):\n",
        "        \"\"\"\n",
        "        Required for selecting the residues within a peptide to write them to a\n",
        "        file.\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, p):\n",
        "            self.pL = [r.get_full_id() for r in p]\n",
        "\n",
        "        def accept_residue(self, res):\n",
        "            # pdb.set_trace()\n",
        "            resid = res.get_full_id()  #[:-1]\n",
        "            if resid in self.pL:\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "\n",
        "    pp = PPBuilder().build_peptides(stx[0])\n",
        "    io = PDBIO()\n",
        "    io.set_structure(stx)\n",
        "    dssp = dict()\n",
        "    out_file = tempfile.NamedTemporaryFile(suffix='.dssp')\n",
        "    tmpfname = out_file.name\n",
        "    out_file.close()\n",
        "    try:\n",
        "        for p in pp:\n",
        "            io.save(tmpfname, pepSelect(p))\n",
        "            d = DSSP(stx[0], tmpfname)\n",
        "            dssp = dict(dssp, **d)\n",
        "    except:\n",
        "        e = sys.exc_info()[0]\n",
        "        print (e)\n",
        "        pdb.set_trace()\n",
        "        print  (\"Oops! Problem running DSSP! Is it installed correctly?\")\n",
        "    finally:\n",
        "        os.remove(tmpfname)\n",
        "    return dssp\n",
        "\n",
        "\n",
        "def getFileParts(fname):\n",
        "    \"Returns the parts of a file\"\n",
        "    (path, name) = os.path.split(fname)\n",
        "    n = os.path.splitext(name)[0]\n",
        "    ext = os.path.splitext(name)[1]\n",
        "    return (path, n, ext)\n",
        "\n",
        "\n",
        "def getResiId(fid):\n",
        "    \"\"\"\n",
        "    Given the full id of a residue, return the tuple id form\n",
        "    \"\"\"\n",
        "    (_, _, cid, (_, ridx, rinum)) = fid\n",
        "    return (cid, str(ridx) + rinum.strip())  #\n",
        "\n",
        "\n",
        "def getResLetter(r2):\n",
        "    \"\"\"\n",
        "    Get the letter code for a biopython residue object\n",
        "    \"\"\"\n",
        "    r2name = r2.get_resname()\n",
        "    if to_one_letter_code.has_key(r2name):\n",
        "        scode = to_one_letter_code[r2name]\n",
        "    else:\n",
        "        scode = '-'\n",
        "    return scode\n",
        "\n",
        "\n",
        "def getSideChainV(r):\n",
        "    \"\"\"\n",
        "    Find the average of the unit vectors to different atoms in the side chain\n",
        "    from the c-alpha atom. For glycine the average of the N-Ca and C-Ca is\n",
        "    used.\n",
        "    Returns (C-alpha coordinate vector, side chain unit vector) for residue r\n",
        "    \"\"\"\n",
        "    u = None\n",
        "    gly = 0\n",
        "    if Polypeptide.is_aa(r) and r.has_id('CA'):\n",
        "        ca = r['CA'].get_coord()\n",
        "        dv = np.array([ak.get_coord() for ak in r.get_unpacked_list()[4:]])\n",
        "        if len(dv) < 1:\n",
        "            if r.has_id('N') and r.has_id('C'):\n",
        "                dv = []\n",
        "                dv.append(r['C'].get_coord())\n",
        "                dv.append(r['N'].get_coord())\n",
        "                dv = np.array(dv)\n",
        "                gly = 1\n",
        "            else:\n",
        "                # pdb.set_trace()\n",
        "                return None\n",
        "        dv = dv - ca\n",
        "        if gly:\n",
        "            dv = -dv\n",
        "        n = np.sum(np.abs(dv) ** 2, axis=-1) ** (1. / 2)\n",
        "        v = dv / n[:, np.newaxis]\n",
        "        u = (Vector(ca), Vector(v.mean(axis=0)))\n",
        "    return u\n",
        "\n",
        "\n",
        "def getCoords(R):\n",
        "    \"\"\"\n",
        "    Get atom coordinates given a list of biopython residues\n",
        "    \"\"\"\n",
        "    Coords = []\n",
        "    for (idx, r) in enumerate(R):\n",
        "        v = [ak.get_coord() for ak in r.get_list()]\n",
        "        Coords.append(v)\n",
        "    return Coords\n",
        "\n",
        "\n",
        "def getDistMat(C0, C1=None):\n",
        "    \"\"\"\n",
        "    for i in range(len(xl)):\n",
        "    for j in range(len(xr)):\n",
        "    \"\"\"\n",
        "    sym = False\n",
        "    if C1 is None:\n",
        "        sym = True\n",
        "        C1 = C0\n",
        "    D = np.zeros((len(C0), len(C1)))\n",
        "    for i in range(len(C0)):\n",
        "        st = 0\n",
        "        if sym:\n",
        "            st = i + 1\n",
        "        for j in range(st, len(C1)):\n",
        "            d = spatial.distance.cdist(C0[i], C1[j]).min()\n",
        "            D[i, j] = d\n",
        "            if sym:\n",
        "                D[j, i] = d\n",
        "    return D\n",
        "\n",
        "\n",
        "def getDist(C0, C1, thr=np.inf):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    N0 = []\n",
        "    N1 = []\n",
        "    for i in range(len(C0)):\n",
        "        for j in range(len(C1)):\n",
        "            d = spatial.distance.cdist(C0[i], C1[j]).min()\n",
        "            # dji=spatial.distance.cdist(C1[j], C0[i]).min()\n",
        "            #d=min(dij,dji)\n",
        "            #print d\n",
        "            if (d < thr):  # and not np.isnan(self.Phi[i]) and not np.isnan(self.Phi[j])\n",
        "                N0.append((i, j, d))\n",
        "                N1.append((j, i, d))\n",
        "    return (N0, N1)\n",
        "\n",
        "\n",
        "def readFASTA(fname):\n",
        "    \"\"\"\n",
        "    Reads the fasta file fname and returns the sequence only\n",
        "    \"\"\"\n",
        "    handle = open(fname, \"rU\")\n",
        "    record = list(SeqIO.parse(handle, \"fasta\"))\n",
        "    handle.close()\n",
        "    if len(record) > 1:\n",
        "        print (\"Warning: Input FASTA file must have only one protein sequence in it. Using only the first sequence.\")\n",
        "    # pdb.set_trace()\n",
        "    record = record[0]\n",
        "    return record.seq.tostring().upper()\n",
        "\n",
        "\n",
        "def readPDB(fname, name=None):\n",
        "    \"\"\"\n",
        "    Extract info from a PDB file\n",
        "        fname: path of pdb file\n",
        "        name: name of the structure (default name of the file without extension)\n",
        "        return:: (stx,R,pp,seq,S2Ri)\n",
        "\n",
        "            stx: structure object\n",
        "            R: list of residues\n",
        "            pp: list of polypeptides in the structure\n",
        "            seq: combined sequence (for all polypeptides)\n",
        "            S2Ri: Sequence to R mapping index list, seq[i] corresponds to\n",
        "                R[S2Ri[i]]\n",
        "    \"\"\"\n",
        "    stxin = (type(fname) != type(''))\n",
        "\n",
        "    if name is None:\n",
        "        if not stxin:\n",
        "            (_, name, _) = getFileParts(fname)\n",
        "        else:\n",
        "            name = fname.id\n",
        "    if not stxin:\n",
        "        stx = PDBParser().get_structure(name, fname)\n",
        "    else:\n",
        "        stx = fname\n",
        "    if len(stx) > 1:\n",
        "        stx2 = Bio.PDB.Structure.Structure(stx.id)\n",
        "        stx2.add(stx[0])\n",
        "        stx = stx2\n",
        "    if len(stx) != 1:\n",
        "        raise ValueError(\"Unexpected number of structures in \" + name)\n",
        "        # assert len(stx)==1 #there should be only one structure\n",
        "        # cid=[]\n",
        "    #    for c in stx[0].get_list():\n",
        "    #        cid.append(c.id)\n",
        "    R = Selection.unfold_entities(stx, 'R')  # list of residues\n",
        "    pp = PPBuilder().build_peptides(stx)\n",
        "    if len(pp) == 0:\n",
        "        pp = CaPPBuilder().build_peptides(stx)\n",
        "    seq = ''.join([p.get_sequence().tostring() for p in pp])\n",
        "    Rdict = dict(zip(R, range(len(R))))\n",
        "    S2Ri = [Rdict[r] for p in pp for r in p]\n",
        "\n",
        "    return (stx, R, pp, seq, S2Ri)\n",
        "\n",
        "\n",
        "def getSeqFV(stx, R, HWS=10):\n",
        "    \"\"\"\n",
        "    Return a FV based on sequence (uses structure)\n",
        "    \"\"\"\n",
        "    HWS = int(HWS)\n",
        "    # FV=[[] for x in range(len(R))]\n",
        "    FV = None\n",
        "    Rdict = dict(zip(R, range(len(R))))\n",
        "    first = 1\n",
        "    #pdb.set_trace()\n",
        "    for c in stx[0]:\n",
        "        pp = PPBuilder().build_peptides(c)\n",
        "        if len(pp) == 0:\n",
        "            print (\"Ignored the empty chain encounted in \", stx.get_full_id())\n",
        "            continue\n",
        "        (s, s2r) = getSeqForChain(pp, Rdict)\n",
        "        s = '-' * HWS + s + '-' * HWS\n",
        "        idx = 0\n",
        "        for m in range(HWS, len(s) - HWS):\n",
        "            if s[m] != '-':\n",
        "                w = s[m - HWS:m + HWS + 1]\n",
        "                (pd1, p1) = getPD1Spec(w)\n",
        "                #pdb.set_trace()\n",
        "                fv = np.vstack([np.array(pd1.todense()), np.array(p1.todense())])\n",
        "                if (first):\n",
        "                    first = 0\n",
        "                    #create k based on legnth of the vector\n",
        "                    FV = lil_matrix((fv.shape[0], len(R)))\n",
        "                FV[:, s2r[idx]] = fv\n",
        "                idx = idx + 1\n",
        "    FVF = FV.todense()\n",
        "    #If no features for the whole residue, set to nan\n",
        "    FVF[:, np.nonzero(np.array(np.sum(FVF, 0) == 0).ravel())[0]] = np.nan\n",
        "    FV = lil_matrix(FVF)\n",
        "    #pdb.set_trace()\n",
        "    return FV\n",
        "\n",
        "\n",
        "def getSeqForChain(pp, Rdict):\n",
        "    \"\"\"\n",
        "    For a single chain only!!!\n",
        "    Get the sequence for a chain and the S2Ri associated with non '-' chars in s\n",
        "    Remember len(s) is equal to len(S2Ri) iff there are no dashes in s\n",
        "    \"\"\"\n",
        "    s = []\n",
        "    s.append(pp[0].get_sequence().tostring())\n",
        "    for idx in range(len(pp) - 1):\n",
        "        b = pp[idx][-1].id[1] + 1\n",
        "        e = pp[idx + 1][0].id[1] - 1\n",
        "        d = '-' * (e - b + 1)\n",
        "        s.append(d)\n",
        "        s.append(pp[idx + 1].get_sequence().tostring())\n",
        "    S2Ri = [Rdict[r] for p in pp for r in p]\n",
        "    s = ''.join(s)\n",
        "    return (s, S2Ri)\n",
        "\n",
        "\n",
        "def getPD1Spec(s, param=None):\n",
        "    \"\"\"\n",
        "    Get 1-spectrum representation of s (ignoring '-'')\n",
        "    \"\"\"\n",
        "    dv = np.sqrt(1 / 20.0)\n",
        "    V = np.zeros((len(AA), len(s)), dtype='float64')\n",
        "    for k in range(len(s)):\n",
        "        if s[k] != '-':\n",
        "            try:\n",
        "                V[aaidx[s[k]], k] = V[aaidx[s[k]], k] + 1.0\n",
        "            except KeyError:\n",
        "                pass\n",
        "        else:\n",
        "            for a in aaidx:\n",
        "                if a != '-':\n",
        "                    V[aaidx[a], k] = V[aaidx[a], k] + dv\n",
        "    # pdb.set_trace()\n",
        "    #m=np.sqrt(len(s))\n",
        "    v = V.reshape((np.prod(V.shape), 1))\n",
        "    v = lil_matrix(v / np.linalg.norm(v))\n",
        "    v1 = V.sum(axis=1)\n",
        "    v1 = lil_matrix(v1 / np.linalg.norm(v1)).T\n",
        "    return (v, v1)\n",
        "\n",
        "\n",
        "def mergePDBFiles(fnames, ofname):\n",
        "    ofh = open(ofname, 'w')\n",
        "    for f in fnames:\n",
        "        fh = open(f, 'r')\n",
        "        d = fh.read()\n",
        "        fh.close()\n",
        "        ofh.write(d)\n",
        "    ofh.close()\n",
        "\n",
        "\n",
        "def copy_dict(d, *keys):\n",
        "    \"\"\"Make a copy of only the `keys` from dictionary `d`.\"\"\"\n",
        "    return {key: d[key] for key in keys}\n",
        "\n",
        "\n",
        "def chunks(l, n):\n",
        "    \"\"\" Yield successive n-sized chunks from list l, returns list of lists.\"\"\"\n",
        "    for i in xrange(0, len(l), n):\n",
        "        yield l[i:i + n]\n",
        "\n",
        "\n",
        "def mergeDicts(Dlist):\n",
        "    \"\"\" merges a list of dictionaries into a single dictionary\"\"\"\n",
        "    dld = {}\n",
        "    for d in Dlist: dld.update(d)\n",
        "    return dld\n",
        "\n",
        "\n",
        "def combineList(l):\n",
        "    return [item for sublist in l for item in sublist]\n",
        "import pandas as pd\n",
        "from scipy import spatial\n",
        "import numpy as np\n",
        "from Bio.Data import IUPACData\n",
        "from Bio.PDB.Polypeptide import *\n",
        "from os import listdir\n",
        "#import torch\n",
        "import pdb\n",
        "import csv\n",
        "import json\n",
        "import pickle\n",
        "from tqdm import tqdm as tqdm\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import DataStructs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm as tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score\n",
        "from Bio import SeqIO\n",
        "from Bio.SeqIO import FastaIO\n",
        "from itertools import product\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from sklearn.preprocessing import normalize\n",
        "import math\n",
        "import pickle\n",
        "def External_GenerateRandomNegativeandBinders(Pdict,posexamples):\n",
        "    NegtiveRatio=1\n",
        "    #path='D:/PhD/Inhibitor/InhibitorNewModel2022/'\n",
        "    path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "    ###SuperDrugbank\n",
        "    ###Names\n",
        "    SuperdrugNames=pd.read_excel(path+'Inhibitor Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#'approved_drugs_chemical_structure_identifiers.xlsx'\n",
        "    SuperdrugNames=SuperdrugNames[1:]\n",
        "    SuperdrugNames = np.array([s[0] for s in SuperdrugNames])\n",
        "\n",
        "    df_Superdrug=pd.read_excel(path+'Inhibitor Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"C\").values\n",
        "    df_Superdrug=df_Superdrug[1:]\n",
        "    ###\n",
        "    df_Superdrug_Compounds=np.array([c[0] for c in df_Superdrug])#3638\n",
        "    SuperDrug_dict=dict (zip (SuperdrugNames,df_Superdrug_Compounds))\n",
        "    ################\n",
        "    DBD5_ProteinData_dict=pickle.load(open(path+'NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "    Ubench5CompNames=list (set (list (DBD5_ProteinData_dict.keys())))\n",
        "    ####\n",
        "    Filename=path+'2p2i unique inhibitors.xlsx'\n",
        "    df=pd.read_excel(Filename)\n",
        "    Name1='Inhibitor name';Name2=' Inhibitor SMILES'\n",
        "    Lsetname=df[Name1].values;Inhibitor_SMILES=df[Name2].values;\n",
        "    Ldict=dict(zip(Lsetname,Inhibitor_SMILES))\n",
        "    ########\n",
        "    AllNeg=[];AllPos=[];\n",
        "    complex_ligand_dict={}\n",
        "    for i in  range(len(posexamples)):\n",
        "      complexname,inhibitor=posexamples[i]\n",
        "      if  getFP(Ldict[inhibitor]) is not None:\n",
        "          if complexname not in complex_ligand_dict:\n",
        "              complex_ligand_dict[complexname]=inhibitor#inhibitor for NAME of the inhibitor, Ldict[inhibitor] for smiles\n",
        "          else:\n",
        "            #print(\"else\",key,val)\n",
        "            complex_ligand_dict[complexname]=np.append( complex_ligand_dict.get(complexname, ()) ,inhibitor)#stores names of the inhibitors\n",
        "#    Complexnames=list (complex_ligand_dict.keys())\n",
        "    ##############\n",
        "    totalcomp=list (set (complex_ligand_dict.keys()))\n",
        "    totalligands_train=[]\n",
        "    for t in  totalcomp:\n",
        "        totalligands_train.extend(complex_ligand_dict[t])\n",
        "    totalligands_train=list (set (totalligands_train))\n",
        "    \"\"\"\n",
        "    Complex,Csr (Compound random from superDrugbank2)\n",
        "    \"\"\"\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        origanlL=[getFP(origanlL[t]) is not None for t in range(len(origanlL))]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):# and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            LigandR = random.choice(SuperdrugNames)\n",
        "            Npair=((everycomp,LigandR))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(SuperDrug_dict[LigandR]) is not None:\n",
        "                if LigandR==' ' or LigandR==',':\n",
        "                    1/0\n",
        "                negs.append(Npair)\n",
        "#                print(\"Npair SuperdrugNames\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "#        AllPos.extend(pos)\n",
        "        #####\n",
        "#    pdb.set_trace()\n",
        "#    print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "#    print(\"second method Cr\")\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        origanlL=[getFP(origanlL[t]) is not None for t in range(len(origanlL))]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN) and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            if len(totalcomp)>1:\n",
        "                LigandR = random.choice(totalligands_train)#Lsetname)#\n",
        "            else:\n",
        "                LigandR = random.choice(Lsetname)\n",
        "            Npair=((everycomp,LigandR))#LigandR))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(Ldict[LigandR]) is not None:\n",
        "                if LigandR==' ' or LigandR==',':\n",
        "                    1/0\n",
        "                negs.append(Npair)\n",
        "#                print(\"Npair\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "        AllPos.extend(pos)\n",
        "#        print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "        ###\n",
        "        #possiblecomp=list(set (np.array([(Allexamples[t][0]) for t in posexamples])))\n",
        "        #possiblecomp.remove(everycomp)\n",
        "#        print(\"possiblecomp\",possiblecomp)\n",
        "     #####\n",
        "#    pdb.set_trace()\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        origanlL=[getFP(origanlL[t]) is not None for t in range(len(origanlL))]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):\n",
        "            for everyL in origanlL:\n",
        "                ComplexR = random.choice(Ubench5CompNames)\n",
        "                Npair=((ComplexR,everyL))#everyL))\n",
        "                if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "    #                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                    negs.append(Npair)\n",
        "\n",
        "        AllNeg.extend(negs)\n",
        "        ###################\n",
        "#    print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "     #####\n",
        "#    pdb.set_trace()\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        origanlL=[getFP(origanlL[t]) is not None for t in range(len(origanlL))]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        for everyL in origanlL:\n",
        "            if len(totalcomp)>1:\n",
        "                #print(\"everyL\",everyL)\n",
        "                ComplexR = random.choice(totalcomp)#Complexnames)#\n",
        "            else:\n",
        "                ComplexR = random.choice(Complexnames)#\n",
        "            Npair=((ComplexR,everyL))\n",
        "            if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos :\n",
        "#                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                negs.append(Npair)\n",
        "        AllNeg.extend(negs)\n",
        "##    print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "#    fields=['Complexname','Binders SMILES']\n",
        "#    df=pd.read_csv(path+'Inhibitor Data/BindersWithComplexname.csv', skipinitialspace=True, usecols=fields)\n",
        "#    # See the keys\n",
        "#    #print(df.keys())\n",
        "#    neg_Pidname,neg_smiles=df[df.keys()[0]].values,df[df.keys()[1]].values\n",
        "#    \"\"\"\n",
        "#    New_neg_Pidname,New_neg_smiles=[],[]\n",
        "#    for s in range(len(neg_smiles)):\n",
        "#        if getFP(neg_smiles[s]) is None:\n",
        "#            1/0\n",
        "#            New_neg_Pidname.append(neg_Pidname[s]),New_neg_smiles.append(neg_smiles[s])\n",
        "#     \"\"\"\n",
        "##    neg_Pid=[n for n in neg_Pidname]#n.split('_')[0]\n",
        "##    Binders_dict=AppendlistinDict(neg_Pid,neg_smiles)\n",
        "#    Binders_dict=AppendlistinDict(neg_Pidname,neg_smiles)#for svm pairwise\n",
        "##    Binders_dict=AppendlistinDict(New_neg_Pidname,New_neg_smiles)\n",
        "#    Pset = list(set(neg_smiles)) #set of protein sequences#New_\n",
        "#    pidx = list(range(len(Pset)))\n",
        "#    Pdict = dict(zip(Pset, pidx)) #seq->index\n",
        "    ####\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        origanlL=[getFP(origanlL[t]) is not None for t in range(len(origanlL))]\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        if everycomp in Binders_dict:\n",
        "#            Binders=[(getFP(t) is not None for t in Binders_dict[everycomp])]\n",
        "            Binders=Binders_dict[everycomp]\n",
        "            BindersIds=[]\n",
        "            for t in Binders:\n",
        "#                print(t,Pdict[t])\n",
        "                BindersIds.append(Pdict[t])\n",
        "            Binders_neg=[(everycomp,BindersIds[t]) for t in range(len(Binders))]\n",
        "            AllNeg.extend(Binders_neg)\n",
        "            ############################\n",
        "#            print(\"everycomp\",everycomp,\"N Binders=\",len(AllNeg),\"P\",len(AllPos))\n",
        "            ###############\n",
        "#    pdb.set_trace()\n",
        "    #for every binder pick compR from 2p2i\n",
        "#    for everycomp in totalcomp:\n",
        "#        origanlL=complex_ligand_dict[everycomp]\n",
        "##        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "#        Binders=Binders_dict[everycomp]\n",
        "#        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "#        NN =NegtiveRatio*len(pos)\n",
        "#        negs = []\n",
        "#        while (len(negs)<NN):\n",
        "#            for everyB in Binders:\n",
        "#                if getFP(everyB) is not None:\n",
        "#                    if len(totalcomp)>1:\n",
        "#                        #print(\"everyL\",everyL)\n",
        "#                        ComplexR = random.choice(totalcomp)#Complexnames)#\n",
        "#                    else:\n",
        "#                        ComplexR = random.choice(Complexnames)#\n",
        "#                    Npair=((ComplexR,everyB))\n",
        "#                    if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "#        #                    print(\"ComplexR,everycomp)\",Npair)\n",
        "#                        negs.append(Npair)\n",
        "#        AllNeg.extend(negs)\n",
        "#    print(\" binder with 2p2i N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    ################\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        origanlL=[getFP(origanlL[t]) is not None for t in range(len(origanlL))]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        Binders=Binders_dict[everycomp]\n",
        "        BindersIds=[]\n",
        "        for t in Binders:\n",
        "#            print(t,Pdict[t])\n",
        "            if Pdict[t]==' ' or Pdict[t]==',':\n",
        "                    1/0\n",
        "            BindersIds.append(Pdict[t])\n",
        "#        BindersIds=[(Pdict[t] for t in Binders)]\n",
        "#        Binders_neg=[(everycomp,BindersIds[t]) for t in range(len(Binders))]\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):\n",
        "#            everyB = random.choice(Binders)\n",
        "            everyB = random.choice(BindersIds)\n",
        "            ComplexR = random.choice(Ubench5CompNames)\n",
        "            Npair=((ComplexR,everyB))\n",
        "            if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "#                print(\"everyB\",Npair)\n",
        "                negs.append(Npair)\n",
        "        AllNeg.extend(negs)\n",
        "#    print(\"binder with dbd5 N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    return np.array(AllPos),np.array(AllNeg)\n",
        "def AppendlistinDict(list1,list2):\n",
        "    ####\n",
        "    from collections import defaultdict\n",
        "    merge_dict=defaultdict(list);#protein_binders_Namesdict=defaultdict(list);\n",
        "    # For every complex, save its actual binders\n",
        "    for (key, value) in zip(list1,list2):#ProteinSeqBinder, Binders_SMILES):#,Binders_names):,Bname)\n",
        "        if key not in  merge_dict:\n",
        "            merge_dict[key]=[str(value)]\n",
        "        else:\n",
        "            merge_dict[key]=np.append( merge_dict.get(key, ()) , value)\n",
        "    return merge_dict\n",
        "def PredictScorefromFileSVM(filename,Pdbloc,trainedModel_IPPI,train_GNN):\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  InhibitedComp=[];PdbId=[];Ligandnames=[];SMILES=[];labels=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      #if len(d)==6:\n",
        "      Pdbid,inhibtedc,Ligandid,smiles = d.split()\n",
        "      PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);InhibitedComp.append(inhibtedc);#labels.append(float (y));\n",
        "  ################\n",
        "  ################\n",
        "  pdbname=listdir(Pdbloc)\n",
        "  mypdb=[];External_ProteinData_dict={}\n",
        "  for p in pdbname:\n",
        "    if p.split('.pdb')[0] in Pdbid:\n",
        "      mypdb.append(p)\n",
        "  UniqueProtein=list (set (mypdb))\n",
        "  for uniqe in UniqueProtein:\n",
        "      chains=Struct2chain(Pdbloc+'/'+uniqe)\n",
        "      Cname_L,seqL,R_L,xl_L=chains[0]\n",
        "        #############\n",
        "      Cname_R,seqR,R_R,xl_R=chains[1]\n",
        "        #####\n",
        "      LcF=prot_feats_seq(seqL)\n",
        "      RcF=prot_feats_seq(seqR)\n",
        "      Complex_AllFeatures=(LcF+RcF)/2#np.hstack((\n",
        "      External_ProteinData_dict[uniqe]=Complex_AllFeatures\n",
        "  Result_dict={}\n",
        "  pos=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "  Pos,Negs,SuperDrug_dict=External_GenerateRandomNegative(pos)\n",
        "  poslabel=1.0*np.ones(len(Pos));neglabel=-1.0*np.ones(len(Negs))\n",
        "  labels=np.append(poslabel,neglabel )\n",
        "  All_examples=[];All_examples.extend(Pos);All_examples.extend(Negs)\n",
        "  #All_examples=np.hstack((Pos,Negs))\n",
        "  #Testing\n",
        "  DBD5_ProteinData_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "  #Pos_ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "  All_ProteinData_dict=dict( list (External_ProteinData_dict.items())+list (DBD5_ProteinData_dict.items()))\n",
        "  for d in All_ProteinData_dict:\n",
        "    data=All_ProteinData_dict[d]\n",
        "    All_ProteinData_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "  Y_t,Z=[],[]\n",
        "  for complexname,ligandsmile in All_examples:\n",
        "    #complexname,Ligandname, ligandsmile,inhibitC =PdbId[nt], Ligandnames[nt], SMILES[nt],InhibitedComp[nt]\n",
        "    test_score=trainedModel_IPPI(train_GNN,All_ProteinData_dict[complexname],complexname,ligandsmile)\n",
        "    if test_score!=0.0:\n",
        "        test_score=test_score.cpu().data.numpy()[0]\n",
        "        Z.append(test_score)\n",
        "        Result_dict[(complexname,Ligandname)]=test_score\n",
        "  return Result_dict,Z,labels\n",
        "def twomerFromSeq(s):\n",
        "    k=2\n",
        "    groups={'A':'1','V':'1','G':'1','I':'2','L':'2','F':'2','P':'2','Y':'3',\n",
        "            'M':'3','T':'3','S':'3','H':'4','N':'4','Q':'4','W':'4',\n",
        "            'R':'5','K':'5','D':'6','E':'6','C':'7'}\n",
        "    crossproduct=[''.join (i) for i in product(\"1234567\",repeat=k)]\n",
        "    for i in range (0,len(crossproduct)): crossproduct[i]=int(crossproduct[i])\n",
        "    ind=[]\n",
        "    for i in range (0,len(crossproduct)): ind.append(i)\n",
        "    combinations=dict(zip(crossproduct,ind))\n",
        "\n",
        "    V=np.zeros(int((math.pow(7,k))))      #defines a vector of 343 length with zero entries\n",
        "    try:\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                c+=groups[kmer[l]]\n",
        "                V[combinations[int(c)]]+=1\n",
        "    except:\n",
        "        count={'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0}\n",
        "        for q in range(0,len(s)):\n",
        "            if s[q]=='A' or s[q]=='V' or s[q]=='G':\n",
        "                count['1']+=1\n",
        "            if s[q]=='I' or s[q]=='L'or s[q]=='F' or s[q]=='P':\n",
        "                count['2']+=1\n",
        "            if s[q]=='Y' or s[q]=='M'or s[q]=='T' or s[q]=='S':\n",
        "                count['3']+=1\n",
        "            if s[q]=='H' or s[q]=='N'or s[q]=='Q' or s[q]=='W':\n",
        "                count['4']+=1\n",
        "            if s[q]=='R' or s[q]=='K':\n",
        "                count['5']+=1\n",
        "            if s[q]=='D' or s[q]=='E':\n",
        "                count['6']+=1\n",
        "            if s[q]=='C':\n",
        "                count['7']+=1\n",
        "        val=list(count.values()  )           #[ 0,0,0,0,0,0,0]\n",
        "        key=list(count.keys()     )           #['1', '2', '3', '4', '5', '6', '7']\n",
        "        m=0\n",
        "        ind=0\n",
        "        for t in range(0,len(val)):     #find maximum value from val\n",
        "            if m<val[t]:\n",
        "                m=val[t]\n",
        "                ind=t\n",
        "        m=key [ind]                     # m=group number of maximum occuring group alphabets in protein\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                if kmer[l] not in groups:\n",
        "                    c+=m\n",
        "                else:\n",
        "                    c+=groups[kmer[l]]\n",
        "            V[combinations[int(c)]]+=1\n",
        "\n",
        "    V=V/(len(s)-1)\n",
        "    return np.array(V)\n",
        "def threemerFromSeq(s):\n",
        "    k=3\n",
        "    groups={'A':'1','V':'1','G':'1','I':'2','L':'2','F':'2','P':'2','Y':'3',\n",
        "            'M':'3','T':'3','S':'3','H':'4','N':'4','Q':'4','W':'4',\n",
        "            'R':'5','K':'5','D':'6','E':'6','C':'7'}\n",
        "    crossproduct=[''.join (i) for i in product(\"1234567\",repeat=k)]\n",
        "    for i in range (0,len(crossproduct)): crossproduct[i]=int(crossproduct[i])\n",
        "    ind=[]\n",
        "    for i in range (0,len(crossproduct)): ind.append(i)\n",
        "    combinations=dict(zip(crossproduct,ind))\n",
        "\n",
        "    V=np.zeros(int((math.pow(7,k))))      #defines a vector of 343 length with zero entries\n",
        "    try:\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                c+=groups[kmer[l]]\n",
        "                V[combinations[int(c)]]+=1\n",
        "    except:\n",
        "        count={'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0}\n",
        "        for q in range(0,len(s)):\n",
        "            if s[q]=='A' or s[q]=='V' or s[q]=='G':\n",
        "                count['1']+=1\n",
        "            if s[q]=='I' or s[q]=='L'or s[q]=='F' or s[q]=='P':\n",
        "                count['2']+=1\n",
        "            if s[q]=='Y' or s[q]=='M'or s[q]=='T' or s[q]=='S':\n",
        "                count['3']+=1\n",
        "            if s[q]=='H' or s[q]=='N'or s[q]=='Q' or s[q]=='W':\n",
        "                count['4']+=1\n",
        "            if s[q]=='R' or s[q]=='K':\n",
        "                count['5']+=1\n",
        "            if s[q]=='D' or s[q]=='E':\n",
        "                count['6']+=1\n",
        "            if s[q]=='C':\n",
        "                count['7']+=1\n",
        "        val=list(count.values())              #[ 0,0,0,0,0,0,0]\n",
        "        key=list(count.keys() )              #['1', '2', '3', '4', '5', '6', '7']\n",
        "        m=0\n",
        "        ind=0\n",
        "        for t in range(0,len(val)):     #find maximum value from val\n",
        "            if m<val[t]:\n",
        "                m=val[t]\n",
        "                ind=t\n",
        "        m=key [ind]                     # m=group number of maximum occuring group alphabets in protein\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                if kmer[l] not in groups:\n",
        "                    c+=m\n",
        "                else:\n",
        "                    c+=groups[kmer[l]]\n",
        "            V[combinations[int(c)]]+=1\n",
        "\n",
        "    V=V/(len(s)-1)\n",
        "    return np.array(V)\n",
        "def prot_feats_seq(seq):\n",
        "    #Interfacedict=pickle.load(open(path+\"InhibitorNewModel2022/InterfaceFeatures2chainsSVM.npy\",\"rb\"))\n",
        "    #InterfaceF=Interfacedict[complexname]\n",
        "    aa=['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "    f=[]\n",
        "    X = ProteinAnalysis(str(seq))\n",
        "    X.molecular_weight() #throws an error if 'X' in sequence. we skip such sequences\n",
        "    p=X.get_amino_acids_percent()\n",
        "    dp=[]\n",
        "    for a in aa:\n",
        "        dp.append(p[a])\n",
        "    dp=np.array(dp)\n",
        "    dp=normalize(np.atleast_2d(dp), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "    f.extend(dp[0])\n",
        "\n",
        "    tm=np.array(twomerFromSeq(str(seq)))\n",
        "    tm=normalize(np.atleast_2d(tm), norm='l2', copy=True, axis=1,return_norm=False)\n",
        "    f.extend(tm[0])\n",
        "#    #####\n",
        "#    thm=np.array(threemerFromSeq(str(seq)))\n",
        "#    thm=normalize(np.atleast_2d(thm), norm='l2', copy=True, axis=1,return_norm=False)\n",
        "#    f.extend(thm[0])\n",
        "    ###\n",
        "    return np.array(f)\n",
        "#New start\n",
        "\n",
        "def GenerateRandomNegative(posexamples,Allexamples,complex_ligand_dict,Ldict):\n",
        "    NegtiveRatio=1\n",
        "\n",
        "    ###SuperDrugbank\n",
        "    ###Names\n",
        "    SuperdrugNames=pd.read_excel('D:/PhD/Inhibitor/InhibitorNewModel2022/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#'approved_drugs_chemical_structure_identifiers.xlsx'\n",
        "    SuperdrugNames=SuperdrugNames[1:]\n",
        "    SuperdrugNames = np.array([s[0] for s in SuperdrugNames])\n",
        "    ###############\n",
        "    Filename='D:/PhD/Inhibitor/InhibitorNewModel2022/2p2i unique inhibitors.xlsx'\n",
        "    df=pd.read_excel(Filename)\n",
        "    Name1='Inhibitor name';Name2=' Inhibitor SMILES'\n",
        "    Lsetname=df[Name1].values;Inhibitor_SMILES=df[Name2].values;\n",
        "    ##########\n",
        "    totalcomp=Complexnames\n",
        "    ###########Binders\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        AllPos.extend(pos)\n",
        "        if everycomp in Binders_dict:\n",
        "            Binders=Binders_dict[everycomp]\n",
        "            Binders_neg=[(everycomp,Binders[t]) for t in range(len(Binders))]\n",
        "            AllNeg.extend(Binders_neg)\n",
        "    return np.array(AllPos),np.array(AllNeg)\n",
        "#    c=dict(zip(Lsetname,Inhibitor_SMILES))\n",
        "    ########benchmark5.5\n",
        "    benchmark5=listdir(\"D:/PhD/Inhibitor/InhibitorNewModel2022/benchmark5.5/structures/\")\n",
        "    bench5CompNames=[]\n",
        "\n",
        "    for b in benchmark5:\n",
        "        n=b.split('_')\n",
        "        if len(n)==3:\n",
        "            bench5CompNames.append(n[0])\n",
        "    Ubench5CompNames=list (set (bench5CompNames))\n",
        "    ####\n",
        "    Complexnames=list (complex_ligand_dict.keys())\n",
        "    AllNeg=[];AllPos=[];\n",
        "    totalcomp=list(set (np.array([(Allexamples[t][0]) for t in posexamples])))\n",
        "    totalligands_train=[]\n",
        "    for t in  totalcomp:\n",
        "        totalligands_train.extend(complex_ligand_dict[t])\n",
        "    totalligands_train=list (set (totalligands_train))\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):# and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            LigandR = random.choice(SuperdrugNames)\n",
        "            Npair=((everycomp,LigandR))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(Ldict[LigandR]) is not None:\n",
        "                negs.append(Npair)\n",
        "#                print(\"Npair SuperdrugNames\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "#        AllPos.extend(pos)\n",
        "        #####\n",
        "#    pdb.set_trace()\n",
        "#    print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "#    print(\"second method Cr\")\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN) and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            if len(totalcomp)>1:\n",
        "                LigandR = random.choice(totalligands_train)#Lsetname)#\n",
        "            else:\n",
        "                LigandR = random.choice(Lsetname)\n",
        "            Npair=((everycomp,LigandR))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(Ldict[LigandR]) is not None:\n",
        "                negs.append(Npair)\n",
        "#                print(\"Npair\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "        AllPos.extend(pos)\n",
        "#    print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "        ###\n",
        "        #possiblecomp=list(set (np.array([(Allexamples[t][0]) for t in posexamples])))\n",
        "        #possiblecomp.remove(everycomp)\n",
        "#        print(\"possiblecomp\",possiblecomp)\n",
        "     #####\n",
        "#    pdb.set_trace()\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):\n",
        "            for everyL in origanlL:\n",
        "                ComplexR = random.choice(Ubench5CompNames)\n",
        "                Npair=((ComplexR,everyL))\n",
        "                if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "    #                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                    negs.append(Npair)\n",
        "\n",
        "        AllNeg.extend(negs)\n",
        "        ###################\n",
        "#    print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "     #####\n",
        "#    pdb.set_trace()\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        for everyL in origanlL:\n",
        "            if len(totalcomp)>1:\n",
        "                #print(\"everyL\",everyL)\n",
        "                ComplexR = random.choice(totalcomp)#Complexnames)#\n",
        "            else:\n",
        "                ComplexR = random.choice(Complexnames)#\n",
        "            Npair=((ComplexR,everyL))\n",
        "            if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "#                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                negs.append(Npair)\n",
        "        AllNeg.extend(negs)\n",
        "#    print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    return np.array(AllPos),np.array(AllNeg)\n",
        "\n",
        "def getFP(s,r = 2,nBits =2048):\n",
        "    compound = Chem.MolFromSmiles(s.strip())\n",
        "    if compound is not None:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(compound, r, nBits = nBits)\n",
        "        #fp = pat.GetAvalonCountFP(compound,nBits=nBits)\n",
        "        m = np.zeros((0, ), dtype=np.int8)\n",
        "        DataStructs.ConvertToNumpyArray(fp, m)\n",
        "        return m\n",
        "def make_dic():\n",
        "    prot_dic={}\n",
        "    letters=IUPACData.protein_letters\n",
        "    for i in range(len(letters)):\n",
        "        for j in range(i,len(letters)):\n",
        "            prot_dic[(letters[i],letters[j])]=0.0\n",
        "    prot_dic[('_','_')]=0.0# for Amino acids other than 20 natural\n",
        "    return prot_dic\n",
        "def extract_feats(dic):\n",
        "    feats=[]\n",
        "    key_list=np.load('prote_letter_pair_keys.npy')#to keep features order same\n",
        "    for key in key_list:\n",
        "#        pdb.set_trace()\n",
        "        feats.append(dic[(key[0].decode('utf-8'),key[1].decode('utf-8'))])\n",
        "\n",
        "    return feats\n",
        "def generate_pair_features(dist_info,xl,xr):\n",
        "    prot_dic=make_dic()\n",
        "#    pdb.set_trace()\n",
        "    for rec in dist_info:\n",
        "\n",
        "        try:\n",
        "            l_letter= three_to_one(xl[rec[0]].get_resname())\n",
        "            r_letter= three_to_one(xr[rec[1]].get_resname())\n",
        "#            print(l_letter,l_letter)\n",
        "            if (l_letter,r_letter) in prot_dic.keys():\n",
        "                prot_dic[(l_letter,r_letter)]+=1\n",
        "            elif (r_letter,l_letter) in prot_dic.keys():\n",
        "                prot_dic[(r_letter,l_letter)]+=1\n",
        "        except:\n",
        "            prot_dic[('_','_')]+=1\n",
        "    return prot_dic\n",
        "def getDist(C0, C1, thr=np.inf):\n",
        "    N0 = []\n",
        "    for i in range(len(C0)):\n",
        "        for j in range(len(C1)):\n",
        "            d = spatial.distance.cdist(C0[i], C1[j]).min()\n",
        "            if (d < thr):  # and not np.isnan(self.Phi[i]) and not np.isnan(self.Phi[j])\n",
        "                N0.append((i, j, d))\n",
        "    return N0\n",
        "def Struct2chain(stx):\n",
        "    \"\"\"\n",
        "    Seq: sequence of the chain\n",
        "    seq_L:sequence Length\n",
        "    \"\"\"\n",
        "    p = PDBParser()\n",
        "    L=[]\n",
        "    stx=p.get_structure('X',stx)\n",
        "    for model in stx:\n",
        "        for C in model:\n",
        "            RL=[]\n",
        "            for R in C:\n",
        "                RL.append(R)\n",
        "            pp=PPBuilder().build_peptides(C)\n",
        "            if len(pp)==0:\n",
        "                pp=CaPPBuilder().build_peptides(C)\n",
        "            seq=''.join([str(p.get_sequence()) for p in pp])\n",
        "            #seq=''.join([p.get_sequence().tostring() for p in pp])\n",
        "            seq_L=len(seq)\n",
        "            L.append((C.full_id[2],seq,seq_L,RL))\n",
        "    return L\n",
        "def chainLabel(Cname_T,xl_T,Cname,xl):\n",
        "    \"\"\"\n",
        "    Cname_T: Target chain Name\n",
        "    xl_T: Target chain co-ordinates\n",
        "    Cname: Off Target chain Name\n",
        "    xl: Off Target chain co-ordinates\n",
        "    \"\"\"\n",
        "    tc = getCoords(xl_T)\n",
        "    nc = getCoords(xl)\n",
        "    D = getDist(tc, nc, thr = 8.0)\n",
        "    feats=extract_feats(generate_pair_features(D,xl_T,xl))\n",
        "    return feats\n",
        "def InterfaceFeatures(Complexs,pdbloc):\n",
        "    Found =  listdir(pdbloc)\n",
        "    InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "    comp_id=list(set(Complexs))\n",
        "    for ids in range(len(comp_id)):\n",
        "        if comp_id[ids]+'.pdb' in Found:\n",
        "            stx=pdbloc+'/'+comp_id[ids]+'.pdb'#'/2XA0.pdb'\n",
        "            chains=Struct2chain(stx)\n",
        "            for j in range(len(chains)):\n",
        "                Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "                for k in range(j,len(chains)):\n",
        "                    Cname,seq,L,xl=chains[k]\n",
        "                    #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                    name=comp_id[ids]#+'_'+Cname_T+'_2_'+Cname\n",
        "                    Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                    InterfaceF=np.array(Interface)\n",
        "                    InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                    if name not in InterfaceFeatures.keys():\n",
        "                        InterfaceFeatures[name]=Interface\n",
        "    #pickle.dump(InterfaceFeatures, open(path+Filename+\"_InterfaceFeatures.npy\", \"wb\"))\n",
        "    return InterfaceFeatures\n",
        "def LoadProtein_SVM_Features(UniqueProtein,Pdbloc):\n",
        "    pdbname=listdir(Pdbloc)\n",
        "    InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "    SequenceFeatures=[];SequenceFeatures=dict(SequenceFeatures)\n",
        "    AllFeatures=[];AllFeatures=dict(AllFeatures)\n",
        "    for  b in range(len(UniqueProtein)):\n",
        "        if UniqueProtein[b]+'.pdb'in pdbname:\n",
        "            stx=Pdbloc+UniqueProtein[b]+'.pdb'#directory+'/2XA0.pdb'#\n",
        "            chains=Struct2chain(stx)\n",
        "            #########Interface Features\n",
        "            for j in range(len(chains)):\n",
        "                Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "                for k in range(j,len(chains)):\n",
        "                    Cname,seq,L,xl=chains[k]\n",
        "                    #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                    name=UniqueProtein[b]#+'_'+Cname_T+'_2_'+Cname\n",
        "                    Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                    seq_TF=prot_feats_seq(seq_T)\n",
        "                    seq_NTF=prot_feats_seq(seq)\n",
        "                    SeQFeatures=(seq_TF+seq_NTF)/2\n",
        "                    InterfaceF=np.array(Interface)\n",
        "                    InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                    if name not in InterfaceFeatures.keys():\n",
        "                        InterfaceFeatures[name]=Interface\n",
        "                        SequenceFeatures[name]=SeQFeatures\n",
        "                        AllFeatures[name]=np.append(SeQFeatures,Interface)\n",
        "    return InterfaceFeatures,SequenceFeatures,AllFeatures\n",
        "def External_GenerateRandomNegative(path,posexamples):\n",
        "    NegtiveRatio=1\n",
        "    ###SuperDrugbank\n",
        "    ###Names\n",
        "    SuperdrugNames=pd.read_excel(path+'Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#'approved_drugs_chemical_structure_identifiers.xlsx'\n",
        "    SuperdrugNames=SuperdrugNames[1:]\n",
        "    SuperdrugNames = np.array([s[0] for s in SuperdrugNames])\n",
        "    ###############\n",
        "    ###SuperDrugbank\n",
        "    df_Superdrug=pd.read_excel(path+'Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"C\").values\n",
        "    df_Superdrug=df_Superdrug[1:]\n",
        "    ###\n",
        "    df_Superdrug_Compounds=np.array([c[0] for c in df_Superdrug])#3638\n",
        "    SuperDrug_dict=dict (zip (SuperdrugNames,df_Superdrug_Compounds))\n",
        "    ################\n",
        "    #path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "    DBD5_ProteinData_dict=pickle.load(open(path+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "    Ubench5CompNames=list (set (list (DBD5_ProteinData_dict.keys())))\n",
        "    ####\n",
        "    AllNeg=[];AllPos=[];\n",
        "    complex_ligand_dict={}\n",
        "    for key,val in  posexamples:\n",
        "      #print(key,val,posexamples[key,val][1])\n",
        "      if key not in complex_ligand_dict:\n",
        "        complex_ligand_dict[key]=posexamples[key,val][1]\n",
        "      else:\n",
        "        #print(\"else\",key,val)\n",
        "        complex_ligand_dict[key]=np.append( complex_ligand_dict.get(key, ()) ,posexamples[key,val][1])\n",
        "    Complexnames=list (complex_ligand_dict.keys())\n",
        "    totalcomp=list (set (complex_ligand_dict.keys()))\n",
        "    for everycomp in totalcomp:\n",
        "        pos=[]\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "        #print(origanlL)\n",
        "        #print(\"complexname=\",everycomp,\"origanlInhibitors\",len(origanlL))\n",
        "        for t in range(len(origanlL)):\n",
        "            if getFP(origanlL[t]) is not None:\n",
        "                pos.append((everycomp,origanlL[t]))\n",
        "        #print(pos)\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        AllPos.extend(pos)\n",
        "        while (len(negs)<NN):# and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            LigandR = random.choice(SuperdrugNames)\n",
        "            LigandR_smile=SuperDrug_dict[LigandR]\n",
        "            Npair=((everycomp,LigandR_smile))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(LigandR_smile) is not None:\n",
        "                negs.append(Npair)\n",
        "#                print(\"Npair SuperdrugNames\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "    #print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    #print(\"second method Cr\")\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):\n",
        "            for everyL in origanlL:\n",
        "                ComplexR = random.choice(Ubench5CompNames)\n",
        "                Npair=((ComplexR,everyL))\n",
        "                if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos and getFP(everyL) is not None:\n",
        "    #                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                    negs.append(Npair)\n",
        "        AllNeg.extend(negs)\n",
        "        ###################\n",
        "    #print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    return np.array(AllPos),np.array(AllNeg),SuperDrug_dict\n",
        "def PredictScorefromFileSVM(path,filename,Pdbloc,trainedModel_SVM,Ptr,Ctr,Pscaler, Cscaler):\n",
        "#  path,filename,Pdbloc,trainedModel_SVM,Ptr,Ctr,Pscaler, Cscaler=path,path+'/HansonACE2hits.txt',path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler\n",
        "#  path,path+'/2dhy_all_pos.txt',path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  InhibitedComp=[];PdbId=[];Ligandnames=[];SMILES=[];labels=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      #if len(d)==6:\n",
        "      Pdbid,inhibtedc,Ligandid,smiles = d.split()\n",
        "      PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);InhibitedComp.append(inhibtedc);#labels.append(float (y));\n",
        "  ################\n",
        "  #path='D:/PhD/Inhibitor/InhibitorNewModel2022/'\n",
        "  #githubpath='/content/PPI-Inhibitors/'\n",
        "  ################\n",
        "  pdbname=listdir(Pdbloc)\n",
        "  mypdb=[]\n",
        "  for p in pdbname:\n",
        "    p=p.split('.pdb')[0]\n",
        "    if  p in Pdbid:\n",
        "      mypdb.append(p)\n",
        "  UniqueProtein=list (set (mypdb))\n",
        "  proteindataFilename=filename.split('/')[-1].split('.txt')[0]\n",
        "  #print(proteindataFilename)\n",
        "  #/content/PPI-Inhibitors/Features/2dyh_all_External_ProteinData_dict.txt\n",
        "  External_AllFeatures=pickle.load(open(path+'Features/'+proteindataFilename+'_External_ProteinData_dict.npy',\"rb\"))\n",
        "  labels_All=[];Z=[]\n",
        "  pos=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "\n",
        "  Pos,Negs,SuperDrug_dict=External_GenerateRandomNegative(path,pos)\n",
        "  poslabel=1.0*np.ones(len(Pos));neglabel=-1.0*np.ones(len(Negs))\n",
        "  labels=np.append(poslabel,neglabel )\n",
        "  All_examples=[];All_examples.extend(Pos);All_examples.extend(Negs)\n",
        "  #Testing\n",
        "  DBD5_SVM_ProteinData_dict=pickle.load(open(path+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "  #Pos_ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "  All_ProteinData_dict=dict( list (External_AllFeatures.items())+list (DBD5_SVM_ProteinData_dict.items()))\n",
        "  Ctt,Ptt=[],[]\n",
        "  for complexname,ligandsmile in All_examples:\n",
        "    if getFP(ligandsmile) is not None:\n",
        "        Ptt.append(All_ProteinData_dict[complexname])\n",
        "        Ctt.append(getFP(ligandsmile))\n",
        "#        labels.append(labels)\n",
        "  ##########\n",
        "  Ptt,Ctt = Pscaler.transform(Ptt), Cscaler.transform(Ctt)\n",
        "  Kp = kernel(Ptt,Ptr)\n",
        "  Kc = kernel(Ctt,Ctr)\n",
        "  Ktt= Kp*Kc\n",
        "  test_score= trainedModel_SVM.decision_function(Ktt)\n",
        "  Z.extend(test_score);labels_All.extend(labels)\n",
        "#  Result_dict=dict (zip( All_examples,Z))\n",
        "  return Z,labels\n",
        "def LoadSuperDrugdata(path):\n",
        "  #path='/content/PPI-Inhibitors/Data/'\n",
        "  import pandas as pd\n",
        "  df_Superdrug=pd.read_excel(path+'Data\\approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"C\").values#index_col=2)\n",
        "  df_Superdrug=df_Superdrug[1:]\n",
        "  ###Names\n",
        "  SuperdrugNames=pd.read_excel(path+'Data\\approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#index_col=2)\n",
        "  SuperdrugNames=SuperdrugNames[1:]\n",
        "  SuperdrugNames=np.array([c[0] for c in SuperdrugNames])\n",
        "  df_Superdrug_Compounds_SMILES=np.array([c[0] for c in df_Superdrug])#3638\n",
        "  #CompoundFeatures = np.array([getFP(s) for s in df_Superdrug_Compounds])\n",
        "  return df_Superdrug_Compounds_SMILES,SuperdrugNames\n",
        "def Calculate_RFPP(z,Targetlabels):\n",
        "  RPP=[]\n",
        "  index_P=np.argsort(z)\n",
        "  n=len(Targetlabels)\n",
        "  z=np.array(z)\n",
        "  sorted_index=index_P[::-1][:n]\n",
        "  sorted_score=z[index_P[::-1][:n]]\n",
        "  sorted_Targetlabels=Targetlabels[sorted_index]\n",
        "  ###\n",
        "  RPP.append(np.where(sorted_Targetlabels==1))\n",
        "  RFPP=np.min(RPP)+1\n",
        "  return RFPP\n",
        "def PredictRFPPfromFile(path,filename,Pdbloc,trainedModel_SVM,Ptr,Ctr,Pscaler, Cscaler):\n",
        "#filename,Pdbloc,trainedModel_IPPI,train_GNN=Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model\n",
        "#  path,filename,Pdbloc,trainedModel_SVM,Ctr,Ptr,Pscaler, Cscaler=path,path+'/2dhy_all_pos.txt',path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  InhibitedComp=[];PdbId=[];Ligandnames=[];SMILES=[];labels=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      #if len(d)==6:\n",
        "      Pdbid,inhibtedc,Ligandid,smiles = d.split()\n",
        "      PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);InhibitedComp.append(inhibtedc);#labels.append(float (y));\n",
        "  ################\n",
        "  ################\n",
        "  pdbname=listdir(Pdbloc)\n",
        "  mypdb=[]\n",
        "  for p in pdbname:\n",
        "    #print (p)\n",
        "    p=str(p).split('.pdb')[0]\n",
        "    if p in Pdbid:\n",
        "      mypdb.append(p)\n",
        "  UniqueProtein=list (set (mypdb))\n",
        "  ##############\n",
        "  proteindataFilename=filename.split('/')[-1].split('.txt')[0]\n",
        "  External_AllFeatures=pickle.load(open(path+proteindataFilename+'_External_ProteinData_dict.npy',\"rb\"))\n",
        "#  if len(filename.split('/')) >1:\n",
        "#      proteindataFilename=filename.split('/')[-1].split('.txt')[0]\n",
        "#  if proteindataFilename+'_External_ProteinData_dict.npy' in path:\n",
        "#      External_AllFeatures=pickle.load(open(path+proteindataFilename+'_External_ProteinData_dict.npy',\"rb\"))\n",
        "#  else:\n",
        "#      print (\"ComputIing protein features\")\n",
        "#      InterfaceFeatures,SequenceFeatures,External_AllFeatures=LoadProtein_SVM_Features(UniqueProtein,Pdbloc)\n",
        "#      pickle.dump(External_AllFeatures,open(path+proteindataFilename+'_External_ProteinData_dict.npy',\"wb\"))\n",
        "##  External_ProteinData_dict=pickle.load(open(path+'2dhy_all_pos_External_ProteinData_dict.npy',\"rb\"))\n",
        "###  External_ProteinData_dict=PrepairDataset.processProtein(UniqueProtein,Pdbloc)\n",
        "##  #Result_dict={}\n",
        "  External_ProteinData_dict=External_AllFeatures\n",
        "#  pos=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "  pos={}\n",
        "  for index in range (len(SMILES)):\n",
        "      pos[(PdbId[index], Ligandnames[index])]=(InhibitedComp[index],SMILES[index])\n",
        "  #########\n",
        "  Superdrug_SMILES,SuperdrugNames=LoadSuperDrugdata(path)\n",
        "  ####P2C_dict is the protein paired with all compounds in positive set\n",
        "  from collections import defaultdict\n",
        "  P2C_dict={}#defaultdict(list)\n",
        "  for (key, val) in pos:\n",
        "    #key,value=Pset[key],Cset[value]\n",
        "    if key not in P2C_dict:\n",
        "      P2C_dict[key]=pos[key,val][1]\n",
        "    else:\n",
        "      P2C_dict[key]=np.append(P2C_dict.get(key, ()) , pos[key,val][1])\n",
        "  ##########loop\n",
        "  \"\"\"\n",
        "  for d in External_ProteinData_dict:\n",
        "    data=External_ProteinData_dict[d]\n",
        "    External_ProteinData_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "  \"\"\"\n",
        "  Y_t,Z,Targrtlabels=[],[],[]\n",
        "  RFPP_all=[];Result_All=[];Targetlabels_All=[];\n",
        "  perntile_values = [0,1,5,10,20,40,50,60,70,80,90,95,99,100]#99\n",
        "  for Pi in P2C_dict:\n",
        "    ####For one example Pi\n",
        "    Pi_Feature=External_ProteinData_dict[Pi]#P features of that index\n",
        "    Actual_Compound=P2C_dict[Pi]#Actul_Compound paired with Pi\n",
        "    #Actual_Compound_Features=np.array([U[c] for c in Actual_Compound])\n",
        "    Cseq=[];Cseq.extend(Actual_Compound);Cseq.extend(Superdrug_SMILES)#Features all unique Compounds of Superdrugbank and truepositive examples\n",
        "    Ptt=[];Ctt=[];#np.array([Pi_Feature for i in range(len(Ctt))])#Copy same feature of protein equal to number of unique compounds\n",
        "    for smile in Cseq:\n",
        "        if getFP(smile)  is not None:\n",
        "            Ctt.append(getFP(smile))\n",
        "            Ptt.append(Pi_Feature)\n",
        "    ########\n",
        "    Names=[];Names.extend(Ligandnames);Names.extend(SuperdrugNames)\n",
        "    #All_examples=dict(zip(Ptt,Ctt))\n",
        "    poslabel=1.0*np.ones(len(Actual_Compound));neglabel=-1.0*np.ones(len(Superdrug_SMILES))\n",
        "    Targetlabels=np.append(poslabel,neglabel )\n",
        "    ##########\n",
        "    Ptt,Ctt = Pscaler.transform(Ptt), Cscaler.transform(Ctt)\n",
        "    Kp = kernel(Ptt,Ptr)\n",
        "    Kc = kernel(Ctt,Ctr)\n",
        "    Ktt= Kp*Kc\n",
        "    Z= trainedModel_SVM.decision_function(Ktt)\n",
        "    RFPP=Calculate_RFPP(Z,Targetlabels)\n",
        "    #print(RFPP)\n",
        "    #Result_dict=dict (zip(All_examples,test_score))\n",
        "    #RFPP=Calculate_RFPP(z_random,Targetlabels)\n",
        "    RFPP_all.append(RFPP)#;Targetlabels_All.extend(Targetlabels);Result_All.extend(Z);\n",
        "  return RFPP_all\n",
        "def all_non_consecutive(arr):\n",
        "    ans = []\n",
        "    start = arr[0]\n",
        "    index = 0\n",
        "    for number in arr:\n",
        "        if start == number:\n",
        "            start += 1\n",
        "            index += 1\n",
        "            continue\n",
        "\n",
        "        ans.append({'i': index, 'n': number})\n",
        "        start = number + 1\n",
        "        index += 1\n",
        "    return ans\n",
        "if __name__==\"__main__\":\n",
        "    #path='D:/PhD/Inhibitor/InhibitorNewModel2022/'\n",
        "    path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "    githubpath='/content/PPI-Inhibitors/'\n",
        "    Ubench5InterfaceandSeq_dict=pickle.load(open(githubpath+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "    Pos_seqandInterfaceF_dict=pickle.load(open(githubpath+'Features/Pos_seqandInterfaceF_dict.npy',\"rb\"))\n",
        "    Complex_AllFeatures_dict=dict( list (Pos_seqandInterfaceF_dict.items())+list (Ubench5InterfaceandSeq_dict.items()))\n",
        "    # Filename=path+'2p2i unique inhibitors.xlsx'\n",
        "    # df=pd.read_excel(Filename)\n",
        "    # Name1='Inhibitor name';Name2=' Inhibitor SMILES'\n",
        "    # Lsetname=df[Name1].values;Inhibitor_SMILES=df[Name2].values;\n",
        "    # Ldict=dict(zip(Lsetname,Inhibitor_SMILES))\n",
        "\n",
        "\n",
        "    # === REPLACEMENT CODE BLOCK ===\n",
        "# The original '.xlsx' file is missing from the repository.\n",
        "# This code loads the same data from '2p2iInhibitorsSMILES.txt' instead.\n",
        "\n",
        "Lsetname = []\n",
        "Inhibitor_SMILES = []\n",
        "\n",
        "# Define the path to the file that actually exists\n",
        "filepath = '/content/PPI-Inhibitors/Data/2p2iInhibitorsSMILES.txt'\n",
        "\n",
        "with open(filepath, 'r') as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        # Based on the file format, the inhibitor name is the 2nd item\n",
        "        # and the SMILES string is the 5th item.\n",
        "        if len(parts) >= 5:\n",
        "            inhibitor_name = parts[1]\n",
        "            smiles_string = parts[4]\n",
        "            Lsetname.append(inhibitor_name)\n",
        "            Inhibitor_SMILES.append(smiles_string)\n",
        "\n",
        "    print(f\"Successfully loaded {len(Lsetname)} inhibitors from 2p2iInhibitorsSMILES.txt\")\n",
        "\n",
        "    #/content/PPI-Inhibitors/Data/2p2iInhibitorsSMILES.txt\n",
        "    with open(githubpath+'Data/2p2iInhibitorsSMILES.txt') as f:#30May\n",
        "        D = f.readlines()\n",
        "    C=[];P=[];Y=[];Cseq=[];Pseq=[]\n",
        "    Ids=[];Ligandnames=[];Complexs=[];SMILESlist=[];\n",
        "    Inhibitedcomplexs=[]\n",
        "    Ldict = set(Lsetname)\n",
        "    for d in tqdm(D):\n",
        "      Id,Inhibitedcomp,Complex,Ligandname,SMILES,y = d.split()\n",
        "      # if Ligandname in Ldict:\n",
        "      Ids.append(Id);Ligandnames.append(Ligandname);Complexs.append(Complex);SMILESlist.append(SMILES);Inhibitedcomplexs.append(Inhibitedcomp);Y.append(y)\n",
        "    from collections import defaultdict\n",
        "    complex_ligand_dict=defaultdict(list)\n",
        "    AllCompoundslist=[]\n",
        "    Ldict=dict (zip(Ligandnames,SMILESlist))\n",
        "   # For every complex, save its actual ligand\n",
        "    for (key, value) in zip(Ids, Ligandnames):\n",
        "        if getFP(Ldict[value]) is not None:\n",
        "          if key not in complex_ligand_dict:\n",
        "            complex_ligand_dict[key]=[value]\n",
        "          else:\n",
        "            complex_ligand_dict[key]=np.append(complex_ligand_dict.get(key, ()) , value)\n",
        "    ###SuperDrugbank\n",
        "    df_Superdrug=pd.read_excel(githubpath+'Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"C\").values\n",
        "    df_Superdrug=df_Superdrug[1:]\n",
        "    ###Names\n",
        "    SuperdrugNames=pd.read_excel(githubpath+'Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#'approved_drugs_chemical_structure_identifiers.xlsx'\n",
        "    SuperdrugNames=SuperdrugNames[1:]\n",
        "    SuperdrugNames = np.array([s[0] for s in SuperdrugNames])\n",
        "    SuperdrugNames = np.append(SuperdrugNames,Ligandnames)#Lsetname)\n",
        "     ###\n",
        "    df_Superdrug_Compounds=np.array([c[0] for c in df_Superdrug])#3638\n",
        "    for s in range(len( df_Superdrug_Compounds)):\n",
        "        Ldict[SuperdrugNames[s]]=df_Superdrug_Compounds[s]\n",
        "    ###Binders\n",
        "    fields=['Complexname','Binders SMILES']\n",
        "    df=pd.read_csv(githubpath+'Data/BindersWithComplexname.csv', skipinitialspace=True, usecols=fields)\n",
        "    neg_Pidname,neg_smiles=df[df.keys()[0]].values,df[df.keys()[1]].values\n",
        "    Binders_dict=AppendlistinDict(neg_Pidname,neg_smiles)\n",
        "    Bset = list(set(neg_smiles)) #set of protein sequences\n",
        "    Bidx = list(range(len(Bset)))\n",
        "    Bseq2ids= dict(zip(Bset, Bidx)) #seq->index\n",
        "    Bids2seq = dict(zip(Bidx,Bset)) #index->seq\n",
        "    for b in Bids2seq:\n",
        "        Ldict[str (b)]=Bids2seq[b]\n",
        "    CompoundFingerprintFeaturesDict=pickle.load(open(githubpath+'Features/Compound_Fingerprint_Features_Dict.npy',\"rb\"))\n",
        "    Z = []; Yo = []; A = [];Yp=[]\n",
        "    AUC_ROC_final=[];Precision_final=[];Recall_final=[];Avg_P_final=[];\n",
        "    #############\n",
        "    comp=dict(zip(zip (Ids,Ligandnames),zip(Ids,SMILESlist)))\n",
        "    from sklearn.model_selection import GroupKFold\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.preprocessing import normalize\n",
        "    from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "    from sklearn.svm import LinearSVC,SVC\n",
        "    import pandas as pd\n",
        "    import random\n",
        "    from sklearn.metrics import auc,precision_recall_curve\n",
        "    CC=comp.keys()\n",
        "    CC=list(CC);KK=[]\n",
        "    [KK.append(k[0].split('_')[0]) for k in CC]\n",
        "    groups = pd.DataFrame(KK)\n",
        "    gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "    ###########\n",
        "    Testlen=0\n",
        "#    WriteAllexamplesRandomBindersAll=open(path+'WriteAllexamplesRandomBindersIdsAll_6JAN.txt', 'w')\n",
        "#    WriteAllexamplesRandomBindersAll=open(path+'WriteAllexamplesRandomBindersIdsAll_6JAN_Second.txt', 'r')\n",
        "    #with open(path+'WriteAllexamplesRandomBindersIdsAll_6JAN_Second.txt') as f:#30May\n",
        "    with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN.txt') as f:\n",
        "#    WriteAllexamplesRandomBindersIdsAll_6JAN\n",
        "        D = f.readlines()\n",
        "    Labels=[];Ligandnames=[];Complexs=[];#SMILESlist=[];\n",
        "    TestPoscomplexes=[]\n",
        "    for d in tqdm(D):\n",
        "      if len(d.split())==4:\n",
        "          TestPoscomp,Complexname,Ligandname,label = d.split()\n",
        "      else:\n",
        "          TestPoscomp,Complexname,Ligandname,label = d.split()[0],d.split()[1],(' ').join(d.split()[2:-1]),d.split()[-1]\n",
        "      TestPoscomplexes.append(TestPoscomp),Ligandnames.append(Ligandname);Complexs.append(Complexname);Labels.append(float (label))\n",
        "    #########\n",
        "    Allexamples=dict (zip(zip(TestPoscomplexes,zip(Complexs,Ligandnames)),Labels))\n",
        "#    KK=[]\n",
        "    Alldata=list (Allexamples.keys())\n",
        "    KK=[k[0].split('_')[0] for k in Alldata]\n",
        "    groups = pd.DataFrame(KK)\n",
        "    gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "    ###########\n",
        "    AlltestExamples=[];Externallabels=[];ExternalscoresLOCO=[];covid19_Externallabels=[];covid19_ExternalscoresLOCO=[];\n",
        "    Complexnames=list (complex_ligand_dict.keys())\n",
        "    Labels=np.array(Labels)\n",
        "    Complexs,Ligandnames=np.array(Complexs),np.array(Ligandnames)\n",
        "#    prev=0;\n",
        "#    for gc in gamma:\n",
        "#        for gp in gamma:\n",
        "    for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "        Alldata=list (Allexamples.keys());Alldata=np.array(Alldata, dtype=object)\n",
        "        train,test=Alldata[trainindex],Alldata[testindex]\n",
        "        Ctr=[];Ptr=[];y_train=[];\n",
        "        for t in train:\n",
        "            Ctr.append(CompoundFingerprintFeaturesDict[t[1][1]]);Ptr.append( Complex_AllFeatures_dict[t[1][0]]);y_train.append(Allexamples[t[0],t[1]])\n",
        "        Pscaler = StandardScaler().fit(Ptr)\n",
        "        Cscaler = StandardScaler().fit(Ctr)\n",
        "        Ptr,Ctr = Pscaler.transform(Ptr), Cscaler.transform(Ctr)\n",
        "        ########33\n",
        "        Kp = kernel(Ptr)#,gamma=1.0)\n",
        "        Kc = kernel(Ctr)#,gamma=1.0)\n",
        "\n",
        "        Ktr = Kp*Kc#for checking only\n",
        "        clf = SVC(C = 1.0, kernel = 'precomputed',class_weight='balanced')\n",
        "        clf.fit(Ktr,y_train)\n",
        "\n",
        "        Ctt=[];Ptt=[];y_test=[]\n",
        "        for t in test:\n",
        "            Ctt.append(CompoundFingerprintFeaturesDict[t[1][1]]);Ptt.append( Complex_AllFeatures_dict[t[1][0]]);y_test.append(Allexamples[t[0],t[1]])\n",
        "        Testlen=Testlen+len(test)\n",
        "        print (\"Testlen\",len(test),\"train length\",len(train))\n",
        "        Ptt,Ctt = Pscaler.transform(Ptt), Cscaler.transform(Ctt)\n",
        "        Kp = kernel(Ptt,Ptr)\n",
        "        Kc = kernel(Ctt,Ctr)\n",
        "#        Ktt_test= Kptest*Kctest#for checking only\n",
        "        Ktt = (Kp*Kc)#**2# (Kp**2+Kc**2+2*Kp*Kc)Kp*Kc#\n",
        "        z = clf.decision_function(Ktt)\n",
        "        yp=clf.predict(Ktt)\n",
        "        Auc = roc_auc_score(y_test, z)\n",
        "        average_P_score=average_precision_score(y_test, z)\n",
        "#        print(\"\\nfold Auc:\",Auc,\"average_P_score=\",average_P_score ,\"P\",len(testPos),\"N\",len(testNeg))\n",
        "        print(t[0],\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3))# ,\"\\t\",len(),\"\\t\",len(testNegcomp),\"\\t\",round (len(testNegcomp) /len(testPoscomp),1))\n",
        "#        pdb.set_trace()\n",
        "        AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "        Z.extend(list(z));Yo.extend(list(y_test))#;Yp.extend(list(yp))\n",
        "        #######################3\n",
        "        #/content/PPI-Inhibitors/Data/External data/2dyh_all.txt#githubpath+'/Data/External data/2dyh_all.txt\n",
        "        #RFPP_all=PredictRFPPfromFile(githubpath,githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "        External_score,External_labels=PredictScorefromFileSVM(githubpath,githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "        ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "        External_Auc= roc_auc_score(External_labels, External_score)\n",
        "        External_AP=average_precision_score(External_labels, External_score)\n",
        "        print(\"External_Auc,PR,RFPP\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "        #########\n",
        "        #Covid19_RFPP_all=PredictRFPPfromFile(githubpath,githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "        Covid19_External_score,Covid19_External_labels=PredictScorefromFileSVM(githubpath,githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "        covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "        Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "        Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "        print(\"Covid19_External_Auc,PR,RFPP\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")\n",
        "    #####\n",
        "    External_fpr, External_tpr, External_thresholds = roc_curve(Externallabels,ExternalscoresLOCO)\n",
        "    External_Auc = roc_auc_score(Externallabels,ExternalscoresLOCO)\n",
        "    External_Auc=(External_Auc).round(2)\n",
        "    fig = plt.figure()\n",
        "    plt.plot(External_fpr, External_tpr,color='k',marker='d',label='External_Auc:{: .2f}'.format(External_Auc))\n",
        "    plt.title('AUCROC External');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "    fig .savefig(path+\"AUCROC External SVM PPI Inhibitors Random and Binders combine Negative.pdf\", bbox_inches='tight')\n",
        "    ########\n",
        "    covid19_External_fpr, covid19_External_tpr, covid19_External_thresholds = roc_curve(covid19_Externallabels,covid19_ExternalscoresLOCO)\n",
        "    covid19_External_Auc = roc_auc_score(covid19_Externallabels,covid19_ExternalscoresLOCO)\n",
        "    covid19_External_Auc=(covid19_External_Auc).round(2)\n",
        "    fig = plt.figure()\n",
        "    plt.plot(covid19_External_fpr, covid19_External_tpr,color='k',marker='d',label='covid19_External_Auc:{: .2f}'.format(covid19_External_Auc))\n",
        "    plt.title('covid19 AUCROC External');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "    fig .savefig(path+\"AUCROC covid19 External SVM PPI Inhibitors Random and Binders combine Negative.pdf\", bbox_inches='tight')\n",
        "    ########\n",
        "    fpr, tpr, thresholds = roc_curve(Yo, Z)\n",
        "    Auc = roc_auc_score(Yo, Z)\n",
        "    Auc=(Auc).round(2)\n",
        "    # calculate precision-recall curve\n",
        "    precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "    aucpr=auc(recall,precision)\n",
        "    aucpr=(aucpr).round(2)\n",
        "    #####\n",
        "    Yo=np.array(Yo)\n",
        "    print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "    ######\n",
        "    fig = plt.figure()\n",
        "    plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "    plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "    fig .savefig(path+\"AUCROC SVM PPI Inhibitors Random and Binders combine Negative.pdf\", bbox_inches='tight')\n",
        "    ###\n",
        "    precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "    aucpr=average_precision_score (Yo, Z)\n",
        "    ######\n",
        "    fig = plt.figure()\n",
        "    plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "    plt.title('AUC-PR');plt.xlabel('Recall');plt.ylabel('Precision');plt.grid();plt.legend();plt.show();\n",
        "    fig .savefig(path+\"AUC-PR PPI Inhibitors  Random and Binders combine.pdf\", bbox_inches='tight')\n",
        "    print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "    # 1/0\n",
        "    InterfaceFeatures(Complexs,Filename,path,pdbloc)\n",
        "\n",
        "    # 1/0\n",
        "    LsetFeature_dict=dict(zip(Lsetname,Inhibitor_SMILES))\n",
        "    AlltestExamplesInhibitorScores_dict={}\n",
        "    Labels=[]\n",
        "    for t in range (len(AlltestExamples)):\n",
        "        I=(AlltestExamples[t][1])\n",
        "#        print(AlltestExamples[t],AlltestExamples[t][1],\"\\n\")\n",
        "        if I not in AlltestExamplesInhibitorScores_dict:#and I not in LsetFeature_dict and Yo[t]==-1.0:\n",
        "            if I in LsetFeature_dict:\n",
        "                Label=1.0\n",
        "            else:\n",
        "                Label=-1.0\n",
        "            Labels.append(Label)\n",
        "            AlltestExamplesInhibitorScores_dict[I]=(Z[t],Label)\n",
        "        else:\n",
        "            AlltestExamplesInhibitorScores_dict[I]=np.max(np.asarray(AlltestExamplesInhibitorScores_dict[I][0],Z[t])),AlltestExamplesInhibitorScores_dict[I][1]\n",
        "    ########################\n",
        "    Pscores,Plabels=[],[]\n",
        "    for ee in AlltestExamplesInhibitorScores_dict:\n",
        "        Pscore,Plabel=AlltestExamplesInhibitorScores_dict[ee]\n",
        "        Pscores.append(Pscore),Plabels.append(Plabel)\n",
        "    fpr, tpr, thresholds = roc_curve(Plabels,Pscores)\n",
        "    AucProteinIndependent = roc_auc_score(Plabels,Pscores)\n",
        "    AucProteinIndependent=(AucProteinIndependent).round(2)\n",
        "    # calculate precision-recall curve\n",
        "#    precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "#    aucpr=auc(recall,precision)\n",
        "#    aucpr=(aucpr).round(2)\n",
        "#    auc = roc_auc_score(Yo, Z)\n",
        "    #####\n",
        "#    Yo=np.array(Yo)\n",
        "#    print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "    ######\n",
        "    fig = plt.figure()\n",
        "    plt.plot(fpr,tpr,color='b',marker='*',label='AUC Protein Independent:{: .2f}'.format(AucProteinIndependent))\n",
        "    plt.title('AUCROC ProteinIndependent');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "    fig .savefig(path+\"PPI Inhibitors AucProteinIndependent.pdf\", bbox_inches='tight')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "ZIfLGNhsBQPs",
        "outputId": "98eabc9b-a1d7-469c-8abb-9bf12be3f86c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'Bio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1137173881.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#from BISEPutils import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#from myPDBUpdated import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mBio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmapU2B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muS2Ri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mulR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbS2Ri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Bio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "name": "svmreadfromfile-generate-prediction-binders-and-random-both-as-negative.ipynb",
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}